{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = \"10.48550/arXiv.2412.10415\"\n",
    "# s2_paper_id = \"9e57dda195973c4b6c81386b1cc44595ecfd4697\"\n",
    "title = \"Generative Adversarial Reviews: When LLMs Become the Critic\"\n",
    "abstract = \"The peer review process is fundamental to scientific progress, determining which papers meet the quality standards for publication. Yet, the rapid growth of scholarly production and increasing specialization in knowledge areas strain traditional scientific feedback mechanisms. In light of this, we introduce Generative Agent Reviewers (GAR), leveraging LLM-empowered agents to simulate faithful peer reviewers. To enable generative reviewers, we design an architecture that extends a large language model with memory capabilities and equips agents with reviewer personas derived from historical data. Central to this approach is a graph-based representation of manuscripts, condensing content and logically organizing information - linking ideas with evidence and technical details. GAR's review process leverages external knowledge to evaluate paper novelty, followed by detailed assessment using the graph representation and multi-round assessment. Finally, a meta-reviewer aggregates individual reviews to predict the acceptance decision. Our experiments demonstrate that GAR performs comparably to human reviewers in providing detailed feedback and predicting paper outcomes. Beyond mere performance comparison, we conduct insightful experiments, such as evaluating the impact of reviewer expertise and examining fairness in reviews. By offering early expert-level feedback, typically restricted to a limited group of researchers, GAR democratizes access to transparent and in-depth evaluation.\"\n",
    "text = f\"TITLE: {title}\\nABSTRACT: {abstract}\"\n",
    "paper = {'title':title, 'abstract': abstract}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TITLE: Generative Adversarial Reviews: When LLMs Become the Critic\\nABSTRACT: The peer review process is fundamental to scientific progress, determining which papers meet the quality standards for publication. Yet, the rapid growth of scholarly production and increasing specialization in knowledge areas strain traditional scientific feedback mechanisms. In light of this, we introduce Generative Agent Reviewers (GAR), leveraging LLM-empowered agents to simulate faithful peer reviewers. To enable generative reviewers, we design an architecture that extends a large language model with memory capabilities and equips agents with reviewer personas derived from historical data. Central to this approach is a graph-based representation of manuscripts, condensing content and logically organizing information - linking ideas with evidence and technical details. GAR's review process leverages external knowledge to evaluate paper novelty, followed by detailed assessment using the graph representation and multi-round assessment. Finally, a meta-reviewer aggregates individual reviews to predict the acceptance decision. Our experiments demonstrate that GAR performs comparably to human reviewers in providing detailed feedback and predicting paper outcomes. Beyond mere performance comparison, we conduct insightful experiments, such as evaluating the impact of reviewer expertise and examining fairness in reviews. By offering early expert-level feedback, typically restricted to a limited group of researchers, GAR democratizes access to transparent and in-depth evaluation.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"paper_nodes_json.jsonl\"\n",
    "\n",
    "nodes_json = []\n",
    "with open(filename, 'r') as f:\n",
    "    for item in f:\n",
    "        nodes_json.append(json.loads(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_reorder_dict(input_dict, keys_to_keep):\n",
    "    \"\"\"filter and re-order keys of dict\"\"\"\n",
    "    return {key: input_dict[key] for key in keys_to_keep if key in input_dict}\n",
    "\n",
    "nodes_json_rvsd = [filter_and_reorder_dict(x, ['type', 'id', 'labels', 'properties']) for x in nodes_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"paper_edges_json.jsonl\"\n",
    "\n",
    "edges_json = []\n",
    "with open(filename, 'r') as f:\n",
    "    for item in f:\n",
    "        edges_json.append(json.loads(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on title and abstract through semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jiezi/Code/GitHub/ResearchTree/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(parent_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_dois = []\n",
    "for edge in edges_json:\n",
    "    if edge['startNodeId'] == doi and edge['relationshipType'] == 'CITES':\n",
    "        cited_dois.append(edge['endNodeId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 47\n"
     ]
    }
   ],
   "source": [
    "cited_abs_missing_ids = []\n",
    "for node in nodes_json:\n",
    "    if node['id'] in cited_dois:\n",
    "        abstract = node['properties'].get('abstract')\n",
    "        if abstract is None:\n",
    "            cited_abs_missing_ids.append(node['properties']['s2PaperId'])\n",
    "print(len(cited_dois), len(cited_abs_missing_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 10:58:08,006 - INFO - HTTP Request: POST https://api.semanticscholar.org/graph/v1/paper/batch?fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from apis.s2_api import SemanticScholarKit\n",
    "\n",
    "s2 = SemanticScholarKit()\n",
    "cited_papers_metadata_missing_abs = s2.search_paper_by_ids(id_list=cited_abs_missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cited_papers_metadata_missing_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = [x['paperId'] for x in cited_papers_metadata_missing_abs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The rapid advancement of scientific progress requires innovative tools that can accelerate knowledge discovery. Although recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short of replicating the collaborative nature of real-world scientific practices, where diverse experts work together in teams to tackle complex problems. To address the limitations, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci), designed to mimic the teamwork inherent in scientific research. VirSci organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel scientific ideas. We further investigate the collaboration mechanisms that contribute to its tendency to produce ideas with higher novelty, offering valuable insights to guide future research and illuminating pathways toward building a robust system for autonomous scientific discovery. The code is available at https://github.com/open-sciencelab/Virtual-Scientists.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cited_papers_metadata_missing_abs[0]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "titles, abstracts = [], []\n",
    "for node in nodes_json:\n",
    "    if node['id'] in cited_dois:\n",
    "        title = node['properties']['title']\n",
    "        s2_paper_id = node['properties']['s2PaperId']\n",
    "        abstract = node['properties'].get('abstract')\n",
    "        if abstract is None:\n",
    "            idx = missing_ids.index(s2_paper_id)\n",
    "            abstract = cited_papers_metadata_missing_abs[idx]['abstract']\n",
    "            node['properties']['abstract'] = abstract\n",
    "\n",
    "        texts.append(f\"TITLE: {title}\\nABSTRACT: {abstract}\")\n",
    "        titles.append(title)\n",
    "        abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "config_file = \"config.toml\"\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as toml_file:\n",
    "        config_param = toml.load(toml_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file '{config_file}' not found. Please ensure it exists.\")\n",
    "    config_param = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_api_key = config_param.get('models', {}).get('llm', {}).get('api_key')\n",
    "llm_model_name = config_param.get('models', {}).get('llm', {}).get('model_name')\n",
    "embed_api_key = config_param.get('models', {}).get('embed', {}).get('api_key')\n",
    "embed_model_name = config_param.get('models', {}).get('embed', {}).get('model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "#           {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\n",
    "\n",
    "assert len(titles) == len(abstracts)\n",
    "papers = []\n",
    "for idx, title in enumerate(titles):\n",
    "    abstract = abstracts[idx] if abstracts[idx] else 'NAN'\n",
    "    papers.append({'title':title, 'abstract':abstract})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.embedding_models import gemini_embedding_async\n",
    "\n",
    "embeds = await gemini_embedding_async(embed_api_key, embed_model_name, [text]+texts, 10) # Assuming texts_embed_gen is an async function for IO-bound operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.embedding_models import semantic_similarity_matrix\n",
    "\n",
    "sim_matrix = semantic_similarity_matrix(embeds[0], embeds[1:])\n",
    "sim_matrix = np.array(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = []\n",
    "for idx, sim in enumerate(sim_matrix[0]):\n",
    "    opt.append({'doi': cited_dois[idx], 'title':titles[idx], 'score':round(sim, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_opt = sorted(opt, key=lambda item: item['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doi': '10.48550/arXiv.2408.10365',\n",
       "  'title': 'AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews',\n",
       "  'score': 0.8349},\n",
       " {'doi': '10.48550/arXiv.2410.03019',\n",
       "  'title': 'Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review',\n",
       "  'score': 0.8037},\n",
       " {'doi': '10.48550/arXiv.2307.05492',\n",
       "  'title': 'ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing',\n",
       "  'score': 0.7953},\n",
       " {'doi': '10.48550/arXiv.2404.16130',\n",
       "  'title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models',\n",
       "  'score': 0.7949},\n",
       " {'doi': '10.48550/arXiv.2307.14984',\n",
       "  'title': 'GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study',\n",
       "  'score': 0.7798},\n",
       " {'doi': '10.48550/arXiv.2406.10252',\n",
       "  'title': 'Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions',\n",
       "  'score': 0.7768},\n",
       " {'doi': '10.48550/arXiv.2310.05984',\n",
       "  'title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis',\n",
       "  'score': 0.7726},\n",
       " {'doi': '10.48550/arXiv.2305.14259',\n",
       "  'title': 'Can Large Language Models Be an Alternative to Human Evaluations?',\n",
       "  'score': 0.7343},\n",
       " {'doi': '10.48550/arXiv.2408.06292',\n",
       "  'title': 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery',\n",
       "  'score': 0.7278},\n",
       " {'doi': '10.48550/arXiv.2410.09403',\n",
       "  'title': 'Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System',\n",
       "  'score': 0.7142},\n",
       " {'doi': '10.48550/arXiv.2402.05200',\n",
       "  'title': 'Large Language Models for Scientific Synthesis, Inference and Explanation',\n",
       "  'score': 0.7138},\n",
       " {'doi': '10.48550/arXiv.2309.12871',\n",
       "  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n",
       "  'score': 0.7084},\n",
       " {'doi': '10.48550/arXiv.1805.02262',\n",
       "  'title': 'A multi-disciplinary perspective on emergent and future innovations in peer review',\n",
       "  'score': 0.7018},\n",
       " {'doi': '10.1038/s41586-024-07487-w',\n",
       "  'title': 'From Local to Global: A Graph RAG Approach to Query-Focused Summarization',\n",
       "  'score': 0.7001},\n",
       " {'doi': '10.48550/arXiv.2406.10252',\n",
       "  'title': 'AutoSurvey: Large Language Models Can Automatically Write Surveys',\n",
       "  'score': 0.699},\n",
       " {'doi': '10.48550/arXiv.2310.10436',\n",
       "  'title': 'LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games',\n",
       "  'score': 0.6982},\n",
       " {'doi': '10.48550/arXiv.2409.11239',\n",
       "  'title': 'LLM-as-a-Judge & Reward Model: What They Can and Cannot Do',\n",
       "  'score': 0.6964},\n",
       " {'doi': '10.1038/s41598-023-40858-3',\n",
       "  'title': 'Bias and Fairness in Large Language Models: A Survey',\n",
       "  'score': 0.6856},\n",
       " {'doi': '10.48550/arXiv.2305.03514',\n",
       "  'title': 'Generative Agents: Interactive Simulacra of Human Behavior',\n",
       "  'score': 0.682},\n",
       " {'doi': '10.1038/s41586-023-06221-2',\n",
       "  'title': 'S3: Social-network Simulation System with Large Language Model-Empowered Agents',\n",
       "  'score': 0.6712},\n",
       " {'doi': '10.48550/arXiv.2305.01937',\n",
       "  'title': 'Can Large Language Models Transform Computational Social Science?',\n",
       "  'score': 0.6657},\n",
       " {'doi': '10.48550/arXiv.2305.16867',\n",
       "  'title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty',\n",
       "  'score': 0.6619},\n",
       " {'doi': '10.48550/arXiv.1810.08473',\n",
       "  'title': 'Construction of the Literature Graph in Semantic Scholar',\n",
       "  'score': 0.6585},\n",
       " {'doi': '10.1609/aaai.v35i10.17047',\n",
       "  'title': 'SPECTER: Document-level Representation Learning using Citation-informed Transformers',\n",
       "  'score': 0.6584},\n",
       " {'doi': '10.2139/ssrn.4526071',\n",
       "  'title': 'Large Language Model-Empowered Agents for Simulating Macroeconomic Activities',\n",
       "  'score': 0.6582},\n",
       " {'doi': '10.48550/arXiv.2310.03302',\n",
       "  'title': 'Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms',\n",
       "  'score': 0.6575},\n",
       " {'doi': '10.48550/arXiv.2306.00622',\n",
       "  'title': 'Playing repeated games with Large Language Models',\n",
       "  'score': 0.6555},\n",
       " {'doi': '10.1145/2827872',\n",
       "  'title': \"Editorial Peer Reviewers' Recommendations at a General Medical Journal: Are They Reliable and Do Editors Care?\",\n",
       "  'score': 0.6554},\n",
       " {'doi': '10.48550/arXiv.2404.07738',\n",
       "  'title': 'Are LLMs Ready for Real-World Materials Discovery?',\n",
       "  'score': 0.6451},\n",
       " {'doi': '10.48550/arXiv.2205.08363',\n",
       "  'title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models',\n",
       "  'score': 0.643},\n",
       " {'doi': '10.48550/arXiv.2212.10789',\n",
       "  'title': 'A Decade of Knowledge Graphs in Natural Language Processing: A Survey',\n",
       "  'score': 0.6422},\n",
       " {'doi': '10.48550/arXiv.2004.07180',\n",
       "  'title': 'Longformer: The Long-Document Transformer',\n",
       "  'score': 0.641},\n",
       " {'doi': '10.48550/arXiv.2310.07984',\n",
       "  'title': 'MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation',\n",
       "  'score': 0.6383},\n",
       " {'doi': '10.48550/arXiv.2309.00770',\n",
       "  'title': 'Employing large language models in survey research',\n",
       "  'score': 0.6353},\n",
       " {'doi': '10.48550/arXiv.2304.03442',\n",
       "  'title': 'Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing',\n",
       "  'score': 0.632},\n",
       " {'doi': '10.1371/journal.pone.0254034',\n",
       "  'title': 'The Values Encoded in Machine Learning Research',\n",
       "  'score': 0.63},\n",
       " {'doi': '10.48550/arXiv.2209.14734',\n",
       "  'title': 'Out of One, Many: Using Language Models to Simulate Human Samples',\n",
       "  'score': 0.6266},\n",
       " {'doi': '10.48550/arXiv.2409.17146',\n",
       "  'title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework',\n",
       "  'score': 0.6256},\n",
       " {'doi': '10.48550/arXiv.2210.00105',\n",
       "  'title': 'DiGress: Discrete Denoising diffusion for graph generation',\n",
       "  'score': 0.6238},\n",
       " {'doi': '10.48550/arXiv.2305.16960',\n",
       "  'title': 'Ideas are Dimes a Dozen: Large Language Models for Idea Generation in Innovation',\n",
       "  'score': 0.6171},\n",
       " {'doi': '10.48550/arXiv.2310.01783',\n",
       "  'title': 'AnglE-optimized Text Embeddings',\n",
       "  'score': 0.6147},\n",
       " {'doi': '10.48550/arXiv.2309.17234',\n",
       "  'title': 'Large language models meet cognitive science: LLMs as tools, models, and participants',\n",
       "  'score': 0.6127},\n",
       " {'doi': '10.1145/3460231.3474267',\n",
       "  'title': 'Measuring novelty in science with word embedding',\n",
       "  'score': 0.6124},\n",
       " {'doi': '10.48550/arXiv.2406.08414',\n",
       "  'title': 'Discovering Preference Optimization Algorithms with and for Large Language Models',\n",
       "  'score': 0.6061},\n",
       " {'doi': '1a4c6856292b8c64d19a812a77f0aa6fd47cb96c',\n",
       "  'title': 'Training Socially Aligned Language Models in Simulated Human Society',\n",
       "  'score': 0.6059},\n",
       " {'doi': '10.48550/arXiv.2201.11903',\n",
       "  'title': 'Artificial intelligence: A powerful paradigm for scientific research',\n",
       "  'score': 0.6034},\n",
       " {'doi': '10.48550/arXiv.2209.06899',\n",
       "  'title': 'REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research',\n",
       "  'score': 0.6},\n",
       " {'doi': '10.1016/j.nlp.2023.100020',\n",
       "  'title': 'Scientific discovery in the age of artificial intelligence',\n",
       "  'score': 0.5856},\n",
       " {'doi': '10.1371/journal.pone.0010072',\n",
       "  'title': 'Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models',\n",
       "  'score': 0.5734},\n",
       " {'doi': '10.1016/j.xinn.2021.100179',\n",
       "  'title': 'Recommendation on Live-Streaming Platforms: Dynamic Availability and Repeat Consumption',\n",
       "  'score': 0.559},\n",
       " {'doi': '10.48550/arXiv.2309.07864',\n",
       "  'title': 'Best humans still outperform artificial intelligence in a creative divergent thinking task',\n",
       "  'score': 0.543},\n",
       " {'doi': '10.48550/arXiv.2406.05688',\n",
       "  'title': 'Accurate structure prediction of biomolecular interactions with AlphaFold 3',\n",
       "  'score': 0.542},\n",
       " {'doi': '10.48550/arXiv.2004.05150',\n",
       "  'title': 'From Louvain to Leiden: guaranteeing well-connected communities',\n",
       "  'score': 0.5305},\n",
       " {'doi': '10.48550/arXiv.2106.15590',\n",
       "  'title': 'Learning a Few-shot Embedding Model with Contrastive Learning',\n",
       "  'score': 0.5255},\n",
       " {'doi': '10.12688/f1000research.12037.3',\n",
       "  'title': 'The MovieLens Datasets: History and Context',\n",
       "  'score': 0.5135},\n",
       " {'doi': 'd467207bb561028430eda0af22c4e427c04b4eb1',\n",
       "  'title': 'Bias : A Critical Review',\n",
       "  'score': 0.4596}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Similarity Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "threshold = 0.7\n",
    "\n",
    "top_k_dois = [x['doi'] for x in sorted_opt[0:10] if x['score'] > 0.7]\n",
    "print(len(top_k_dois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:07:47,750 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2408.10365/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:07:54,718 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2410.03019/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:08:01,675 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2307.05492/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:08:08,753 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2404.16130/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:08:16,026 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2307.14984/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:08:23,017 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2406.10252/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:08:29,951 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2310.05984/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 11:08:36,803 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2305.14259/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 404 Not Found\"\n",
      "2025-03-28 11:08:36,805 - ERROR - Error when getting papers cited by 10.48550/arXiv.2305.14259: Paper with id 10.48550/arXiv.2305.14259 not found\n",
      "2025-03-28 11:08:43,050 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2408.06292/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 404 Not Found\"\n",
      "2025-03-28 11:08:43,052 - ERROR - Error when getting papers cited by 10.48550/arXiv.2408.06292: Paper with id 10.48550/arXiv.2408.06292 not found\n",
      "2025-03-28 11:08:49,533 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/10.48550/arXiv.2410.09403/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from apis.s2_api import SemanticScholarKit\n",
    "\n",
    "s2 = SemanticScholarKit()\n",
    "\n",
    "cited_papers_info = []\n",
    "for doi in top_k_dois:\n",
    "    results = s2.get_s2_cited_papers(paper_id=doi)\n",
    "    cited_papers_info.append(results)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "found overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cited_papers_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['contexts', 'contextsWithIntent', 'isInfluential', 'intents', 'citedPaper'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cited_papers_info[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_2_papers = []\n",
    "for results in cited_papers_info:\n",
    "    for item in results:\n",
    "        hop_2_papers.append(item['citedPaper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hop_2_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cited_papers_metadata_missing_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "candit_papers = cited_papers_metadata_missing_abs + hop_2_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [x['paperId'] for x in candit_papers]\n",
    "ids_2 = [x['paperId'] for x in cited_papers_metadata_missing_abs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'paperId' 对应的取值统计 (从高到低排序):\n",
      "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery: 3\n",
      "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions: 3\n",
      "Can large language models provide useful feedback on research papers? A large-scale empirical analysis: 3\n",
      "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing: 3\n",
      "Generative Agents: Interactive Simulacra of Human Behavior: 3\n",
      "GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study: 2\n",
      "Out of One, Many: Using Language Models to Simulate Human Samples: 2\n",
      "Longformer: The Long-Document Transformer: 2\n",
      "From Louvain to Leiden: guaranteeing well-connected communities: 2\n",
      "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System: 1\n",
      "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review: 1\n",
      "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do: 1\n",
      "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews: 1\n",
      "Discovering Preference Optimization Algorithms with and for Large Language Models: 1\n",
      "Accurate structure prediction of biomolecular interactions with AlphaFold 3: 1\n",
      "From Local to Global: A Graph RAG Approach to Query-Focused Summarization: 1\n",
      "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models: 1\n",
      "Are LLMs Ready for Real-World Materials Discovery?: 1\n",
      "Large Language Models for Scientific Synthesis, Inference and Explanation: 1\n",
      "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation: 1\n",
      "Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms: 1\n",
      "AnglE-optimized Text Embeddings: 1\n",
      "The Rise and Potential of Large Language Model Based Agents: A Survey: 1\n",
      "Best humans still outperform artificial intelligence in a creative divergent thinking task: 1\n",
      "Employing large language models in survey research: 1\n",
      "Scientific discovery in the age of artificial intelligence: 1\n",
      "S3: Social-network Simulation System with Large Language Model-Empowered Agents: 1\n",
      "Playing repeated games with Large Language Models: 1\n",
      "SciMON: Scientific Inspiration Machines Optimized for Novelty: 1\n",
      "Can Large Language Models Be an Alternative to Human Evaluations?: 1\n",
      "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing: 1\n",
      "A Decade of Knowledge Graphs in Natural Language Processing: A Survey: 1\n",
      "DiGress: Discrete Denoising diffusion for graph generation: 1\n",
      "REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research: 1\n",
      "Chain of Thought Prompting Elicits Reasoning in Large Language Models: 1\n",
      "Artificial intelligence: A powerful paradigm for scientific research: 1\n",
      "Recommendation on Live-Streaming Platforms: Dynamic Availability and Repeat Consumption: 1\n",
      "The Values Encoded in Machine Learning Research: 1\n",
      "The MovieLens Datasets: History and Context: 1\n",
      "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models: 1\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework: 1\n",
      "Training Socially Aligned Language Models in Simulated Human Society: 1\n",
      "Ideas are Dimes a Dozen: Large Language Models for Idea Generation in Innovation: 1\n",
      "Large Language Model-Empowered Agents for Simulating Macroeconomic Activities: 1\n",
      "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games: 1\n",
      "Large language models meet cognitive science: LLMs as tools, models, and participants: 1\n",
      "Bias : A Critical Review: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "target_key = 'paperId'\n",
    "value_counts = defaultdict(int)\n",
    "\n",
    "for item in candit_papers:\n",
    "    if target_key in item:\n",
    "        value = item[target_key]\n",
    "        value_counts[value] += 1\n",
    "\n",
    "# 按照出现次数从高到低排序\n",
    "sorted_counts = sorted(value_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(f\"'{target_key}' 对应的取值统计 (从高到低排序):\")\n",
    "for value, count in sorted_counts:\n",
    "    paper_id = value\n",
    "    if paper_id in ids_2:\n",
    "        idx = ids.index(paper_id)\n",
    "        title = candit_papers[idx]['title']\n",
    "        print(f\"{title}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sim in enumerate(sim_matrix.tolist()[0]):\n",
    "    print(titles[idx], sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sim_matrix.tolist()[0].index(0.9483)\n",
    "texts[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.round(sim_matrix, 4).tolist()[0].index(0.7278)\n",
    "texts[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.round(sim_matrix, 4).tolist()[0].index(0.7037)\n",
    "texts[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sim_matrix.tolist()[0]), len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_scores = [\n",
    "  {\"title\": \"Long-context LLMs Struggle with Long In-context Learning\", \"score\": 0.4},\n",
    "  {\"title\": \"Distilling Text Style Transfer With Self-Explanation From LLMs\", \"score\": 0.1},\n",
    "  {\"title\": \"KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models\", \"score\": 0.5},\n",
    "  {\"title\": \"Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\", \"score\": 0.7},\n",
    "  {\"title\": \"Nomic Embed: Training a Reproducible Long Context Text Embedder\", \"score\": 0.3},\n",
    "  {\"title\": \"With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation\", \"score\": 0.4},\n",
    "  {\"title\": \"Retrieval-Augmented Generation for Large Language Models: A Survey\", \"score\": 0.6},\n",
    "  {\"title\": \"LooGLE: Can Long-Context Language Models Understand Long Contexts?\", \"score\": 0.5},\n",
    "  {\"title\": \"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity\", \"score\": 0.6},\n",
    "  {\"title\": \"Automatic Sensor-free Affect Detection: A Systematic Literature Review\", \"score\": 0.05},\n",
    "  {\"title\": \"BooookScore: A systematic exploration of book-length summarization in the era of LLMs\", \"score\": 0.45},\n",
    "  {\"title\": \"BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models\", \"score\": 0.5},\n",
    "  {\"title\": \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\", \"score\": 0.5},\n",
    "  {\"title\": \"Challenges and Applications of Large Language Models\", \"score\": 0.6},\n",
    "  {\"title\": \"A Survey on Evaluation of Large Language Models\", \"score\": 0.7},\n",
    "  {\"title\": \"Lost in the Middle: How Language Models Use Long Contexts\", \"score\": 0.6},\n",
    "  {\"title\": \"Extending Context Window of Large Language Models via Positional Interpolation\", \"score\": 0.7},\n",
    "  {\"title\": \"Augmenting Language Models with Long-Term Memory\", \"score\": 0.7},\n",
    "  {\"title\": \"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\", \"score\": 0.4},\n",
    "  {\"title\": \"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\", \"score\": 0.45},\n",
    "  {\"title\": \"Enabling Large Language Models to Generate Text with Citations\", \"score\": 0.55}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in llm_scores:\n",
    "    title = item.get('title')\n",
    "    llm_score = item.get('score')\n",
    "    for idx, text in enumerate(texts):\n",
    "        if title in text:\n",
    "            embed_score = sim_matrix.tolist()[0][idx]\n",
    "            print(title, llm_score, embed_score)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in edges_json:\n",
    "    if edge['startNodeId'] == doi and edge['relationshipType'] == 'SIMILAR_TO' and edge['endNodeId'] in cited_dois:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Based on contexts information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_info = []\n",
    "for edge in edges_json:\n",
    "    if edge['startNodeId'] == doi and edge['endNodeId'] in cited_dois:\n",
    "        info = edge['properties'].get('contextsWithIntent')\n",
    "        cited_info.append(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cited_info))\n",
    "print(len([x for x in cited_info if len(x) > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_contexts, filtered_idx = [], []\n",
    "citation_info = []\n",
    "for idx, item in enumerate(cited_info):\n",
    "    if len(item) > 0:\n",
    "        context = '...'. join([x.get('context') for x in item])\n",
    "        filtered_contexts.append(context)\n",
    "        filtered_idx.append(idx)\n",
    "        title = titles[idx]\n",
    "        citation_info.append({'title':title, 'context':context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output = [\n",
    "  {\n",
    "    \"title\": \"Long-context LLMs Struggle with Long In-context Learning\",\n",
    "    \"reason\": \"Cited in the context of 'window limitations', this paper directly addresses one of the core challenges that AutoSurvey aims to solve, providing insights into the inherent restrictions of long-context LLMs.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models\",\n",
    "    \"reason\": \"Cited in the context of 'Muti-LLM-as-judge evaluation', this paper introduces an evaluation framework relevant to the evaluation strategy employed by AutoSurvey for assessing the quality of generated surveys.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\",\n",
    "    \"reason\": \"Cited in the context of 'parametric knowledge constraints', this paper explores methods for using LLMs to generate comprehensive and accurate content, which is relevant to overcoming the limitations of LLM's internal knowledge in survey creation.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Retrieval-Augmented Generation for Large Language Models: A Survey\",\n",
    "    \"reason\": \"Cited in the context of 'Real-time knowledge update', this paper provides a comprehensive overview of Retrieval-Augmented Generation (RAG), the core technique used by AutoSurvey for incorporating up-to-date information into the generated surveys.\",\n",
    "    \"score\": 0.9\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"LooGLE: Can Long-Context Language Models Understand Long Contexts?\",\n",
    "    \"reason\": \"Cited in the context of 'Long-form Text Generation' and related to 'window limitations', this paper investigates the fundamental ability of long-context LLMs to process and understand long texts, a crucial aspect for generating comprehensive surveys.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity\",\n",
    "    \"reason\": \"Cited in the context of 'parametric knowledge constraints', this survey discusses the critical issue of factuality in LLMs, which is paramount for the reliability and accuracy of automatically generated literature surveys.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Lost in the Middle: How Language Models Use Long Contexts\",\n",
    "    \"reason\": \"Cited in the context of 'window limitations', this paper delves into how language models utilize long context, directly addressing the challenges associated with processing long input sequences in LLMs.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Extending Context Window of Large Language Models via Positional Interpolation\",\n",
    "    \"reason\": \"Cited in the context of long-context scenarios, this paper presents a specific technique for extending the context window of LLMs, which is a key area of research relevant to overcoming the limitations faced by AutoSurvey.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Augmenting Language Models with Long-Term Memory\",\n",
    "    \"reason\": \"Cited in the context of long-context scenarios, this paper explores methods for equipping LLMs with long-term memory capabilities, which is relevant to the challenge of maintaining coherence and incorporating extensive information in automatically generated surveys.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\",\n",
    "    \"reason\": \"Cited in the context of 'Muti-LLM-as-judge evaluation', this paper evaluates the 'LLM-as-judge' method, providing insights into the reliability and effectiveness of using LLMs to evaluate text, a strategy employed by AutoSurvey.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\",\n",
    "    \"reason\": \"Cited in the context of 'Muti-LLM-as-judge evaluation', this paper introduces an automatic evaluation benchmark for LLMs, which is relevant to the need for robust evaluation methods for automatically generated content like surveys.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Enabling Large Language Models to Generate Text with Citations\",\n",
    "    \"reason\": \"Cited in the context of Retrieval-Augmented Generation techniques, this paper specifically addresses the challenge of generating text with proper citations, which is crucial for the academic rigor of literature surveys.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Active Retrieval Augmented Generation\",\n",
    "    \"reason\": \"Cited in the context of 'Real-time knowledge update', this paper focuses on active RAG, a more sophisticated approach to retrieval-augmented generation that could enhance the real-time knowledge update mechanism in AutoSurvey.\",\n",
    "    \"score\": 0.9\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Teaching language models to support answers with verified quotes\",\n",
    "    \"reason\": \"Cited in the context of Retrieval-Augmented Generation techniques, this paper explores how to train language models to provide answers supported by verified quotes, directly relevant to ensuring the factuality and credibility of automatically generated surveys.\",\n",
    "    \"score\": 0.8\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Progressive Generation of Long Text with Pretrained Language Models\",\n",
    "    \"reason\": \"Cited in the context of 'Long-form Text Generation', this paper presents a method for progressively generating long text, which is highly relevant to the task of automatically creating comprehensive literature surveys.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Self-Attention with Structural Position Representations\",\n",
    "    \"reason\": \"Cited in the context of extending the context window, this paper proposes a technique to enhance the self-attention mechanism for better handling of long sequences, directly addressing the context window limitations of LLMs.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Towards Coherent and Cohesive Long-form Text Generation\",\n",
    "    \"reason\": \"Cited in the context of focusing on aspects beyond linguistic coherence in LLM-based writing, this paper addresses the crucial aspects of coherence and cohesion in long-form text generation, essential for the quality of automatically generated surveys.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Self-Attention with Relative Position Representations\",\n",
    "    \"reason\": \"Cited in the context of extending the context window, this paper introduces a method for incorporating relative positional information in the self-attention mechanism, aiming to improve the processing of long sequences.\",\n",
    "    \"score\": 0.7\n",
    "  },\n",
    "  {\n",
    "    \"title\": \"Discourse-Aware Neural Rewards for Coherent Text Generation\",\n",
    "    \"reason\": \"Cited in the context of focusing on aspects beyond linguistic coherence, this paper proposes a reward mechanism to encourage discourse coherence in generated text, which is important for the logical flow of literature surveys.\",\n",
    "    \"score\": 0.7\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "config_file = \"config.toml\"\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as toml_file:\n",
    "        config_param = toml.load(toml_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file '{config_file}' not found. Please ensure it exists.\")\n",
    "    config_param = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_api_key = config_param.get('models', {}).get('llm', {}).get('api_key')\n",
    "llm_model_name = config_param.get('models', {}).get('llm', {}).get('model_name')\n",
    "embed_api_key = config_param.get('models', {}).get('embed', {}).get('api_key')\n",
    "embed_model_name = config_param.get('models', {}).get('embed', {}).get('model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "\n",
    "client = genai.Client(api_key=llm_api_key)\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=None,\n",
    "    temperature=0.6)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\", \n",
    "    contents=\"which is larger, 8.11 or 8.9?\",\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
