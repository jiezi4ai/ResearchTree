{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Tree PoC 20250313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic = \"llm literature review\"\n",
    "seed_dois = ['10.48550/arXiv.2406.10252',  # AutoSurvey: Large Language Models Can Automatically Write Surveys\n",
    "             '10.48550/arXiv.2412.10415',  # Generative Adversarial Reviews: When LLMs Become the Critic\n",
    "             '10.48550/arXiv.2402.12928',  # A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence \n",
    "             ]\n",
    "seed_titles = ['PaperRobot: Incremental Draft Generation of Scientific Ideas',\n",
    "               'From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import toml\n",
    "from typing import List, Dict, Optional, Union\n",
    "from json_repair import repair_json  # https://github.com/mangiucugna/json_repair/  # Consider pros and cons for using this 3rd party lib, and error handling when JSON repair fails\n",
    "\n",
    "# Assuming these modules are defined elsewhere as per original code structure\n",
    "from apis.s2_api import SemanticScholarKit\n",
    "from paper_metadata_process import process_paper_metadata, process_citation_metadata, process_related_metadata\n",
    "from topic_tree_gen import search_query_gen, texts_embed_gen, semantic_similarity_gen\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up basic logging (can be configured further)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_file = \"config.toml\"\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as toml_file:\n",
    "        config_param = toml.load(toml_file)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Config file '{config_file}' not found. Please ensure it exists.\")\n",
    "    config_param = {}  # Initialize to empty dict to avoid further errors, but the program might not function correctly\n",
    "\n",
    "\n",
    "class PaperExploration:\n",
    "    \"\"\"\n",
    "    A class for exploring academic papers using Semantic Scholar API and LLMs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            research_topic: Optional[str] = None,\n",
    "            seed_paper_titles: Optional[Union[List[str], str]] = None, # Revised type hint to use Union\n",
    "            seed_paper_dois: Optional[Union[List[str], str]] = None,   # Revised type hint to use Union\n",
    "            llm_api_key: Optional[str] = config_param.get('models', {}).get('llm', {}).get('api_key'), # Use .get to avoid KeyError\n",
    "            llm_model_name: Optional[str] = config_param.get('models', {}).get('llm', {}).get('model_name'), # Use .get to avoid KeyError\n",
    "            embed_api_key: Optional[str] = config_param.get('models', {}).get('embed', {}).get('api_key'), # Use .get to avoid KeyError\n",
    "            embed_model_name: Optional[str] = config_param.get('models', {}).get('embed', {}).get('model_name'), # Use .get to avoid KeyError\n",
    "            from_dt: Optional[str] = None,     # filter publish dt no earlier than\n",
    "            to_dt: Optional[str] = None,       # filter publish dt no late than\n",
    "            fields: Optional[List[str]] = None,  # list of field of study, consistent naming as 'fields' (plural)\n",
    "            min_citation_cnt: Optional[int] = 0,   # citation count no less than\n",
    "            institutions: Optional[List[str]] = None, # restricted to list of institutions, to be implemented\n",
    "            journals: Optional[List[str]] = None,     # restricted to list of journals, to be implemented\n",
    "            author_ids: Optional[List[str]] = None,   # restricted to list of authors' ids\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize PaperExploration parameters.\n",
    "        User can input research topic, seed paper title(s), or seed paper doi(s) as a starting point.\n",
    "\n",
    "        Args: # Added Args section as per feedback\n",
    "            research_topic (Optional[str]): Research topic to start exploration.\n",
    "            seed_paper_titles (Optional[Union[List[str], str]]): Seed paper titles to start exploration. Can be a single title (str) or a list of titles (List[str]).\n",
    "            seed_paper_dois (Optional[Union[List[str], str]]): Seed paper DOIs to start exploration. Can be a single DOI (str) or a list of DOIs (List[str]).\n",
    "            llm_api_key (Optional[str]): API key for LLM model. Loaded from config.toml if not provided.\n",
    "            llm_model_name (Optional[str]): Name of LLM model. Loaded from config.toml if not provided.\n",
    "            embed_api_key (Optional[str]): API key for embedding model. Loaded from config.toml if not provided.\n",
    "            embed_model_name (Optional[str]): Name of embedding model. Loaded from config.toml if not provided.\n",
    "            from_dt (Optional[str]): Filter papers published no earlier than this date (YYYY-MM-DD).\n",
    "            to_dt (Optional[str]): Filter papers published no later than this date (YYYY-MM-DD).\n",
    "            fields (Optional[List[str]]): List of fields of study to filter papers.\n",
    "            min_citation_cnt (Optional[int]): Minimum citation count for papers.\n",
    "            institutions (Optional[List[str]]): List of institutions to restrict paper search (not implemented).\n",
    "            journals (Optional[List[str]]): List of journals to restrict paper search (not implemented).\n",
    "            author_ids (Optional[List[str]]): List of author IDs to restrict paper search.\n",
    "        \"\"\"\n",
    "        self.s2 = SemanticScholarKit()\n",
    "        self.research_topic = research_topic\n",
    "        self.seed_paper_titles = [seed_paper_titles] if isinstance(seed_paper_titles, str) and seed_paper_titles else seed_paper_titles if isinstance(seed_paper_titles, list) else [] # Handle None and empty string cases more robustly\n",
    "        self.seed_paper_dois = [seed_paper_dois] if isinstance(seed_paper_dois, str) and seed_paper_dois else seed_paper_dois if isinstance(seed_paper_dois, list) else [] # Handle None and empty string cases more robustly\n",
    "\n",
    "\n",
    "        # for search result filtering\n",
    "        self.from_dt = from_dt\n",
    "        self.to_dt = to_dt\n",
    "        self.fields = fields # Consistent naming: self.fields\n",
    "        self.min_citation_cnt = min_citation_cnt\n",
    "        self.institutions = institutions\n",
    "        self.journals = journals\n",
    "        self.author_ids = author_ids\n",
    "\n",
    "        # llm\n",
    "        self.llm_api_key = llm_api_key\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.embed_api_key = embed_api_key\n",
    "        self.embed_model_name = embed_model_name\n",
    "\n",
    "        # store and process all potential papers information\n",
    "        self.nodes_json = []\n",
    "        self.edges_json = []\n",
    "\n",
    "\n",
    "    def initial_paper_query(\n",
    "            self,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,     # filter publish dt no earlier than\n",
    "            to_dt: Optional[str] = None,       # filter publish dt no late than\n",
    "            fields: Optional[List[str]] = None,  # list of field of study, consistent naming as 'fields' (plural)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve papers based on user's input text (research topic, seed paper titles/DOIs).\n",
    "\n",
    "        Args:\n",
    "            limit (Optional[int]): Maximum number of papers to retrieve per query. Defaults to 100.\n",
    "            from_dt (Optional[str]): Filter papers published no earlier than this date (YYYY-MM-DD). Overrides class-level `from_dt` if provided.\n",
    "            to_dt (Optional[str]): Filter papers published no later than this date (YYYY-MM-DD). Overrides class-level `to_dt` if provided.\n",
    "            fields (Optional[List[str]]): List of fields of study to filter papers. Overrides class-level `fields` if provided.\n",
    "        \"\"\"\n",
    "        # retrieve paper metadata from s2\n",
    "        seed_paper_metadata, searched_paper_metadata = [], [] \n",
    "        current_fields = fields if fields is not None else self.fields\n",
    "        current_from_dt = from_dt if from_dt is not None else self.from_dt \n",
    "        current_to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        if self.seed_paper_dois:\n",
    "            s2_paper_metadata = self.s2.search_paper_by_ids(id_list=self.seed_paper_dois, fields=current_fields)\n",
    "            seed_paper_metadata.extend(s2_paper_metadata)\n",
    "            time.sleep(5)\n",
    "\n",
    "        if self.seed_paper_titles and len(self.seed_paper_titles) > 0:\n",
    "            for title in self.seed_paper_titles:\n",
    "                s2_paper_metadata = self.s2.search_paper_by_keywords(query=title, fields=current_fields, limit=limit)\n",
    "                if s2_paper_metadata: # Check if s2_paper_metadata is not empty to avoid IndexError\n",
    "                    seed_paper_metadata.append(s2_paper_metadata[0])\n",
    "                    searched_paper_metadata.extend(s2_paper_metadata[1:]) \n",
    "                    time.sleep(5)\n",
    "\n",
    "        if self.research_topic:\n",
    "            s2_paper_metadata = self.s2.search_paper_by_keywords(query=self.research_topic, fields=current_fields, limit=limit)\n",
    "            searched_paper_metadata.extend(s2_paper_metadata) # Renamed 'srched_paper_metadata' to 'searched_paper_metadata'\n",
    "            time.sleep(5)\n",
    "\n",
    "        # store seed paper metadata\n",
    "        if len(seed_paper_metadata) > 0:\n",
    "            # convert to standard format, be aware the output include nodes and edges\n",
    "            seed_papermetadata_json = process_paper_metadata(\n",
    "                s2_paper_metadata=seed_paper_metadata,\n",
    "                from_dt=current_from_dt,\n",
    "                to_dt=current_to_dt,\n",
    "                fields=current_fields)\n",
    "\n",
    "            # iterate processed paper metadata, and store information to nodes_json and edge_json separately\n",
    "            # nodes need to dedup, however, edges do not need to dedup\n",
    "            node_id_pool = [x['id'] for x in self.nodes_json]\n",
    "            for item in seed_papermetadata_json:\n",
    "                if item['type'] == 'node':\n",
    "                    curr_node_id = item['id']\n",
    "                    if curr_node_id not in node_id_pool:  # for new node\n",
    "                        item['properties']['source'] = ['Seed']\n",
    "                        item['properties']['sourceDesc'] = ['Original seed papers']\n",
    "                        self.nodes_json.append(item)\n",
    "                    else:   # for existing node\n",
    "                        idx = node_id_pool.index(curr_node_id)\n",
    "                        if isinstance(self.nodes_json[idx].get('source'), list):\n",
    "                            self.nodes_json[idx]['source'].append('Seed')\n",
    "                        else:\n",
    "                            self.nodes_json[idx]['source'] = ['Seed']\n",
    "                        if isinstance(self.nodes_json[idx].get('sourceDesc'), list):\n",
    "                            self.nodes_json[idx]['sourceDesc'].append('Original seed papers')\n",
    "                        else:\n",
    "                            self.nodes_json[idx]['sourceDesc'] = ['Original seed papers']\n",
    "\n",
    "                elif item['type'] == 'relationship':\n",
    "                    # we aim to construct multigraph, which allows multiple edges between any pair of nodes\n",
    "                    self.edges_json.append(item)\n",
    "\n",
    "        # store searched paper metadata\n",
    "        if len(searched_paper_metadata) > 0: # Renamed 'srched_paper_metadata' to 'searched_paper_metadata'\n",
    "            # convert to standard format, be aware the output include nodes and edges\n",
    "            searched_papermetadata_json = process_paper_metadata( # Renamed 'srched_papermetadata_json' to 'searched_papermetadata_json'\n",
    "                s2_paper_metadata=searched_paper_metadata, # Renamed 'srched_paper_metadata' to 'searched_paper_metadata'\n",
    "                from_dt=current_from_dt,\n",
    "                to_dt=current_to_dt,\n",
    "                fields=current_fields)\n",
    "\n",
    "            # iterate processed paper metadata, and store information to nodes_json and edge_json separately\n",
    "            # nodes need to dedup, however, edges do not need to dedup\n",
    "            node_id_pool = [x['id'] for x in self.nodes_json]\n",
    "            for idx, item in enumerate(searched_papermetadata_json): # Renamed 'srched_papermetadata_json' to 'searched_papermetadata_json'\n",
    "                if item['type'] == 'node':\n",
    "                    curr_node_id = item['id']\n",
    "                    if curr_node_id not in node_id_pool:  # for new node\n",
    "                        item['properties']['source'] = ['InitialSearch']\n",
    "                        item['properties']['sourceDesc'] = ['Search from S2 based on user input']\n",
    "                        self.nodes_json.append(item)\n",
    "                    else:   # for existing node\n",
    "                        idx = node_id_pool.index(curr_node_id)\n",
    "                        if isinstance(self.nodes_json[idx].get('source'), list):\n",
    "                            self.nodes_json[idx]['source'].append('InitialSearch')\n",
    "                        else:\n",
    "                            self.nodes_json[idx]['source'] = ['InitialSearch']\n",
    "                        if isinstance(self.nodes_json[idx].get('sourceDesc'), list):\n",
    "                            self.nodes_json[idx]['sourceDesc'].append('Search from S2 based on user input')\n",
    "                        else:\n",
    "                            self.nodes_json[idx]['sourceDesc'] = ['Search from S2 based on user input']\n",
    "\n",
    "                elif item['type'] == 'relationship':\n",
    "                    # we aim to construct multigraph, which allows multiple edges between any pair of nodes\n",
    "                    self.edges_json.append(item)\n",
    "\n",
    "\n",
    "    def get_cited_papers(\n",
    "            self,\n",
    "            paper_doi,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,     # filter publish dt no earlier than\n",
    "            to_dt: Optional[str] = None,       # filter publish dt no late than\n",
    "            fields: Optional[List[str]] = None,  # list of field of study\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get papers cited by the paper with the given DOI.\n",
    "\n",
    "        Args:\n",
    "            paper_doi (str): DOI of the paper to find cited papers for.\n",
    "            limit (Optional[int]): Maximum number of cited papers to retrieve. Defaults to 100.\n",
    "            from_dt (Optional[str]): Filter papers published no earlier than this date (YYYY-MM-DD). Overrides class-level `from_dt` if provided.\n",
    "            to_dt (Optional[str]): Filter papers published no later than this date (YYYY-MM-DD). Overrides class-level `to_dt` if provided.\n",
    "            fields (Optional[List[str]]): List of fields of study to filter papers. Overrides class-level `fields` if provided.\n",
    "        \"\"\"\n",
    "        # retrieve cited paper metadata from s2\n",
    "        current_fields = fields if fields is not None else self.fields \n",
    "        current_from_dt = from_dt if from_dt is not None else self.from_dt \n",
    "        current_to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        s2_citedpaper_metadata = self.s2.get_s2_cited_papers(paper_doi, fields=current_fields, limit=limit)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # convert to standard format, be aware the output include nodes and edges\n",
    "        s2_citedpapermeta_json = process_citation_metadata(\n",
    "            original_paper_doi=paper_doi,\n",
    "            s2_citation_metadata=s2_citedpaper_metadata,\n",
    "            citation_type='citedPaper',\n",
    "            from_dt=current_from_dt,\n",
    "            to_dt=current_to_dt,\n",
    "            fields=current_fields)\n",
    "\n",
    "        # iterate processed paper metadata, and store information to nodes_json and edge_json separately\n",
    "        # nodes need to dedup, however, edges do not need to dedup\n",
    "        node_id_pool = [x['id'] for x in self.nodes_json]\n",
    "        for item in s2_citedpapermeta_json:\n",
    "            if item['type'] == 'node':\n",
    "                curr_node_id = item['id']\n",
    "                if curr_node_id not in node_id_pool:  # for new node\n",
    "                    item['properties']['source'] = ['CitedPaper']\n",
    "                    item['properties']['sourceDesc'] = [f'cited by {paper_doi}']\n",
    "                    self.nodes_json.append(item)\n",
    "                else:   # for existing node\n",
    "                    idx = node_id_pool.index(curr_node_id)\n",
    "                    if isinstance(self.nodes_json[idx].get('source'), list):\n",
    "                        self.nodes_json[idx]['source'].append('CitedPaper')\n",
    "                    else:\n",
    "                        self.nodes_json[idx]['source'] = ['CitedPaper']\n",
    "                    if isinstance(self.nodes_json[idx].get('sourceDesc'), list):\n",
    "                        self.nodes_json[idx]['sourceDesc'].append(f'cited by {paper_doi}')\n",
    "                    else:\n",
    "                        self.nodes_json[idx]['sourceDesc'] = [f'cited by {paper_doi}']\n",
    "\n",
    "            elif item['type'] == 'relationship':\n",
    "                # we aim to construct multigraph, which allows multiple edges between any pair of nodes\n",
    "                self.edges_json.append(item)\n",
    "\n",
    "\n",
    "    def get_citing_papers(\n",
    "            self,\n",
    "            paper_doi,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,     # filter publish dt no earlier than\n",
    "            to_dt: Optional[str] = None,       # filter publish dt no late than\n",
    "            fields: Optional[List[str]] = None,  # list of field of study\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve papers that cite the paper with the given DOI.\n",
    "\n",
    "        Args:\n",
    "            paper_doi (str): DOI of the paper to find citing papers for.\n",
    "            limit (Optional[int]): Maximum number of citing papers to retrieve. Defaults to 100.\n",
    "            from_dt (Optional[str]): Filter papers published no earlier than this date (YYYY-MM-DD). Overrides class-level `from_dt` if provided.\n",
    "            to_dt (Optional[str]): Filter papers published no later than this date (YYYY-MM-DD). Overrides class-level `to_dt` if provided.\n",
    "            fields (Optional[List[str]]): List of fields of study to filter papers. Overrides class-level `fields` if provided.\n",
    "        \"\"\"\n",
    "        # retrieve citing paper metadata from s2\n",
    "        current_fields = fields if fields is not None else self.fields \n",
    "        current_from_dt = from_dt if from_dt is not None else self.from_dt \n",
    "        current_to_dt = to_dt if to_dt is not None else self.to_dt \n",
    "\n",
    "        s2_citingpaper_metadata = self.s2.get_s2_citing_papers(paper_doi, fields=current_fields, limit=limit)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # convert to standard format, be aware the output include nodes and edges\n",
    "        s2_citingpapermetadata_json = process_citation_metadata(\n",
    "            original_paper_doi=paper_doi,\n",
    "            s2_citation_metadata=s2_citingpaper_metadata,\n",
    "            citation_type='citingPaper',\n",
    "            from_dt=current_from_dt,\n",
    "            to_dt=current_to_dt,\n",
    "            fields=current_fields)\n",
    "\n",
    "        # iterate processed paper metadata, and store information to nodes_json and edge_json separately\n",
    "        # nodes need to dedup, however, edges do not need to dedup\n",
    "        node_id_pool = [x['id'] for x in self.nodes_json]\n",
    "        for item in s2_citingpapermetadata_json:\n",
    "            if item['type'] == 'node':\n",
    "                curr_node_id = item['id']\n",
    "                if curr_node_id not in node_id_pool:  # for new node\n",
    "                    item['properties']['source'] = ['CitingPaper']\n",
    "                    item['properties']['sourceDesc'] = [f\"citing {paper_doi}\"]\n",
    "                    self.nodes_json.append(item)\n",
    "                else:   # for existing node\n",
    "                    idx = node_id_pool.index(curr_node_id)\n",
    "                    if isinstance(self.nodes_json[idx].get('source'), list):\n",
    "                        self.nodes_json[idx]['source'].append('CitingPaper')\n",
    "                    else:\n",
    "                        self.nodes_json[idx]['source'] = ['CitingPaper']\n",
    "                    if isinstance(self.nodes_json[idx].get('sourceDesc'), list):\n",
    "                        self.nodes_json[idx]['sourceDesc'].append(f\"citing {paper_doi}\")\n",
    "                    else:\n",
    "                        self.nodes_json[idx]['sourceDesc'] = [f\"citing {paper_doi}\"]\n",
    "\n",
    "            elif item['type'] == 'relationship':\n",
    "                # we aim to construct multigraph, which allows multiple edges between any pair of nodes\n",
    "                self.edges_json.append(item)\n",
    "\n",
    "\n",
    "    def get_recommend_papers(\n",
    "            self,\n",
    "            paper_dois: Union[List[str], str], # Revised type hint to use Union\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,     # filter publish dt no earlier than\n",
    "            to_dt: Optional[str] = None,       # filter publish dt no late than\n",
    "            fields: Optional[List[str]] = None,  # list of field of study\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve papers recommended by Semantic Scholar based on a list of paper DOIs.\n",
    "\n",
    "        Args:\n",
    "            paper_dois (Union[List[str], str]): DOI(s) of papers to get recommendations for. Can be a single DOI (str) or a list of DOIs (List[str]).\n",
    "            limit (Optional[int]): Maximum number of recommended papers to retrieve. Defaults to 100.\n",
    "            from_dt (Optional[str]): Filter papers published no earlier than this date (YYYY-MM-DD). Overrides class-level `from_dt` if provided.\n",
    "            to_dt (Optional[str]): Filter papers published no later than this date (YYYY-MM-DD). Overrides class-level `to_dt` if provided.\n",
    "            fields (Optional[List[str]]): List of fields of study to filter papers. Overrides class-level `fields` if provided.\n",
    "        \"\"\"\n",
    "        # retrieve recommended paper metadata from s2\n",
    "        current_fields = fields if fields is not None else self.fields # Use provided fields if available, otherwise use class-level fields\n",
    "        current_from_dt = from_dt if from_dt is not None else self.from_dt # Use provided from_dt if available, otherwise use class-level from_dt\n",
    "        current_to_dt = to_dt if to_dt is not None else self.to_dt # Use provided to_dt if available, otherwise use class-level to_dt\n",
    "\n",
    "        if isinstance(paper_dois, str):\n",
    "            paper_dois = [paper_dois]\n",
    "        s2_recommended_metadata = self.s2.get_s2_recommended_papers(positive_paper_ids=paper_dois, fields=current_fields, limit=limit)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # convert to standard format, be aware the output include nodes and edges\n",
    "        s2_recpapermetadata_json = process_paper_metadata(\n",
    "            s2_paper_metadata=s2_recommended_metadata,\n",
    "            from_dt=current_from_dt,\n",
    "            to_dt=current_to_dt,\n",
    "            fields=current_fields)\n",
    "\n",
    "        # iterate processed paper metadata, and store information to nodes_json and edge_json separately\n",
    "        # nodes need to dedup, however, edges do not need to dedup\n",
    "        node_id_pool = [x['id'] for x in self.nodes_json]\n",
    "        for item in s2_recpapermetadata_json:\n",
    "\n",
    "            if item['type'] == 'node':\n",
    "                curr_node_id = item['id']\n",
    "                if curr_node_id not in node_id_pool:  # for new node\n",
    "                    item['properties']['source'] = ['RecommendedPaper']\n",
    "                    item['properties']['sourceDesc'] = [f\"recommended by s2 given papers {','.join(paper_dois)}\"] # Corrected typo \"recomend\" to \"recommended\"\n",
    "                    self.nodes_json.append(item)\n",
    "                else:   # for existing node\n",
    "                    idx = node_id_pool.index(curr_node_id)\n",
    "                    if isinstance(self.nodes_json[idx].get('source'), list):\n",
    "                        self.nodes_json[idx]['source'].append('RecommendedPaper')\n",
    "                    else:\n",
    "                        self.nodes_json[idx]['source'] = ['RecommendedPaper']\n",
    "                    if isinstance(self.nodes_json[idx].get('sourceDesc'), list):\n",
    "                        self.nodes_json[idx]['sourceDesc'].append(f\"recommended by s2 given papers {','.join(paper_dois)}\") # Corrected typo \"recomend\" to \"recommended\"\n",
    "                    else:\n",
    "                        self.nodes_json[idx]['sourceDesc'] = [f\"recommended by s2 given papers {','.join(paper_dois)}\"] # Corrected typo \"recomend\" to \"recommended\"\n",
    "\n",
    "            elif item['type'] == 'relationship':\n",
    "                # we aim to construct multigraph, which allows multiple edges between any pair of nodes\n",
    "                self.edges_json.append(item)\n",
    "\n",
    "\n",
    "    def get_related_papers(\n",
    "            self,\n",
    "            domain,\n",
    "            input_text,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,     # filter publish dt no earlier than\n",
    "            to_dt: Optional[str] = None,       # filter publish dt no late than\n",
    "            fields: Optional[List[str]] = None,  # list of field of study\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Further expand paper scope by leveraging deep search with LLMs to generate related search queries.\n",
    "\n",
    "        Args:\n",
    "            domain (str): Research domain or area. Used by LLM to generate relevant queries.\n",
    "            input_text (str): User input text related to the research topic. Used by LLM to generate relevant queries.\n",
    "            limit (Optional[int]): Maximum number of papers to retrieve per query. Defaults to 100.\n",
    "            from_dt (Optional[str]): Filter papers published no earlier than this date (YYYY-MM-DD). Overrides class-level `from_dt` if provided.\n",
    "            to_dt (Optional[str]): Filter papers published no later than this date (YYYY-MM-DD). Overrides class-level `to_dt` if provided.\n",
    "            fields (Optional[List[str]]): List of fields of study to filter papers. Overrides class-level `fields` if provided.\n",
    "        \"\"\"\n",
    "        # retrieve related papers using LLM-generated queries\n",
    "        current_fields = fields if fields is not None else self.fields \n",
    "        current_from_dt = from_dt if from_dt is not None else self.from_dt \n",
    "        current_to_dt = to_dt if to_dt is not None else self.to_dt \n",
    "\n",
    "        # llm propose search queries\n",
    "        keywords_topics_info = search_query_gen(domain, input_text, self.llm_api_key, self.llm_model_name)\n",
    "        # extract keywords, topics, queries\n",
    "        try:\n",
    "            keywords_topics_json = json.loads(repair_json(keywords_topics_info)) # Use try-except to handle potential JSON repair/parsing errors\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON Repair or Decode Error: {e}. Original LLM output: {keywords_topics_info}\")\n",
    "            keywords_topics_json = {} # Initialize to empty dict to avoid further errors, handle gracefully later if needed\n",
    "        print(keywords_topics_json)\n",
    "\n",
    "        field_of_study = keywords_topics_json.get('field_of_study')\n",
    "        keywords_and_topics = keywords_topics_json.get('keywords_and_topics')\n",
    "        tags = keywords_topics_json.get('tags')\n",
    "        queries = keywords_topics_json.get('queries')\n",
    "\n",
    "        if queries: # Check if queries is not None and not empty list to avoid errors\n",
    "            for query in queries:\n",
    "                s2_paper_metadata = self.s2.search_paper_by_keywords(query, fields=current_fields, limit=limit)\n",
    "                time.sleep(5)\n",
    "\n",
    "                # convert to standard format, be aware the output include nodes and edges\n",
    "                s2_papermeta_json = process_related_metadata(\n",
    "                    s2_related_metadata=s2_paper_metadata,\n",
    "                    from_dt=current_from_dt,\n",
    "                    to_dt=current_to_dt,\n",
    "                    fields=current_fields)\n",
    "\n",
    "                # iterate processed paper metadata, and store information to nodes_json and edge_json separately\n",
    "                # nodes need to dedup, however, edges do not need to dedup\n",
    "                node_id_pool = [x['id'] for x in self.nodes_json]\n",
    "                for item in s2_papermeta_json:\n",
    "                    if item['type'] == 'node':\n",
    "                        curr_node_id = item['id']\n",
    "                        if curr_node_id not in node_id_pool:  # for new node\n",
    "                            item['properties']['source'] = ['LLMQuery']\n",
    "                            item['properties']['sourceDesc'] = [query]\n",
    "                            self.nodes_json.append(item)\n",
    "                        else:   # for existing node\n",
    "                            idx = node_id_pool.index(curr_node_id)\n",
    "                            if isinstance(self.nodes_json[idx].get('source'), list):\n",
    "                                self.nodes_json[idx]['source'].append('LLMQuery')\n",
    "                            else:\n",
    "                                self.nodes_json[idx]['source'] = ['LLMQuery']\n",
    "                            if isinstance(self.nodes_json[idx].get('sourceDesc'), list):\n",
    "                                self.nodes_json[idx]['sourceDesc'].append(query)\n",
    "                            else:\n",
    "                                self.nodes_json[idx]['sourceDesc'] = [query]\n",
    "\n",
    "                    elif item['type'] == 'relationship':\n",
    "                        # we aim to construct multigraph, which allows multiple edges between any pair of nodes\n",
    "                        self.edges_json.append(item)\n",
    "\n",
    "\n",
    "    async def add_semantic_relationship(self, paper_nodes_json):\n",
    "        \"\"\"\n",
    "        Add semantic similarity relationships (edges) between paper nodes based on title and abstract.\n",
    "        This method calculates semantic similarity using embeddings generated by 'texts_embed_gen' and\n",
    "        adds 'SIMILAR_TO' relationships with 'weight' property representing the similarity score.\n",
    "        \"\"\"\n",
    "        # existing edge ids\n",
    "        edges_id_pool = [(x['startNodeId'], x['endNodeId']) for x in self.edges_json]\n",
    "\n",
    "        # first extract title and abstract from paper nodes json\n",
    "        ids, texts = [], []\n",
    "        for node in paper_nodes_json:\n",
    "            id = node['id']\n",
    "            title = node['properties'].get('title')\n",
    "            abstract = node['properties'].get('abstract')\n",
    "            if title is not None and abstract is not None:\n",
    "                texts.append(f\"TITLE: {title} \\nABSTRACT: {abstract}\")\n",
    "                ids.append(id)\n",
    "\n",
    "        if not texts: # Handle case where no texts are available for embedding\n",
    "            logging.warning(\"No paper titles and abstracts found for semantic similarity calculation.\")\n",
    "            return\n",
    "\n",
    "        # then calculate semantic similarities between the texts\n",
    "        embeds = await texts_embed_gen(texts, self.embed_api_key, self.embed_model_name) # Assuming texts_embed_gen is an async function for IO-bound operations\n",
    "\n",
    "        # calculate similarity matrix\n",
    "        sim_matrix = semantic_similarity_gen(embeds, embeds)\n",
    "\n",
    "        # iterate similarity matrix to add similarity relationships\n",
    "        rows, cols = sim_matrix.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                sim = sim_matrix[i, j]\n",
    "                if i != j:\n",
    "                    # if edge exist, then update weight\n",
    "                    if (ids[i], ids[j]) in edges_id_pool:\n",
    "                        pos = edges_id_pool.index((ids[i], ids[j]))\n",
    "                        self.edges_json[pos]['properties']['weight'] = round(sim, 4)\n",
    "                    # if not exit, then generate new edge\n",
    "                    else:\n",
    "                        edge = {\n",
    "                            \"type\": \"relationship\",\n",
    "                            \"relationshipType\": \"SIMILAR_TO\",\n",
    "                            \"startNodeId\": ids[i],\n",
    "                            \"endNodeId\": ids[j],\n",
    "                            \"properties\": {'source': 'semantic similarity', 'weight': round(sim, 4)}\n",
    "                            }\n",
    "                        self.edges_json.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperbot = PaperExploration(seed_paper_dois=seed_dois[0])\n",
    "# paperbot = PaperExploration(research_topic=research_topic, seed_paper_dois=seed_dois, seed_paper_titles=seed_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get initial papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 11:55:39,286 - INFO - HTTP Request: POST https://api.semanticscholar.org/graph/v1/paper/batch?fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear \"HTTP/1.1 429 \"\n",
      "2025-03-19 11:56:15,642 - INFO - HTTP Request: POST https://api.semanticscholar.org/graph/v1/paper/batch?fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear \"HTTP/1.1 429 \"\n",
      "2025-03-19 11:56:47,933 - INFO - HTTP Request: POST https://api.semanticscholar.org/graph/v1/paper/batch?fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "paperbot.initial_paper_query(limit=50, from_dt='2022-01-01', to_dt='2025-03-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_paper = [x for x in paperbot.nodes_json if x['labels'] == [\"Paper\"] and 'Seed' in x['properties']['source']]\n",
    "init_paper_dois = [x['id'] for x in init_paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.48550/arXiv.2406.10252']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_paper_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 11\n"
     ]
    }
   ],
   "source": [
    "print(len(paperbot.nodes_json), len(paperbot.edges_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search Citation Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_doi in init_paper_dois:\n",
    "    paperbot.get_cited_papers(paper_doi) \n",
    "    time.sleep(5)\n",
    "    paperbot.get_citing_papers(paper_doi) \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(paperbot.nodes_json), len(paperbot.edges_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperbot.get_recommend_papers(paper_dois=init_paper_dois, from_dt='2022-01-01', to_dt='2025-03-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains, init_paper_info = [], []\n",
    "for item in init_paper:\n",
    "    title = item.get('properties',{}).get('title')\n",
    "    abstract = item.get('properties',{}).get('abstract')\n",
    "    domain = item.get('properties',{}).get('fieldsOfStudy')\n",
    "    info = f\"<paper> TITLE: {title}\\nABSTRACT: {abstract} </paper>\"\n",
    "    init_paper_info.append(info)\n",
    "    domains.extend(domain)\n",
    "\n",
    "from collections import Counter\n",
    "domain = Counter(domains).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 11:57:03,376 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-03-19 11:57:05,742 - INFO - AFC remote call 1 is done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'field_of_study': ['Natural Language Processing', 'Information Retrieval', 'Artificial Intelligence', 'Computational Linguistics'], 'keywords_and_topics': ['literature survey automation', 'large language models (LLMs)', 'AutoSurvey', 'context window limitations', 'evaluation benchmarks'], 'tags': ['survey paper generation', 'parametric knowledge', 'systematic approach', 'AI research', 'LLM applications'], 'queries': ['LLM for literature review', 'automated survey generation AI', 'large language models research survey', 'AutoSurvey evaluation methods']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 11:57:08,208 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=LLM+for+literature+review&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-19 11:57:16,193 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=automated+survey+generation+AI&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-03-19 11:57:47,643 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=automated+survey+generation+AI&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-03-19 11:58:20,098 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=automated+survey+generation+AI&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-19 11:58:29,922 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=large+language+models+research+survey&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-03-19 11:59:03,660 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=large+language+models+research+survey&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n",
      "2025-03-19 11:59:11,588 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=AutoSurvey+evaluation+methods&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-03-19 11:59:44,060 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=AutoSurvey+evaluation+methods&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "paperbot.get_related_papers(domain, input_text=\"\\n\".join(init_paper_info), from_dt='2022-01-01', to_dt='2025-03-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_nodes_json = [x for x in paperbot.nodes_json if x['labels'] == [\"Paper\"] ]\n",
    "await paperbot.add_semantic_relationship(paper_nodes_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
