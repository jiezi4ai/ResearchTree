{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jiezi/Code/GitHub/ResearchTree/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(parent_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional, Union, Tuple, Any\n",
    "from json_repair import repair_json\n",
    "from aiolimiter import AsyncLimiter\n",
    "\n",
    "from utils.data_process import generate_hash_key\n",
    "from apis.s2_api import SemanticScholarKit\n",
    "from models.llms import async_llm_gen_w_retry\n",
    "from prompts.query_prompt import keywords_topics_example, keywords_topics_prompt\n",
    "from models.embedding_models import gemini_embedding_async, semantic_similarity_matrix\n",
    "from graph.s2_metadata_process import process_paper_metadata, process_citation_metadata, process_related_metadata, process_author_metadata\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Constants ---\n",
    "# Default S2 API rate limit guidance: 100 requests per 5 minutes (~0.33 req/sec)\n",
    "# Default interval ensures roughly 1 request every 4 seconds (0.25 req/sec)\n",
    "DEFAULT_S2_REQUEST_INTERVAL = 4\n",
    "# Default max concurrent requests allowed to S2 API\n",
    "DEFAULT_S2_MAX_CONCURRENT = 1\n",
    "\n",
    "class PaperSearch:\n",
    "    \"\"\"\n",
    "    A class for exploring academic papers using Semantic Scholar API and LLMs,\n",
    "    optimized for asynchronous operations with controlled concurrency and rate limiting.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            research_topic: Optional[str] = None,\n",
    "            seed_paper_titles: Optional[Union[List[str], str]] = None,\n",
    "            seed_paper_dois: Optional[Union[List[str], str]] = None,\n",
    "            llm_api_key: Optional[str] = None,\n",
    "            llm_model_name: Optional[str] = None,\n",
    "            embed_api_key: Optional[str] = None,\n",
    "            embed_model_name: Optional[str] = None,\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields_of_study: Optional[List[str]] = None,\n",
    "            min_citation_cnt: Optional[int] = 0,\n",
    "            institutions: Optional[List[str]] = None,\n",
    "            journals: Optional[List[str]] = None,\n",
    "            author_ids: Optional[List[str]] = None,\n",
    "            # --- S2 API Control Parameters ---\n",
    "            s2_request_interval: float = DEFAULT_S2_REQUEST_INTERVAL, # Avg seconds between requests\n",
    "            s2_max_concurrent: int = DEFAULT_S2_MAX_CONCURRENT, # Max simultaneous requests\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize PaperExploration parameters.\n",
    "        (Args documentation omitted for brevity, same as original)\n",
    "        New Args:\n",
    "            s2_request_interval (float): Target average interval (seconds) between S2 API requests for rate limiting. Defaults to 4.\n",
    "            s2_max_concurrent (int): Maximum number of concurrent requests allowed to the S2 API. Defaults to 5.\n",
    "        \"\"\"\n",
    "        # set up SemanticScholarKit\n",
    "        self.s2 = SemanticScholarKit()\n",
    "        self.research_topic = research_topic\n",
    "        self.seed_paper_titles = [seed_paper_titles] if isinstance(seed_paper_titles, str) and seed_paper_titles else seed_paper_titles if isinstance(seed_paper_titles, list) else []\n",
    "        self.seed_paper_dois = [seed_paper_dois] if isinstance(seed_paper_dois, str) and seed_paper_dois else seed_paper_dois if isinstance(seed_paper_dois, list) else []\n",
    "\n",
    "        # Filters\n",
    "        self.from_dt = from_dt\n",
    "        self.to_dt = to_dt\n",
    "        self.fields_of_study = fields_of_study\n",
    "        self.min_citation_cnt = min_citation_cnt\n",
    "        self.institutions = institutions\n",
    "        self.journals = journals\n",
    "        self.author_ids = author_ids\n",
    "\n",
    "        # LLM/Embedding (Using async versions now)\n",
    "        self.llm_api_key = llm_api_key\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.embed_api_key = embed_api_key\n",
    "        self.embed_model_name = embed_model_name\n",
    "\n",
    "        # State: Nodes and Edges\n",
    "        self.nodes_json: List[Dict] = []\n",
    "        self.edges_json: List[Dict] = []\n",
    "        self._node_ids = set()\n",
    "        self._edge_tuples = set()\n",
    "\n",
    "        # --- Async specific ---\n",
    "        if s2_max_concurrent <= 0:\n",
    "            raise ValueError(\"s2_max_concurrent must be positive\")\n",
    "        if s2_request_interval <= 0:\n",
    "             raise ValueError(\"s2_request_interval must be positive\")\n",
    "\n",
    "        # Semaphore to limit MAX CONCURRENT requests\n",
    "        self._s2_semaphore = asyncio.Semaphore(s2_max_concurrent)\n",
    "\n",
    "        # Rate limiter (aiolimiter) to control requests PER SECOND on average\n",
    "        # Rate = 1 request per s2_request_interval seconds\n",
    "        rate_limit = 1.0 / s2_request_interval\n",
    "        self._s2_rate_limiter = AsyncLimiter(rate_limit, 1.0) # Max rate, per 1 second\n",
    "\n",
    "        logging.info(f\"S2 API Control: Max Concurrent={s2_max_concurrent}, Avg Interval={s2_request_interval:.2f}s (Rate Limit ~{rate_limit:.2f}/sec)\")\n",
    "\n",
    "\n",
    "    async def _s2_api_call_wrapper(self, coro):\n",
    "        \"\"\"\n",
    "        Wrapper for S2 API calls to enforce both concurrency limit and rate limit.\n",
    "        Acquires rate limiter first, then concurrency semaphore.\n",
    "        \"\"\"\n",
    "        async with self._s2_rate_limiter: # Wait until allowed by rate limit\n",
    "            async with self._s2_semaphore: # Wait for an available concurrency slot\n",
    "                # Once both are acquired, execute the actual S2 call coroutine\n",
    "                # No manual sleep needed here.\n",
    "                result = await coro\n",
    "                return result\n",
    "        # Semaphore and rate limit allowance are released automatically upon exiting 'async with' blocks\n",
    "\n",
    "    def _add_items_to_graph(self, items: List[Dict]):\n",
    "        \"\"\"Adds nodes and relationships from processed data, avoiding duplicates.\"\"\"\n",
    "        nodes_added = 0\n",
    "        edges_added = 0\n",
    "        for item in items:\n",
    "            if not isinstance(item, dict): # Basic check\n",
    "                 logging.warning(f\"Skipping non-dict item during graph update: {type(item)}\")\n",
    "                 continue\n",
    "\n",
    "            item_type = item.get('type')\n",
    "            if item_type == 'node':\n",
    "                node_id = item.get('id')\n",
    "                if node_id and node_id not in self._node_ids:\n",
    "                    self.nodes_json.append(item)\n",
    "                    self._node_ids.add(node_id)\n",
    "                    nodes_added += 1\n",
    "            elif item_type == 'relationship':\n",
    "                rel_type = item.get('relationshipType')\n",
    "                start_id = item.get('startNodeId')\n",
    "                end_id = item.get('endNodeId')\n",
    "                if rel_type and start_id and end_id:\n",
    "                    # Use a canonical tuple (sorted IDs for undirected like SIMILAR_TO)\n",
    "                    # Keep order for directed (e.g., CITES, AUTHOR_OF)\n",
    "                    # Assuming SIMILAR_TO is undirected for canonical check\n",
    "                    if rel_type == \"SIMILAR_TO\":\n",
    "                         edge_tuple_key = tuple(sorted((start_id, end_id))) + (rel_type,)\n",
    "                    else:\n",
    "                         edge_tuple_key = (start_id, end_id, rel_type)\n",
    "\n",
    "                    if edge_tuple_key not in self._edge_tuples:\n",
    "                        self.edges_json.append(item)\n",
    "                        self._edge_tuples.add(edge_tuple_key)\n",
    "                        edges_added += 1\n",
    "            # else: ignore items without 'type' or with unknown 'type'\n",
    "\n",
    "        if nodes_added > 0 or edges_added > 0:\n",
    "             logging.debug(f\"Added {nodes_added} nodes, {edges_added} edges.\")\n",
    "\n",
    "\n",
    "    async def initial_paper_query(\n",
    "            self,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields_of_study: Optional[List[str]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve papers based on user's input text asynchronously.\n",
    "        (Uses the _s2_api_call_wrapper for concurrency/rate limits)\n",
    "        \"\"\"\n",
    "        tasks = []\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "        from_dt = from_dt if from_dt is not None else self.from_dt\n",
    "        to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        # Task for searching by DOIs\n",
    "        if self.seed_paper_dois:\n",
    "            async def _fetch_by_dois():\n",
    "                logging.info(f\"Fetching papers by {len(self.seed_paper_dois)} DOIs...\")\n",
    "                # Use the wrapper for the S2 call\n",
    "                s2_paper_metadata = await self._s2_api_call_wrapper(\n",
    "                    self.s2.search_paper_by_ids(id_list=self.seed_paper_dois)\n",
    "                )\n",
    "                logging.info(f\"Processing {len(s2_paper_metadata)} papers from DOIs.\")\n",
    "                processed = process_paper_metadata(s2_paper_metadata, from_dt, to_dt, fields_of_study)\n",
    "                for item in processed:\n",
    "                     if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                         item.setdefault('properties', {})['from_seed'] = True\n",
    "                         item['properties']['is_complete'] = True\n",
    "                return processed, []\n",
    "            tasks.append(_fetch_by_dois())\n",
    "\n",
    "        # Tasks for searching by Titles\n",
    "        if self.seed_paper_titles:\n",
    "            async def _fetch_by_title(title):\n",
    "                logging.info(f\"Fetching papers by title: '{title[:50]}...'\")\n",
    "                # Use the wrapper for the S2 call\n",
    "                s2_paper_metadata = await self._s2_api_call_wrapper(\n",
    "                    self.s2.search_paper_by_keywords(query=title, fields_of_study=fields_of_study, limit=limit)\n",
    "                )\n",
    "                seed_meta, searched_meta = [], []\n",
    "                if s2_paper_metadata:\n",
    "                    seed_meta.append(s2_paper_metadata[0])\n",
    "                    searched_meta.extend(s2_paper_metadata[1:])\n",
    "                logging.info(f\"Processing {len(seed_meta)} seed and {len(searched_meta)} searched papers for title '{title[:50]}...'\")\n",
    "                processed_seed = process_paper_metadata(seed_meta, from_dt, to_dt, fields_of_study)\n",
    "                processed_searched = process_paper_metadata(searched_meta, from_dt, to_dt, fields_of_study)\n",
    "                for item in processed_seed:\n",
    "                     if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                         item.setdefault('properties', {})['from_seed'] = True\n",
    "                         item['properties']['from_title_search'] = True\n",
    "                         item['properties']['is_complete'] = True\n",
    "                for item in processed_searched:\n",
    "                     if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                         item.setdefault('properties', {})['from_title_search'] = True\n",
    "                         item['properties']['is_complete'] = True\n",
    "                return processed_seed, processed_searched\n",
    "\n",
    "            for title in self.seed_paper_titles:\n",
    "                 tasks.append(_fetch_by_title(title))\n",
    "\n",
    "\n",
    "        # Task for searching by Research Topic\n",
    "        if self.research_topic:\n",
    "            async def _fetch_by_topic():\n",
    "                logging.info(f\"Fetching papers by topic: '{self.research_topic[:50]}...'\")\n",
    "                # Use the wrapper for the S2 call\n",
    "                s2_paper_metadata = await self._s2_api_call_wrapper(\n",
    "                    self.s2.search_paper_by_keywords(query=self.research_topic, fields=fields_of_study, limit=limit)\n",
    "                )\n",
    "                logging.info(f\"Processing {len(s2_paper_metadata)} papers from topic search.\")\n",
    "                processed = process_paper_metadata(s2_paper_metadata, from_dt, to_dt, fields_of_study)\n",
    "                for item in processed:\n",
    "                     if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                         item.setdefault('properties', {})['from_topic_search'] = True\n",
    "                         item['properties']['is_complete'] = True\n",
    "                return [], processed\n",
    "            tasks.append(_fetch_by_topic())\n",
    "\n",
    "        # Run all initial query tasks concurrently\n",
    "        all_results = []\n",
    "        if tasks:\n",
    "            logging.info(f\"Running {len(tasks)} initial query tasks concurrently...\")\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            all_results.extend(results)\n",
    "        else:\n",
    "            logging.warning(\"No initial query criteria (DOI, Title, Topic) provided.\")\n",
    "            return\n",
    "\n",
    "        # Process results and add to graph state\n",
    "        logging.info(\"Aggregating results from initial queries...\")\n",
    "        aggregated_items = []\n",
    "        for result in all_results:\n",
    "            if isinstance(result, Exception):\n",
    "                logging.error(f\"Initial query task failed: {result}\")\n",
    "            elif isinstance(result, tuple) and len(result) == 2:\n",
    "                seed_items, searched_items = result\n",
    "                aggregated_items.extend(seed_items)\n",
    "                aggregated_items.extend(searched_items)\n",
    "            else:\n",
    "                logging.warning(f\"Unexpected result type from initial query task: {type(result)}\")\n",
    "\n",
    "        self._add_items_to_graph(aggregated_items)\n",
    "        logging.info(f\"Initial query complete. Nodes: {len(self.nodes_json)}, Edges: {len(self.edges_json)}\")\n",
    "\n",
    "\n",
    "    async def get_author_info(\n",
    "            self,\n",
    "            author_ids: List[str],\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields_of_study: Optional[List[str]] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Fetches and processes author information asynchronously.\"\"\"\n",
    "        if not author_ids: return []\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "        from_dt = from_dt if from_dt is not None else self.from_dt\n",
    "        to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        logging.info(f\"Fetching info for {len(author_ids)} authors...\")\n",
    "        # Use the wrapper for the S2 call\n",
    "        authors_info = await self._s2_api_call_wrapper(\n",
    "            self.s2.search_author_by_ids(author_ids=author_ids, with_abstract=True )\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Processing metadata for {len(authors_info)} authors.\")\n",
    "        s2_author_meta_json = process_author_metadata(\n",
    "            s2_author_metadata=authors_info,\n",
    "            from_dt=from_dt,\n",
    "            to_dt=to_dt,\n",
    "            fields_of_study=fields_of_study \n",
    "        )\n",
    "\n",
    "        for item in s2_author_meta_json:\n",
    "            if item.get('type') == 'node':\n",
    "                item.setdefault('properties', {})\n",
    "                if item.get('labels') == ['Paper']:\n",
    "                    item['properties']['from_same_author'] = True\n",
    "                    item['properties']['is_complete'] = True\n",
    "                elif item.get('labels') == ['Author']:\n",
    "                    item['properties']['is_complete'] = True\n",
    "        return s2_author_meta_json\n",
    "\n",
    "\n",
    "    async def get_cited_papers( # References\n",
    "            self,\n",
    "            paper_doi: str,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields_of_study: Optional[List[str]] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Get papers cited by the paper (references) asynchronously.\"\"\"\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "        from_dt = from_dt if from_dt is not None else self.from_dt\n",
    "        to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        logging.info(f\"Fetching papers cited by {paper_doi}...\")\n",
    "         # Use the wrapper for the S2 call\n",
    "        s2_citedpaper_metadata_raw = await self._s2_api_call_wrapper(\n",
    "            self.s2.get_s2_cited_papers( # This calls get_paper_references in kit\n",
    "                paper_id=paper_doi,\n",
    "                limit=limit,\n",
    "                with_abstract=True # Let the kit handle fetching missing abstracts\n",
    "                )\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Processing {len(s2_citedpaper_metadata_raw)} cited papers (references) for {paper_doi}.\")\n",
    "        s2_citedpapermeta_json = process_citation_metadata(\n",
    "            original_paper_doi=paper_doi,\n",
    "            s2_citation_metadata=s2_citedpaper_metadata_raw, # Pass the raw result from S2 kit\n",
    "            citation_type='citedPaper', # Reference (paper cites this)\n",
    "            from_dt=from_dt,\n",
    "            to_dt=to_dt,\n",
    "            fields_of_study=fields_of_study \n",
    "        )\n",
    "\n",
    "        for item in s2_citedpapermeta_json:\n",
    "            if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                item.setdefault('properties', {})['from_reference'] = True\n",
    "                item['properties']['is_complete'] = True\n",
    "        return s2_citedpapermeta_json\n",
    "\n",
    "\n",
    "    async def get_citing_papers( # Citations\n",
    "            self,\n",
    "            paper_doi: str,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields: Optional[List[str]] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Retrieve papers that cite the paper asynchronously.\"\"\"\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "        from_dt = from_dt if from_dt is not None else self.from_dt\n",
    "        to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        logging.info(f\"Fetching papers citing {paper_doi}...\")\n",
    "        # Use the wrapper for the S2 call\n",
    "        s2_citingpaper_metadata_raw = await self._s2_api_call_wrapper(\n",
    "            self.s2.get_s2_citing_papers( # This calls get_paper_citations in kit\n",
    "                paper_id=paper_doi,\n",
    "                limit=limit,\n",
    "                with_abstract=True # Let the kit handle fetching missing abstracts\n",
    "                )\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Processing {len(s2_citingpaper_metadata_raw)} citing papers for {paper_doi}.\")\n",
    "        s2_citingpapermetadata_json = process_citation_metadata(\n",
    "            original_paper_doi=paper_doi,\n",
    "            s2_citation_metadata=s2_citingpaper_metadata_raw, # Pass the raw result\n",
    "            citation_type='citingPaper', # Citation (this paper cites original)\n",
    "            from_dt=from_dt,\n",
    "            to_dt=to_dt,\n",
    "            fields_of_study=fields_of_study # Pass fields for filtering\n",
    "        )\n",
    "\n",
    "        for item in s2_citingpapermetadata_json:\n",
    "            if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                item.setdefault('properties', {})['from_citation'] = True\n",
    "                item['properties']['is_complete'] = True\n",
    "        return s2_citingpapermetadata_json\n",
    "\n",
    "    async def get_recommend_papers(\n",
    "            self,\n",
    "            paper_dois: List[str],\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields_of_study: Optional[List[str]] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Retrieve recommended papers asynchronously.\"\"\"\n",
    "        if not paper_dois: return []\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "        from_dt = from_dt if from_dt is not None else self.from_dt\n",
    "        to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        logging.info(f\"Fetching recommendations based on {len(paper_dois)} papers...\")\n",
    "         # Use the wrapper for the S2 call\n",
    "        s2_recommended_metadata = await self._s2_api_call_wrapper(\n",
    "             self.s2.get_s2_recommended_papers(\n",
    "                positive_paper_ids=paper_dois,\n",
    "                limit=limit,\n",
    "                with_abstract=True # Let kit handle abstract fetching\n",
    "                )\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Processing {len(s2_recommended_metadata)} recommended papers.\")\n",
    "        s2_recpapermetadata_json = process_paper_metadata( # Use standard paper processing\n",
    "            s2_paper_metadata=s2_recommended_metadata,\n",
    "            from_dt=from_dt,\n",
    "            to_dt=to_dt,\n",
    "            fields_of_study=fields_of_study\n",
    "        )\n",
    "\n",
    "        for item in s2_recpapermetadata_json:\n",
    "            if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                item.setdefault('properties', {})['from_recommended'] = True\n",
    "                item['properties']['is_complete'] = True\n",
    "        return s2_recpapermetadata_json\n",
    "\n",
    "\n",
    "    async def llm_gen_related_topics(self, seed_paper_ids: List[str]) -> Optional[Dict]:\n",
    "        \"\"\"Generate related topics using LLM asynchronously.\"\"\"\n",
    "        logging.info(f\"Generating related topics for {len(seed_paper_ids)} seed papers...\")\n",
    "        # Use existing node data\n",
    "        nodes_dict = {node['id']: node for node in self.nodes_json}\n",
    "        domains, seed_paper_texts = [], []\n",
    "\n",
    "        for paper_id in seed_paper_ids:\n",
    "            item = nodes_dict.get(paper_id)\n",
    "            if item and item.get('labels') == ['Paper']:\n",
    "                props = item.get('properties', {})\n",
    "                title = props.get('title')\n",
    "                abstract = props.get('abstract')\n",
    "                # Handle both list and potentially string fieldsOfStudy\n",
    "                domain_info = props.get('fieldsOfStudy')\n",
    "                paper_domains = []\n",
    "                if isinstance(domain_info, list):\n",
    "                     paper_domains = domain_info\n",
    "                elif isinstance(domain_info, str): # Handle case if it's just a string\n",
    "                     paper_domains = [domain_info]\n",
    "\n",
    "                if title and abstract:\n",
    "                    info = f\"<paper>\\nTitle: {title}\\nAbstract: {abstract}\\n</paper>\"\n",
    "                    seed_paper_texts.append(info)\n",
    "                if paper_domains:\n",
    "                    domains.extend(paper_domains)\n",
    "            else:\n",
    "                 logging.warning(f\"Seed paper ID {paper_id} not found or not a Paper node.\")\n",
    "\n",
    "        if not seed_paper_texts:\n",
    "            logging.error(\"No text found for seed papers to generate topics.\")\n",
    "            return None\n",
    "        if not domains:\n",
    "            logging.warning(\"No domains found for seed papers, using 'General Science'.\")\n",
    "            domain = \"General Science\"\n",
    "        else:\n",
    "            domain_counts = Counter(d for d in domains if d) # Count non-empty domains\n",
    "            domain = domain_counts.most_common(1)[0][0] if domain_counts else \"General Science\"\n",
    "\n",
    "\n",
    "        # LLM call (using async retry version)\n",
    "        qa_prompt = keywords_topics_prompt.format(\n",
    "            domain=domain,\n",
    "            example_json=keywords_topics_example,\n",
    "            input_text=\"\\n\\n\".join(seed_paper_texts)\n",
    "        )\n",
    "        logging.info(\"Calling LLM to generate topics...\")\n",
    "        keywords_topics_json = None\n",
    "        try:\n",
    "            # Use the ASYNC retry wrapper\n",
    "            keywords_topics_info = await async_llm_gen_w_retry(\n",
    "                 self.llm_api_key, self.llm_model_name, qa_prompt, sys_prompt=None, temperature=0.6\n",
    "                 )\n",
    "\n",
    "            if keywords_topics_info:\n",
    "                repaired_json_str = repair_json(keywords_topics_info)\n",
    "                keywords_topics_json = json.loads(repaired_json_str)\n",
    "                logging.info(f\"LLM generated topics: {json.dumps(keywords_topics_json)}\")\n",
    "            else:\n",
    "                 logging.error(\"LLM returned empty or None response for topic generation after retries.\")\n",
    "                 return None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"LLM Topic Generation - JSON Repair/Decode Error: {e}. Original output: {keywords_topics_info if 'keywords_topics_info' in locals() else 'N/A'}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during LLM topic generation or processing: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Add Topic nodes and relationships (synchronous part, safe after await)\n",
    "        processed_topic_items = []\n",
    "        query_topics = keywords_topics_json.get('queries', [])\n",
    "        current_node_ids = self._node_ids # Use the class attribute set\n",
    "\n",
    "        for topic in query_topics:\n",
    "            if not isinstance(topic, str) or not topic: continue # Skip empty/invalid topics\n",
    "            topic_hash_id = generate_hash_key(topic)\n",
    "            if topic_hash_id not in current_node_ids:\n",
    "                topic_node = {\n",
    "                    'type': 'node', 'id': topic_hash_id, 'labels': ['Topic'],\n",
    "                    'properties': {'topic_hash_id': topic_hash_id, 'topic_name': topic, 'hash_method': 'hashlib.sha256'}\n",
    "                }\n",
    "                processed_topic_items.append(topic_node)\n",
    "                # Don't add to self._node_ids here, wait for _add_items_to_graph\n",
    "\n",
    "            for paper_id in seed_paper_ids:\n",
    "                 if paper_id in nodes_dict:\n",
    "                    edge_tuple_key = (paper_id, topic_hash_id, \"DISCUSS\")\n",
    "                    if edge_tuple_key not in self._edge_tuples:\n",
    "                        paper_topic_relationship = {\n",
    "                            \"type\": \"relationship\", \"relationshipType\": \"DISCUSS\",\n",
    "                            \"startNodeId\": paper_id, \"endNodeId\": topic_hash_id,\n",
    "                            \"properties\": {}\n",
    "                        }\n",
    "                        processed_topic_items.append(paper_topic_relationship)\n",
    "                        # Don't add to self._edge_tuples here\n",
    "\n",
    "        # Add newly created topic nodes/edges to the main graph state centrally\n",
    "        self._add_items_to_graph(processed_topic_items)\n",
    "        logging.info(f\"Added items related to {len(query_topics)} generated topics.\")\n",
    "\n",
    "        return keywords_topics_json\n",
    "\n",
    "\n",
    "    async def get_related_papers(\n",
    "            self,\n",
    "            topic_json: Dict,\n",
    "            limit: Optional[int] = 100,\n",
    "            from_dt: Optional[str] = None,\n",
    "            to_dt: Optional[str] = None,\n",
    "            fields_of_study: Optional[List[str]] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Fetch related papers based on LLM queries concurrently.\"\"\"\n",
    "        queries = topic_json.get('queries', [])\n",
    "        if not queries or not isinstance(queries, list):\n",
    "            logging.warning(\"No valid queries found in topic_json for related paper search.\")\n",
    "            return []\n",
    "\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "        from_dt = from_dt if from_dt is not None else self.from_dt\n",
    "        to_dt = to_dt if to_dt is not None else self.to_dt\n",
    "\n",
    "        tasks = []\n",
    "        async def _fetch_related_for_query(topic):\n",
    "            if not isinstance(topic, str) or not topic: return [] # Skip invalid queries\n",
    "            logging.info(f\"Fetching related papers for query: '{topic[:50]}...'\")\n",
    "             # Use the wrapper for the S2 call\n",
    "            s2_paper_metadata = await self._s2_api_call_wrapper(\n",
    "                self.s2.search_paper_by_keywords(\n",
    "                    query=topic,\n",
    "                    fields_of_study=fields_of_study, # Pass fields\n",
    "                    limit=limit)\n",
    "            )\n",
    "            logging.info(f\"Processing {len(s2_paper_metadata)} related papers for query '{topic[:50]}...'\")\n",
    "            # Use standard paper processing\n",
    "            s2_papermeta_json = process_related_metadata(\n",
    "                s2_paper_metadata=s2_paper_metadata,\n",
    "                topic=topic,\n",
    "                from_dt=from_dt,\n",
    "                to_dt=to_dt,\n",
    "                fields_of_study=fields_of_study)\n",
    "\n",
    "            for item in s2_papermeta_json:\n",
    "                if item.get('type') == 'node' and item.get('labels') == ['Paper']:\n",
    "                    item.setdefault('properties', {})['from_related_topics'] = True\n",
    "                    item['properties']['is_complete'] = True\n",
    "            return s2_papermeta_json\n",
    "\n",
    "        for topic in queries:\n",
    "            tasks.append(_fetch_related_for_query(topic))\n",
    "\n",
    "        # Run all related paper searches concurrently\n",
    "        all_processed_items = []\n",
    "        if tasks:\n",
    "            logging.info(f\"Running {len(tasks)} related paper search tasks concurrently...\")\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            for result in results:\n",
    "                if isinstance(result, Exception):\n",
    "                    logging.error(f\"Related paper search task failed: {result}\")\n",
    "                elif isinstance(result, list):\n",
    "                    all_processed_items.extend(result)\n",
    "        return all_processed_items\n",
    "\n",
    "\n",
    "    async def cal_semantic_similarity(self, paper_nodes_json: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Calculate semantic similarity asynchronously.\"\"\"\n",
    "        # (Code remains largely the same as previous async version)\n",
    "        semantic_similar_pool = []\n",
    "        publish_dt_ref = {\n",
    "            x['id']: x['properties'].get('publicationDate')\n",
    "            for x in paper_nodes_json if x.get('id') and x.get('properties') and x['properties'].get('publicationDate')\n",
    "        }\n",
    "\n",
    "        ids, texts = [], []\n",
    "        for node in paper_nodes_json:\n",
    "             if not isinstance(node, dict): continue\n",
    "             node_id = node.get('id')\n",
    "             props = node.get('properties', {})\n",
    "             title = props.get('title')\n",
    "             abstract = props.get('abstract')\n",
    "             if node_id and title and abstract:\n",
    "                texts.append(f\"Title: {title}\\nAbstract: {abstract}\") # Simpler format\n",
    "                ids.append(node_id)\n",
    "\n",
    "        if not texts:\n",
    "            logging.warning(\"No paper titles and abstracts found for semantic similarity calculation.\")\n",
    "            return []\n",
    "\n",
    "        logging.info(f\"Generating embeddings for {len(texts)} papers...\")\n",
    "        try:\n",
    "             embeds = await gemini_embedding_async(self.embed_api_key, self.embed_model_name, texts, 10)\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Embedding generation failed: {e}\")\n",
    "             return []\n",
    "\n",
    "        if not embeds or len(embeds) != len(texts):\n",
    "             logging.error(f\"Embedding generation returned unexpected result. Expected {len(texts)}, got {len(embeds) if embeds else 0}.\")\n",
    "             return []\n",
    "\n",
    "        logging.info(\"Calculating similarity matrix...\")\n",
    "        try:\n",
    "            # If semantic_similarity_matrix is very CPU intensive and blocks, wrap it:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            sim_matrix = await loop.run_in_executor(None, semantic_similarity_matrix, embeds, embeds)\n",
    "            # Otherwise, call directly if it's fast enough:\n",
    "            # sim_matrix = semantic_similarity_matrix(embeds, embeds)\n",
    "            sim_matrix = np.array(sim_matrix) # Ensure numpy array\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Similarity matrix calculation failed: {e}\")\n",
    "            return []\n",
    "\n",
    "        logging.info(\"Processing similarity matrix to create relationships...\")\n",
    "        rows, cols = sim_matrix.shape\n",
    "        added_pairs = set() # To store tuples of (id1, id2) where id1 < id2\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(i + 1, cols): # Iterate upper triangle only (j > i)\n",
    "                sim = sim_matrix[i, j]\n",
    "                id_i = ids[i]\n",
    "                id_j = ids[j]\n",
    "\n",
    "                # Determine start/end node consistently (lexicographically)\n",
    "                start_node_id, end_node_id = min(id_i, id_j), max(id_i, id_j)\n",
    "\n",
    "                pair_tuple = (start_node_id, end_node_id)\n",
    "                if pair_tuple not in added_pairs:\n",
    "                    edge = {\n",
    "                        \"type\": \"relationship\",\n",
    "                        \"relationshipType\": \"SIMILAR_TO\",\n",
    "                        \"startNodeId\": start_node_id,\n",
    "                        \"endNodeId\": end_node_id,\n",
    "                        \"properties\": {\n",
    "                            'source': 'semantic similarity',\n",
    "                            'weight': round(float(sim), 4),\n",
    "                         }\n",
    "                     }\n",
    "                    semantic_similar_pool.append(edge)\n",
    "                    added_pairs.add(pair_tuple)\n",
    "\n",
    "        logging.info(f\"Generated {len(semantic_similar_pool)} potential similarity relationships.\")\n",
    "        return semantic_similar_pool\n",
    "\n",
    "\n",
    "    async def init_collect(\n",
    "            self,\n",
    "            limit=50,\n",
    "            from_dt='2022-01-01',\n",
    "            to_dt='2025-04-01', # Use current date or future if appropriate\n",
    "            fields_of_study=None,\n",
    "    ):\n",
    "        \"\"\"Initializes the collection by running the initial paper query.\"\"\"\n",
    "        await self.initial_paper_query(limit=limit, from_dt=from_dt, to_dt=to_dt, fields_of_study=fields_of_study)\n",
    "\n",
    "\n",
    "    # The main optimized async collect method\n",
    "    async def collect(\n",
    "            self,\n",
    "            seed_paper_dois: List[str],\n",
    "            with_reference: Optional[bool] = True,\n",
    "            with_author: Optional[bool] = True,\n",
    "            with_recommend: Optional[bool] = True,\n",
    "            with_expanded_search: Optional[bool] = True,\n",
    "            add_semantic_similarity: Optional[bool] = True,\n",
    "            similarity_threshold: Optional[float] = 0.7,\n",
    "            limit=50,\n",
    "            from_dt='2022-01-01',\n",
    "            to_dt='2025-04-01', # Use current date or future if appropriate\n",
    "            fields_of_study=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Main asynchronous collection method orchestrating various data fetching tasks.\n",
    "        \"\"\"\n",
    "        if not seed_paper_dois:\n",
    "             logging.warning(\"Collect called with no seed paper DOIs.\")\n",
    "             # Optionally exit or proceed based on requirements\n",
    "             # return\n",
    "\n",
    "        fields_of_study = fields_of_study if fields_of_study is not None else self.fields_of_study\n",
    "\n",
    "        data_fetching_tasks = []\n",
    "        processed_items_aggregate = []\n",
    "\n",
    "        # --- Task Prep: Author Info ---\n",
    "        if with_author:\n",
    "            author_ids_to_fetch = set()\n",
    "            current_nodes_dict = {node['id']: node for node in self.nodes_json}\n",
    "            current_complete_author_ids = {node['id'] for node in self.nodes_json if node.get('labels') == [\"Author\"] and node.get('properties', {}).get('is_complete')}\n",
    "\n",
    "            for doi in seed_paper_dois:\n",
    "                 paper_node = current_nodes_dict.get(doi)\n",
    "                 if paper_node and paper_node.get('labels') == [\"Paper\"]:\n",
    "                     authors = paper_node.get('properties', {}).get('authors', [])\n",
    "                     for author in authors:\n",
    "                         author_id = author.get('authorId')\n",
    "                         if author_id and author_id not in current_complete_author_ids:\n",
    "                             author_ids_to_fetch.add(author_id)\n",
    "\n",
    "            if author_ids_to_fetch:\n",
    "                 logging.info(f\"Preparing author info task for {len(author_ids_to_fetch)} authors.\")\n",
    "                 data_fetching_tasks.append(self.get_author_info(list(author_ids_to_fetch), from_dt, to_dt, fields_of_study))\n",
    "            else:\n",
    "                 logging.info(\"No new authors found for seed DOIs or all are already complete.\")\n",
    "\n",
    "        # --- Task Prep: References & Citations ---\n",
    "        if with_reference and seed_paper_dois:\n",
    "            logging.info(f\"Preparing reference/citation tasks for {len(seed_paper_dois)} seed papers.\")\n",
    "            for paper_doi in seed_paper_dois:\n",
    "                # Pass combined fields list\n",
    "                data_fetching_tasks.append(self.get_cited_papers(paper_doi, limit, from_dt, to_dt, fields_of_study))\n",
    "                data_fetching_tasks.append(self.get_citing_papers(paper_doi, limit, from_dt, to_dt, fields_of_study))\n",
    "\n",
    "        # --- Task Prep: Recommendations ---\n",
    "        if with_recommend and seed_paper_dois:\n",
    "             logging.info(\"Preparing recommendations task.\")\n",
    "             data_fetching_tasks.append(self.get_recommend_papers(seed_paper_dois, limit, from_dt, to_dt, fields_of_study))\n",
    "\n",
    "        # --- Task Prep: Expanded Search ---\n",
    "        expanded_search_task = None\n",
    "        if with_expanded_search and seed_paper_dois:\n",
    "            async def _handle_expanded_search():\n",
    "                logging.info(\"Starting expanded search sub-task...\")\n",
    "                # LLM gen adds topic nodes/edges directly via _add_items_to_graph\n",
    "                keywords_topics_json = await self.llm_gen_related_topics(seed_paper_dois)\n",
    "                related_papers_items = []\n",
    "                if keywords_topics_json:\n",
    "                    # Fetch related papers based on topics\n",
    "                    related_papers_items = await self.get_related_papers(\n",
    "                        keywords_topics_json, limit, from_dt, to_dt, fields_of_study\n",
    "                        )\n",
    "                else:\n",
    "                    logging.warning(\"Skipping related paper fetch as no topics were generated.\")\n",
    "                return related_papers_items # Return newly fetched related papers\n",
    "\n",
    "            logging.info(\"Preparing expanded search task.\")\n",
    "            expanded_search_task = asyncio.create_task(_handle_expanded_search())\n",
    "\n",
    "        # --- Execute Concurrent Data Fetching Tasks ---\n",
    "        if data_fetching_tasks:\n",
    "            logging.info(f\"Running {len(data_fetching_tasks)} main data collection tasks concurrently...\")\n",
    "            results = await asyncio.gather(*data_fetching_tasks, return_exceptions=True)\n",
    "            logging.info(\"Main data collection tasks finished.\")\n",
    "            for result in results:\n",
    "                if isinstance(result, Exception):\n",
    "                    logging.error(f\"Data collection task failed: {result}\")\n",
    "                elif isinstance(result, list):\n",
    "                    processed_items_aggregate.extend(result) # Collect items\n",
    "        else:\n",
    "            logging.info(\"No main data collection tasks to run.\")\n",
    "\n",
    "        # --- Wait for and Process Expanded Search Task Results ---\n",
    "        if expanded_search_task:\n",
    "            logging.info(\"Waiting for expanded search task to complete...\")\n",
    "            try:\n",
    "                expanded_result = await expanded_search_task\n",
    "                if isinstance(expanded_result, list):\n",
    "                     processed_items_aggregate.extend(expanded_result) # Add related papers\n",
    "                logging.info(\"Expanded search task finished.\")\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Expanded search task failed: {e}\")\n",
    "\n",
    "        # --- Add all aggregated items to the graph state ---\n",
    "        logging.info(f\"Adding {len(processed_items_aggregate)} items from tasks to graph...\")\n",
    "        self._add_items_to_graph(processed_items_aggregate)\n",
    "        logging.info(f\"Graph state after collection tasks. Nodes: {len(self.nodes_json)}, Edges: {len(self.edges_json)}\")\n",
    "\n",
    "        # --- Task: Semantic Similarity ---\n",
    "        if add_semantic_similarity:\n",
    "            logging.info(\"Starting semantic similarity calculation...\")\n",
    "            # Use the latest graph state\n",
    "            paper_nodes_for_similarity = [node for node in self.nodes_json if node.get('labels') == [\"Paper\"]]\n",
    "            if paper_nodes_for_similarity:\n",
    "                try:\n",
    "                    semantic_similar_pool = await self.cal_semantic_similarity(paper_nodes_for_similarity)\n",
    "                    semantic_similar_relationships = [\n",
    "                        edge for edge in semantic_similar_pool\n",
    "                        if edge.get('properties', {}).get('weight', 0) > similarity_threshold\n",
    "                    ]\n",
    "                    logging.info(f\"Adding {len(semantic_similar_relationships)} similarity edges above threshold {similarity_threshold}...\")\n",
    "                    self._add_items_to_graph(semantic_similar_relationships)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Semantic similarity calculation or addition failed: {e}\")\n",
    "            else:\n",
    "                 logging.warning(\"No paper nodes found to calculate semantic similarity.\")\n",
    "\n",
    "        # Final state log\n",
    "        logging.info(f\"Collection process complete. Final state: Nodes={len(self.nodes_json)}, Edges={len(self.edges_json)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "research_topic = \"llm literature review\"\n",
    "seed_dois = ['10.48550/arXiv.2406.10252',  # AutoSurvey: Large Language Models Can Automatically Write Surveys\n",
    "             '10.48550/arXiv.2412.10415',  # Generative Adversarial Reviews: When LLMs Become the Critic\n",
    "             '10.48550/arXiv.2402.12928',  # A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence \n",
    "             ]\n",
    "seed_titles = ['PaperRobot: Incremental Draft Generation of Scientific Ideas',\n",
    "               'From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems'\n",
    "               ]\n",
    "llm_api_key = os.getenv('GEMINI_API_KEY_3')\n",
    "llm_model_name=\"gemini-2.0-flash\"\n",
    "embed_api_key = os.getenv('GEMINI_API_KEY_3')\n",
    "embed_model_name=\"models/text-embedding-004\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "if 'a' not in []:\n",
    "    print(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 12:06:34,910 - INFO - S2 API Control: Max Concurrent=1, Avg Interval=3.50s (Rate Limit ~0.29/sec)\n",
      "2025-04-02 12:06:34,911 - INFO - Running 1 initial query tasks concurrently...\n",
      "2025-04-02 12:06:34,911 - INFO - Fetching papers by 1 DOIs...\n",
      "2025-04-02 12:06:34,912 - INFO - Aggregating results from initial queries...\n",
      "2025-04-02 12:06:34,912 - ERROR - Initial query task failed: Can't acquire more than the maximum capacity\n",
      "2025-04-02 12:06:34,912 - INFO - Initial query complete. Nodes: 0, Edges: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Initial Query ---\n",
      "Nodes after init: 0\n",
      "Edges after init: 0\n",
      "Warning: Seed DOI(s) not found after initial query.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Example Usage (similar to before, adjust parameters if needed) ---\n",
    "async def main():\n",
    "    searcher = PaperSearch(\n",
    "        seed_paper_dois=[\"10.1109/CVPR.2016.90\"], # Example DOI\n",
    "        llm_api_key=llm_api_key,\n",
    "        llm_model_name=llm_model_name,\n",
    "        embed_api_key=embed_api_key,\n",
    "        embed_model_name=embed_model_name,\n",
    "        # --- Control Concurrency/Rate ---\n",
    "        s2_max_concurrent=1, # Allow up to 5 S2 requests at once\n",
    "        s2_request_interval=3.5 # Target avg 3.5s between S2 requests (~0.28/sec)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"--- Running Initial Query ---\")\n",
    "    fields_of_study = ['Computer Science']\n",
    "    await searcher.init_collect(limit=10, from_dt=\"2015-01-01\", to_dt=\"2025-04-01\", fields_of_study=fields_of_study)\n",
    "    print(f\"Nodes after init: {len(searcher.nodes_json)}\")\n",
    "    print(f\"Edges after init: {len(searcher.edges_json)}\")\n",
    "\n",
    "    seed_dois_in_graph = [node['id'] for node in searcher.nodes_json if node.get('labels') == ['Paper'] and node.get('properties', {}).get('from_seed')]\n",
    "    if not seed_dois_in_graph:\n",
    "         print(\"Warning: Seed DOI(s) not found after initial query.\")\n",
    "         return\n",
    "\n",
    "    print(f\"\\n--- Running Main Collection for seed DOIs: {seed_dois_in_graph} ---\")\n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "\n",
    "    await searcher.collect(\n",
    "        seed_paper_dois=seed_dois_in_graph,\n",
    "        with_reference=True,\n",
    "        with_author=True,\n",
    "        with_recommend=True,\n",
    "        with_expanded_search=False, # Set True to test LLM part\n",
    "        add_semantic_similarity=True,\n",
    "        similarity_threshold=0.75,\n",
    "        limit=20,\n",
    "        from_dt=\"2015-01-01\",\n",
    "        to_dt=\"2025-04-01\",\n",
    "        fields_of_study=fields_of_study # Pass desired fields for collect phase\n",
    "    )\n",
    "    end_time = asyncio.get_event_loop().time()\n",
    "    print(f\"\\nOptimized collection took {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Final Nodes: {len(searcher.nodes_json)}\")\n",
    "    print(f\"Final Edges: {len(searcher.edges_json)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PaperSearch(research_topic=research_topic,\n",
    "                 seed_paper_titles=seed_titles,\n",
    "                 seed_paper_dois=seed_dois,\n",
    "                 llm_api_key=llm_api_key,\n",
    "                 llm_model_name=\"gemini-2.0-flash\", \n",
    "                 embed_api_key=embed_api_key,\n",
    "                 embed_model_name=\"models/text-embedding-004\",\n",
    "                 from_dt=\"2020-01-01\",\n",
    "                 to_dt=\"2025-06-01\",\n",
    "                 fields=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Based on your input, the search will started from the following seed papers:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.init_collect(limit=50, from_dt='2022-01-01', to_dt='2025-06-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.nodes_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_paper_dois = [x['id'] for x in ps.nodes_json \n",
    "                   if x['labels'][0] in ['Paper'] \n",
    "                   and x['properties'].get('from_seed')==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ps.collect(\n",
    "    seed_paper_dois=seed_paper_dois,\n",
    "    with_author=True,\n",
    "    with_recommend=True,\n",
    "    with_expanded_search=True, \n",
    "    add_semantic_similarity=True,\n",
    "    similarity_threshold=0.5,\n",
    "    limit=100, \n",
    "    from_dt='2022-01-01', \n",
    "    to_dt='2025-03-13',\n",
    "    fields=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
