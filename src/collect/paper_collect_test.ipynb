{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9db46f5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aabc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import List, Dict, Optional, Union, Tuple, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3375619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9623dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.paper_graph import PaperGraph\n",
    "from graph.graph_viz import GraphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57ad172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driving examples\n",
    "llm_api_key = os.getenv('GEMINI_API_KEY_3')\n",
    "llm_model_name = \"gemini-2.5-flash-preview-04-17\"\n",
    "embed_api_key = os.getenv('GEMINI_API_KEY_3')\n",
    "embed_model_name = \"models/text-embedding-004\"\n",
    "\n",
    "research_topics = [\"llm literature review\"]\n",
    "seed_dois = ['10.48550/arXiv.2406.10252',  # AutoSurvey: Large Language Models Can Automatically Write Surveys\n",
    "            '10.48550/arXiv.2412.10415',  # Generative Adversarial Reviews: When LLMs Become the Critic\n",
    "            '10.48550/arXiv.2402.12928',  # A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence \n",
    "            ]\n",
    "seed_titles = ['PaperRobot: Incremental Draft Generation of Scientific Ideas',\n",
    "            'From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4216dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_limit = 100\n",
    "author_paper_limit = 10\n",
    "\n",
    "if len(seed_dois) < 10 or len(seed_titles) < 10:\n",
    "    search_limit = 100\n",
    "    recommend_limit = 100\n",
    "else:\n",
    "    search_limit = 50\n",
    "    recommend_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a365f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:12:03,557 - SemanticScholarKit - INFO - SemanticScholarKit initialized with: max_concurrency=20, max_retry=20, sleep_interval=3.0s\n",
      "INFO:SemanticScholarKit:SemanticScholarKit initialized with: max_concurrency=20, max_retry=20, sleep_interval=3.0s\n",
      "2025-04-30 15:12:03,558 - SemanticScholarKit - INFO - SemanticScholarKit initialized with: max_concurrency=20, max_retry=20, sleep_interval=3.0s\n",
      "INFO:SemanticScholarKit:SemanticScholarKit initialized with: max_concurrency=20, max_retry=20, sleep_interval=3.0s\n"
     ]
    }
   ],
   "source": [
    "from collect.paper_data_collect import PaperCollector\n",
    "\n",
    "ps = PaperCollector(   \n",
    "    seed_research_topics = research_topics,   \n",
    "    seed_paper_titles = seed_titles, \n",
    "    seed_paper_ids = seed_dois,\n",
    "    from_dt = '2020-01-01',\n",
    "    to_dt = '2025-04-30',\n",
    "    fields_of_study = ['Computer Science'],\n",
    "    author_paper_limit = author_paper_limit,\n",
    "    search_limit = search_limit,\n",
    "    recommend_limit = recommend_limit,\n",
    "    citation_limit = citation_limit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1724329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/jiezi4ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from typing import List, Dict, Optional, Union, Any, Set, Tuple\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from graph.paper_graph import PaperGraph\n",
    "from collect.paper_sim_calc import PaperSim\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def graph_basic_stats(node_stats: Dict):\n",
    "    \"\"\"calculate key stats index for node stats\"\"\"\n",
    "    node_stats_index = {}\n",
    "    for key, values in node_stats.items():\n",
    "        if key == 'id': continue  # skip id\n",
    "\n",
    "        valid_values = [v for v in values if isinstance(v, (int, float))] # 只处理数值类型\n",
    "        if valid_values:\n",
    "            node_stats_index[key] = {\n",
    "                'min': np.min(valid_values),\n",
    "                'max': np.max(valid_values),\n",
    "                'average': np.mean(valid_values),\n",
    "                'median': np.median(valid_values),\n",
    "                'quantile_25': np.percentile(valid_values, 25),\n",
    "                'quantile_75': np.percentile(valid_values, 75)\n",
    "            }\n",
    "        else:\n",
    "            node_stats_index[key] = {}  \n",
    "    return node_stats_index\n",
    "\n",
    "\n",
    "class PaperRouter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes_json: Optional[List[Dict]] = None,\n",
    "        edges_json: Optional[List[Dict]] = None,\n",
    "        paper_sim: Optional[PaperSim] = None, \n",
    "        \n",
    "        embed_api_key: Optional[str] = None,\n",
    "        embed_model_name: Optional[str] = None\n",
    "        ):\n",
    "        self.nodes_json = nodes_json\n",
    "        self.edges_json = edges_json\n",
    "        \n",
    "        # initiate semantic scholar instances\n",
    "        if paper_sim and isinstance(paper_sim, PaperSim):\n",
    "            self.sim_calc = paper_sim  \n",
    "        else:\n",
    "            self.sim_calc = PaperSim(\n",
    "                embed_api_key = embed_api_key,\n",
    "                embed_model_name = embed_model_name\n",
    "            )\n",
    "\n",
    "\n",
    "    ####################################################################################\n",
    "    # similarity calculation and filter\n",
    "    ####################################################################################\n",
    "    async def score_paper2paper__sim(self, nodes_json):\n",
    "        \"\"\"calculate paper-paper similarity\"\"\"\n",
    "        # valid paper with abstracts\n",
    "        paper_json_w_abstract = [node for node in nodes_json \n",
    "                                if node['labels'] == ['Paper'] \n",
    "                                and node['properties'].get('title') is not None and node['properties'].get('abstract') is not None]\n",
    "        paper_dois_w_abstract = [node['id'] for node in paper_json_w_abstract]\n",
    "\n",
    "        # calculate paper nodes similarity\n",
    "        semantic_similar_pool = await self.sim_cal.cal_embed_and_similarity(\n",
    "            paper_nodes_json = paper_json_w_abstract,\n",
    "            paper_dois_1 = paper_dois_w_abstract, \n",
    "            paper_dois_2 = paper_dois_w_abstract,\n",
    "            similarity_threshold = 0.7,\n",
    "            )\n",
    "        return semantic_similar_pool\n",
    "\n",
    "    async def score_paper2topic_sim(self, nodes_json):\n",
    "        \"\"\"calculate paper-topic similarity\"\"\"\n",
    "        # valid paper with abstracts\n",
    "        paper_json_w_abstract = [node for node in nodes_json \n",
    "                                if node['labels'] == ['Paper'] \n",
    "                                and node['properties'].get('title') is not None and node['properties'].get('abstract') is not None]\n",
    "        paper_dois_w_abstract = [node['id'] for node in paper_json_w_abstract]\n",
    "\n",
    "        # calculate paper nodes similarity\n",
    "        semantic_similar_pool = await self.sim_cal.cal_embed_and_similarity(\n",
    "            paper_nodes_json = paper_json_w_abstract,\n",
    "            paper_dois_1 = paper_dois_w_abstract, \n",
    "            paper_dois_2 = paper_dois_w_abstract,\n",
    "            similarity_threshold = 0.7,\n",
    "            )\n",
    "        return semantic_similar_pool\n",
    "\n",
    "\n",
    "    ####################################################################################\n",
    "    # statistics calculation and significant paper identify\n",
    "    ####################################################################################\n",
    "    \n",
    "\n",
    "    def identify_paper_significant(\n",
    "            self,\n",
    "            paper_stats,\n",
    "            ):\n",
    "        \"\"\"identify significant paper node\"\"\"\n",
    "        paper_stats_df = pd.DataFrame(paper_stats)\n",
    "        for index, row in paper_stats_df.iterrows():\n",
    "            pid = row['id']\n",
    "            # rule 1: global citation greater than or equal to 20\n",
    "            if row['citationCount'] >= 20:\n",
    "                significant_ind = 1\n",
    "                info = 'citationCount'\n",
    "\n",
    "            # rule 2: influential citation greater than or equal to 3\n",
    "            elif row['influentialCitationCount'] >= 3:\n",
    "                significant_ind = 1\n",
    "                info = 'influentialCitationCount'\n",
    "\n",
    "            # rule 3: monthly citation greater than or equal to 5\n",
    "            elif row['monthlyCitationCount'] >= 5:\n",
    "                significant_ind = 1\n",
    "                info = 'monthlyCitationCount'\n",
    "\n",
    "            # rule 4: local citation greater than or equal to 5\n",
    "            elif row['localCitationCount'] >= 5:\n",
    "                significant_ind = 1\n",
    "                info = 'localCitationCount'\n",
    "\n",
    "            row['significance'] = significant_ind\n",
    "            row['sig_info'] = info\n",
    "\n",
    "    def identify_author_significant(\n",
    "            self,\n",
    "            author_stats,\n",
    "            ):\n",
    "        \"\"\"identify significant paper node\"\"\"\n",
    "        author_stats_df = pd.DataFrame(author_stats)\n",
    "        for index, row in author_stats_df.iterrows():\n",
    "            aid = row['id']\n",
    "            # rule 1: h-index greater than or equal to 10\n",
    "            if row['hIndex'] >= 10:\n",
    "                significant_ind = 1\n",
    "\n",
    "            # rule 2: average paper ciation greater than or equal to 20\n",
    "            elif row['paperCount'] / row['paperCount'] >= 20:\n",
    "                significant_ind = 1\n",
    "\n",
    "            # rule 4: local citation greater than or equal to 5\n",
    "            elif row['localPaperCount'] >= 5:\n",
    "                significant_ind = 1\n",
    "\n",
    "            row['significance'] = significant_ind\n",
    "\n",
    "    def identify_paper_significant_relative(self):\n",
    "        pass\n",
    "\n",
    "    def identify_paper_significant_lpa(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96de119",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791ec28",
   "metadata": {},
   "source": [
    "## Initial Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe7dd3",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed374b",
   "metadata": {},
   "source": [
    "Get paper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2e2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6407d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:12:21,612 - Paper Collector - INFO - consolidated_search: Starting...\n",
      "INFO:Paper Collector:consolidated_search: Starting...\n",
      "2025-04-30 15:12:21,614 - Paper Collector - INFO - consolidated_search: Running 1 sub-tasks concurrently...\n",
      "INFO:Paper Collector:consolidated_search: Running 1 sub-tasks concurrently...\n",
      "2025-04-30 15:12:21,615 - Paper Collector - INFO - Search 2 paper titles and 3 for paper information.\n",
      "INFO:Paper Collector:Search 2 paper titles and 3 for paper information.\n",
      "2025-04-30 15:12:21,616 - Paper Collector - INFO - paper_search: Creating task for 3 IDs...\n",
      "INFO:Paper Collector:paper_search: Creating task for 3 IDs...\n",
      "2025-04-30 15:12:21,617 - Paper Collector - INFO - paper_search: Creating 2 tasks for titles...\n",
      "INFO:Paper Collector:paper_search: Creating 2 tasks for titles...\n",
      "2025-04-30 15:12:21,618 - Paper Collector - INFO - paper_search: Running 3 query tasks concurrently...\n",
      "INFO:Paper Collector:paper_search: Running 3 query tasks concurrently...\n",
      "2025-04-30 15:12:21,620 - SemanticScholarKit - INFO - get_papers: Creating 1 tasks for 1 IDs.\n",
      "INFO:SemanticScholarKit:get_papers: Creating 1 tasks for 1 IDs.\n",
      "2025-04-30 15:12:21,621 - SemanticScholarKit - INFO - get_papers: Gathering 1 tasks...\n",
      "INFO:SemanticScholarKit:get_papers: Gathering 1 tasks...\n",
      "2025-04-30 15:12:22,751 - SemanticScholarKit - INFO - get_papers: Gather complete. Processing results.\n",
      "INFO:SemanticScholarKit:get_papers: Gather complete. Processing results.\n",
      "2025-04-30 15:12:23,862 - Paper Collector - INFO - consolidated_search: Sub-tasks finished.\n",
      "INFO:Paper Collector:consolidated_search: Sub-tasks finished.\n",
      "2025-04-30 15:12:23,864 - Paper Collector - INFO - consolidated_search: Finished.\n",
      "INFO:Paper Collector:consolidated_search: Finished.\n"
     ]
    }
   ],
   "source": [
    "await ps.consolidated_search(\n",
    "    paper_titles = seed_titles,\n",
    "    paper_ids = seed_dois\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8be95",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64688e",
   "metadata": {},
   "source": [
    "Paper post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccfdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:12:34,019 - Paper Collector - INFO - post_process: Starting data processing...\n",
      "INFO:Paper Collector:post_process: Starting data processing...\n",
      "2025-04-30 15:12:34,020 - Paper Collector - INFO - Processing 5 raw paper entries...\n",
      "INFO:Paper Collector:Processing 5 raw paper entries...\n",
      "2025-04-30 15:12:34,022 - Paper Collector - INFO - Generated 75 nodes/edges from papers.\n",
      "INFO:Paper Collector:Generated 75 nodes/edges from papers.\n",
      "2025-04-30 15:12:34,023 - Paper Collector - INFO - Total items after paper processing: 75\n",
      "INFO:Paper Collector:Total items after paper processing: 75\n",
      "2025-04-30 15:12:34,024 - Paper Collector - INFO - No author data in pool to process.\n",
      "INFO:Paper Collector:No author data in pool to process.\n",
      "2025-04-30 15:12:34,026 - Paper Collector - INFO - No topic data in pool to process.\n",
      "INFO:Paper Collector:No topic data in pool to process.\n",
      "2025-04-30 15:12:34,027 - Paper Collector - INFO - No citation data (references or citings) in pool to process.\n",
      "INFO:Paper Collector:No citation data (references or citings) in pool to process.\n"
     ]
    }
   ],
   "source": [
    "await ps.post_process(if_supplement_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e069bd0",
   "metadata": {},
   "source": [
    "### Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf6947",
   "metadata": {},
   "source": [
    "Plan next move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698fba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 33\n"
     ]
    }
   ],
   "source": [
    "# core papers and core authors nodes\n",
    "if iteration == 1:\n",
    "    core_paper_ids = set(node['id'] for node in ps.nodes_json if node['labels'] == ['Paper'])\n",
    "    core_author_ids = set(node['id'] for node in ps.nodes_json if node['labels'] == ['Author'])\n",
    "    print(len(core_paper_ids), len(core_author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61975557",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.explored_nodes['paper'].update(core_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57faf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for authors\n",
    "author_ids = [author_id for author_id in core_author_ids if author_id not in ps.explored_nodes['author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b88a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference and citings\n",
    "ref_ids = [pid for pid in core_paper_ids if pid not in ps.explored_nodes['reference']]\n",
    "cit_ids = [pid for pid in core_paper_ids if pid not in ps.explored_nodes['citing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf3c6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation \n",
    "if len(ps.explored_nodes['recommendation']) == 0:\n",
    "    if len(core_paper_ids) > 3:\n",
    "        pos_paper_ids = list(core_paper_ids)\n",
    "        neg_paper_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fffd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:12:40,185 - Paper Collector - INFO - Generating related topics for 5 seed papers...\n",
      "INFO:Paper Collector:Generating related topics for 5 seed papers...\n",
      "2025-04-30 15:12:40,188 - Paper Collector - INFO - Calling LLM to generate topics...\n",
      "INFO:Paper Collector:Calling LLM to generate topics...\n",
      "2025-04-30 15:12:54,312 - Paper Collector - INFO - LLM generated topics: [{\"query\": \"AI research workflow automation support systems\", \"description\": \"This query targets papers providing a broad overview or discussing AI's role in automating or assisting multiple stages of the scientific research process, from literature search and hypothesis generation to writing and publication including peer review. Useful for understanding the landscape of AI in research.\"}, {\"query\": \"LLM literature review survey generation\", \"description\": \"Focuses specifically on studies investigating the use of Large Language Models or other AI techniques for creating, generating, or assisting in the production and analysis of academic literature reviews or surveys.\"}, {\"query\": \"automated scientific peer review AI models\", \"description\": \"Aims to find research exploring the application of AI systems, particularly machine learning or generative models and agents, to automate, support, or simulate the process of peer review for scientific manuscripts, including feedback generation and decision prediction.\"}, {\"query\": \"generative AI scientific paper writing assistance\", \"description\": \"Targets papers on using generative AI technologies to aid researchers in writing scientific articles, including drafting specific sections like abstracts, introductions, conclusions, future work, or providing support for idea generation and structuring.\"}, {\"query\": \"evaluating AI generated academic content quality\", \"description\": \"Searches for studies focused on developing methods, metrics, or conducting experiments to assess the quality, accuracy, reliability, novelty, or human-likeness of academic content (like reviews, papers, feedback, survey summaries) generated by AI systems.\"}]\n",
      "INFO:Paper Collector:LLM generated topics: [{\"query\": \"AI research workflow automation support systems\", \"description\": \"This query targets papers providing a broad overview or discussing AI's role in automating or assisting multiple stages of the scientific research process, from literature search and hypothesis generation to writing and publication including peer review. Useful for understanding the landscape of AI in research.\"}, {\"query\": \"LLM literature review survey generation\", \"description\": \"Focuses specifically on studies investigating the use of Large Language Models or other AI techniques for creating, generating, or assisting in the production and analysis of academic literature reviews or surveys.\"}, {\"query\": \"automated scientific peer review AI models\", \"description\": \"Aims to find research exploring the application of AI systems, particularly machine learning or generative models and agents, to automate, support, or simulate the process of peer review for scientific manuscripts, including feedback generation and decision prediction.\"}, {\"query\": \"generative AI scientific paper writing assistance\", \"description\": \"Targets papers on using generative AI technologies to aid researchers in writing scientific articles, including drafting specific sections like abstracts, introductions, conclusions, future work, or providing support for idea generation and structuring.\"}, {\"query\": \"evaluating AI generated academic content quality\", \"description\": \"Searches for studies focused on developing methods, metrics, or conducting experiments to assess the quality, accuracy, reliability, novelty, or human-likeness of academic content (like reviews, papers, feedback, survey summaries) generated by AI systems.\"}]\n"
     ]
    }
   ],
   "source": [
    "# topics generation\n",
    "core_paper_json = [x for x in ps.nodes_json if x['id'] in core_paper_ids]\n",
    "if len(ps.explored_nodes['topic']) < 4:  # explored topic less than 4, generate new topics\n",
    "    await ps.topic_generation(\n",
    "        paper_json = core_paper_json,\n",
    "        llm_api_key = llm_api_key,\n",
    "        llm_model_name = llm_model_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a088be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI research workflow automation support systems', 'LLM literature review survey generation', 'automated scientific peer review AI models', 'generative AI scientific paper writing assistance', 'evaluating AI generated academic content quality']\n"
     ]
    }
   ],
   "source": [
    "# identify unexplored topics\n",
    "# covert topic data to k-v format\n",
    "topic_pids = {}\n",
    "\n",
    "for item in ps.data_pool['topic']:\n",
    "    topic = item['topic']\n",
    "    paper_id = item['paperId']\n",
    "    \n",
    "    if topic not in topic_pids:\n",
    "        topic_pids[topic] = []\n",
    "        \n",
    "    topic_pids[topic].append(paper_id)\n",
    "\n",
    "# identify topics with insufficient papers\n",
    "topics = []\n",
    "for topic, pids in topic_pids.items():\n",
    "    if len(pids) < 10:\n",
    "        topics.append(topic)\n",
    "print(topics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624cc912",
   "metadata": {},
   "source": [
    "## Expanded Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cdf07",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253541e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:13:17,839 - Paper Collector - INFO - consolidated_search: Starting...\n",
      "INFO:Paper Collector:consolidated_search: Starting...\n",
      "2025-04-30 15:13:17,841 - Paper Collector - INFO - consolidated_search: Running 5 sub-tasks concurrently...\n",
      "INFO:Paper Collector:consolidated_search: Running 5 sub-tasks concurrently...\n",
      "2025-04-30 15:13:17,842 - Paper Collector - INFO - topic_search: Searching 5 topics.\n",
      "INFO:Paper Collector:topic_search: Searching 5 topics.\n",
      "2025-04-30 15:13:17,844 - Paper Collector - INFO - topic_search: Running 5 topic search tasks concurrently...\n",
      "INFO:Paper Collector:topic_search: Running 5 topic search tasks concurrently...\n",
      "2025-04-30 15:13:17,845 - Paper Collector - INFO - authors_search: Searching 33 authors.\n",
      "INFO:Paper Collector:authors_search: Searching 33 authors.\n",
      "2025-04-30 15:13:17,846 - SemanticScholarKit - INFO - get_authors: Creating 1 tasks for 1 IDs.\n",
      "INFO:SemanticScholarKit:get_authors: Creating 1 tasks for 1 IDs.\n",
      "2025-04-30 15:13:17,847 - SemanticScholarKit - INFO - get_authors: Gathering 1 tasks...\n",
      "INFO:SemanticScholarKit:get_authors: Gathering 1 tasks...\n",
      "2025-04-30 15:13:17,848 - Paper Collector - INFO - reference_search: Fetching references for 5 papers (limit per paper: 100).\n",
      "INFO:Paper Collector:reference_search: Fetching references for 5 papers (limit per paper: 100).\n",
      "2025-04-30 15:13:17,849 - Paper Collector - INFO - reference_search: Running 5 reference search tasks concurrently...\n",
      "INFO:Paper Collector:reference_search: Running 5 reference search tasks concurrently...\n",
      "2025-04-30 15:13:17,850 - Paper Collector - INFO - citing_search: Fetching citations for 5 papers (limit per paper: 100).\n",
      "INFO:Paper Collector:citing_search: Fetching citations for 5 papers (limit per paper: 100).\n",
      "2025-04-30 15:13:17,851 - Paper Collector - INFO - Preparing citing for 5 papers ...\n",
      "INFO:Paper Collector:Preparing citing for 5 papers ...\n",
      "2025-04-30 15:13:17,851 - Paper Collector - INFO - citing_search: Running 5 citation search tasks concurrently...\n",
      "INFO:Paper Collector:citing_search: Running 5 citation search tasks concurrently...\n",
      "2025-04-30 15:13:17,852 - Paper Collector - INFO - paper_recommendation: Fetching 100 recommendations based on 5 positive and 0 negative papers.\n",
      "INFO:Paper Collector:paper_recommendation: Fetching 100 recommendations based on 5 positive and 0 negative papers.\n",
      "2025-04-30 15:13:18,695 - SemanticScholarKit - WARNING - Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'AI research workflow automatio...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "WARNING:SemanticScholarKit:Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'AI research workflow automatio...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "2025-04-30 15:13:18,701 - SemanticScholarKit - WARNING - Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'LLM literature review survey g...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "WARNING:SemanticScholarKit:Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'LLM literature review survey g...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "2025-04-30 15:13:18,730 - SemanticScholarKit - WARNING - Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'generative AI scientific paper...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "WARNING:SemanticScholarKit:Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'generative AI scientific paper...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "2025-04-30 15:13:18,738 - SemanticScholarKit - WARNING - Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'automated scientific peer revi...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "WARNING:SemanticScholarKit:Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'automated scientific peer revi...'). Underlying tenacity retries failed. Attempting outer retry 1/21. Retrying after 3.0 seconds...\n",
      "2025-04-30 15:13:21,196 - SemanticScholarKit - INFO - get_authors: Gather complete. Processing results.\n",
      "INFO:SemanticScholarKit:get_authors: Gather complete. Processing results.\n",
      "2025-04-30 15:13:21,197 - Paper Collector - INFO - authors_search: Added 33 new authors and 1195 new papers to data pool.\n",
      "INFO:Paper Collector:authors_search: Added 33 new authors and 1195 new papers to data pool.\n",
      "2025-04-30 15:13:22,474 - SemanticScholarKit - WARNING - Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'AI research workflow automatio...'). Underlying tenacity retries failed. Attempting outer retry 2/21. Retrying after 3.0 seconds...\n",
      "WARNING:SemanticScholarKit:Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'AI research workflow automatio...'). Underlying tenacity retries failed. Attempting outer retry 2/21. Retrying after 3.0 seconds...\n",
      "2025-04-30 15:13:24,742 - Paper Collector - WARNING - citing_search: Task for paper 'cdb34c0092a767848ca1de6fa7e3a6b822585fa4' returned no results or failed silently.\n",
      "WARNING:Paper Collector:citing_search: Task for paper 'cdb34c0092a767848ca1de6fa7e3a6b822585fa4' returned no results or failed silently.\n",
      "2025-04-30 15:13:24,743 - Paper Collector - INFO - citing_search: Added 75 new unique citing papers and 75 citation relationships.\n",
      "INFO:Paper Collector:citing_search: Added 75 new unique citing papers and 75 citation relationships.\n",
      "2025-04-30 15:13:25,320 - Paper Collector - INFO - reference_search: Retrieving reference for paper 'a6aed0c4e0f39a55edb407f492e41f178a62907f'\n",
      "INFO:Paper Collector:reference_search: Retrieving reference for paper 'a6aed0c4e0f39a55edb407f492e41f178a62907f'\n",
      "2025-04-30 15:13:25,322 - Paper Collector - INFO - reference_search: Retrieving reference for paper 'cdb34c0092a767848ca1de6fa7e3a6b822585fa4'\n",
      "INFO:Paper Collector:reference_search: Retrieving reference for paper 'cdb34c0092a767848ca1de6fa7e3a6b822585fa4'\n",
      "2025-04-30 15:13:25,323 - Paper Collector - INFO - reference_search: Retrieving reference for paper '9e57dda195973c4b6c81386b1cc44595ecfd4697'\n",
      "INFO:Paper Collector:reference_search: Retrieving reference for paper '9e57dda195973c4b6c81386b1cc44595ecfd4697'\n",
      "2025-04-30 15:13:25,325 - Paper Collector - INFO - reference_search: Retrieving reference for paper '69b53faee7ce5c007e4d3e3ea532818ed8d0645d'\n",
      "INFO:Paper Collector:reference_search: Retrieving reference for paper '69b53faee7ce5c007e4d3e3ea532818ed8d0645d'\n",
      "2025-04-30 15:13:25,326 - Paper Collector - INFO - reference_search: Retrieving reference for paper '9f3ae8055e227edb413c54417c9c216f1f554f52'\n",
      "INFO:Paper Collector:reference_search: Retrieving reference for paper '9f3ae8055e227edb413c54417c9c216f1f554f52'\n",
      "2025-04-30 15:13:25,327 - Paper Collector - INFO - reference_search: Added 368 new unique referenced papers and 368 reference relationships.\n",
      "INFO:Paper Collector:reference_search: Added 368 new unique referenced papers and 368 reference relationships.\n",
      "2025-04-30 15:13:26,191 - SemanticScholarKit - WARNING - Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'AI research workflow automatio...'). Underlying tenacity retries failed. Attempting outer retry 3/21. Retrying after 3.0 seconds...\n",
      "WARNING:SemanticScholarKit:Rate limit (429) hit (via RetryError) for search_paper (ID/Query/URL: query:'AI research workflow automatio...'). Underlying tenacity retries failed. Attempting outer retry 3/21. Retrying after 3.0 seconds...\n",
      "2025-04-30 15:13:32,236 - Paper Collector - INFO - topic_search: Added 476 new papers and 500 topic links to data pool.\n",
      "INFO:Paper Collector:topic_search: Added 476 new papers and 500 topic links to data pool.\n",
      "2025-04-30 15:13:32,238 - Paper Collector - INFO - consolidated_search: Sub-tasks finished.\n",
      "INFO:Paper Collector:consolidated_search: Sub-tasks finished.\n",
      "2025-04-30 15:13:32,239 - Paper Collector - INFO - consolidated_search: Finished.\n",
      "INFO:Paper Collector:consolidated_search: Finished.\n"
     ]
    }
   ],
   "source": [
    "await ps.consolidated_search(\n",
    "    topics = topics,\n",
    "    paper_titles = None,\n",
    "    paper_ids = None,\n",
    "    author_ids = author_ids,\n",
    "    author_paper_ids = None,\n",
    "    ref_paper_ids = ref_ids,\n",
    "    cit_paper_ids = cit_ids,\n",
    "    pos_paper_ids = pos_paper_ids,\n",
    "    neg_paper_ids = neg_paper_ids,\n",
    "    author_limit = 10,\n",
    "    search_limit = ps.search_limit,\n",
    "    citation_limit = ps.citation_limit,\n",
    "    recommend_limit = ps.recommend_limit,\n",
    "    from_dt = ps.from_dt,\n",
    "    to_dt = ps.to_dt,\n",
    "    fields_of_study = ps.fields_of_study\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d4636",
   "metadata": {},
   "source": [
    "Paper Post-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac2cd4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:13:55,969 - Paper Collector - INFO - post_process: Starting data processing...\n",
      "INFO:Paper Collector:post_process: Starting data processing...\n",
      "2025-04-30 15:13:55,971 - Paper Collector - INFO - Processing 2219 raw paper entries...\n",
      "INFO:Paper Collector:Processing 2219 raw paper entries...\n",
      "2025-04-30 15:13:56,023 - Paper Collector - INFO - Generated 22769 nodes/edges from papers.\n",
      "INFO:Paper Collector:Generated 22769 nodes/edges from papers.\n",
      "2025-04-30 15:13:56,026 - Paper Collector - INFO - Found 1368 paper nodes missing abstracts. Attempting to supplement...\n",
      "INFO:Paper Collector:Found 1368 paper nodes missing abstracts. Attempting to supplement...\n",
      "2025-04-30 15:13:56,027 - Paper Collector - INFO - supplement_abstract: Fetching abstracts for 1368 papers...\n",
      "INFO:Paper Collector:supplement_abstract: Fetching abstracts for 1368 papers...\n",
      "2025-04-30 15:13:56,027 - SemanticScholarKit - INFO - get_papers: Creating 14 tasks for 14 IDs.\n",
      "INFO:SemanticScholarKit:get_papers: Creating 14 tasks for 14 IDs.\n",
      "2025-04-30 15:13:56,028 - SemanticScholarKit - INFO - get_papers: Gathering 14 tasks...\n",
      "INFO:SemanticScholarKit:get_papers: Gathering 14 tasks...\n",
      "2025-04-30 15:13:58,071 - SemanticScholarKit - INFO - get_papers: Gather complete. Processing results.\n",
      "INFO:SemanticScholarKit:get_papers: Gather complete. Processing results.\n",
      "2025-04-30 15:13:58,074 - Paper Collector - INFO - supplement_abstract: Found abstracts for 989/1368 papers.\n",
      "INFO:Paper Collector:supplement_abstract: Found abstracts for 989/1368 papers.\n",
      "2025-04-30 15:13:58,075 - Paper Collector - INFO - Successfully supplemented abstracts for 989 papers.\n",
      "INFO:Paper Collector:Successfully supplemented abstracts for 989 papers.\n",
      "2025-04-30 15:13:58,076 - Paper Collector - INFO - Total items after paper processing: 22769\n",
      "INFO:Paper Collector:Total items after paper processing: 22769\n",
      "2025-04-30 15:13:58,077 - Paper Collector - INFO - Processing 33 raw author entries...\n",
      "INFO:Paper Collector:Processing 33 raw author entries...\n",
      "2025-04-30 15:13:58,078 - Paper Collector - INFO - Generated 43 nodes/edges from authors. Total items: 22812\n",
      "INFO:Paper Collector:Generated 43 nodes/edges from authors. Total items: 22812\n",
      "2025-04-30 15:13:58,078 - Paper Collector - INFO - Processing 525 raw topic link entries...\n",
      "INFO:Paper Collector:Processing 525 raw topic link entries...\n",
      "2025-04-30 15:13:58,080 - Paper Collector - INFO - Generated 530 nodes/edges from topics. Total items: 23342\n",
      "INFO:Paper Collector:Generated 530 nodes/edges from topics. Total items: 23342\n",
      "2025-04-30 15:13:58,081 - Paper Collector - INFO - Processing 443 raw citation entries (368 refs, 75 citings)...\n",
      "INFO:Paper Collector:Processing 443 raw citation entries (368 refs, 75 citings)...\n",
      "2025-04-30 15:13:58,082 - Paper Collector - INFO - Generated 439 citation relationships. Total items: 23781\n",
      "INFO:Paper Collector:Generated 439 citation relationships. Total items: 23781\n"
     ]
    }
   ],
   "source": [
    "await ps.post_process(if_supplement_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2401ef",
   "metadata": {},
   "source": [
    "Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d7b7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nodes_json = [node for node in ps.nodes_json if node['labels'] == ['Topic']\n",
    "                    if node['properties'].get('description') is not None]\n",
    "paper_nodes_json = [node for node in ps.nodes_json if node['labels'] == ['Paper'] \n",
    "                    and node['properties'].get('title') is not None and node['properties'].get('abstract') is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89bdfa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ids = [x['id'] for x in topic_nodes_json]\n",
    "paper_ids = [x['id'] for x in paper_nodes_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e906213",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name_ref = {x['id']:x['properties']['name'] for x in topic_nodes_json}\n",
    "paper_title_ref = {x['id']:x['properties']['title'] for x in paper_nodes_json}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a7162b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity calculation\n",
    "from collect.paper_sim_calc import PaperSim\n",
    "sim_calc = PaperSim(embed_api_key, embed_model_name)\n",
    "\n",
    "await sim_calc.get_topic_embeds(topic_nodes_json, embed_api_key, embed_model_name)\n",
    "await sim_calc.get_abstract_embeds(paper_nodes_json, embed_api_key, embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a491d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_sim_pool = await sim_calc.cal_p2p_similarity(paper_nodes_json, paper_ids, paper_ids)\n",
    "p2t_sim_pool = await sim_calc.cal_p2t_similarity(paper_nodes_json, topic_nodes_json, paper_ids, topic_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c3b1f",
   "metadata": {},
   "source": [
    "2-Hop Filter\n",
    "- 1-hop of similar papers to core\n",
    "- 1-hop of papers of citation chain\n",
    "- similar papers to topic\n",
    "- (if possible) 2-hop of similar papers to 1-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "270e2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_paper_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45a37099",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_paper_ids.update(core_paper_ids)  # first append core paper ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87016aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 218\n"
     ]
    }
   ],
   "source": [
    "# add papers similar to core paper ids\n",
    "i = 0\n",
    "for item in p2p_sim_pool:  # iterate paper -> SIMILAR_TO -> paper\n",
    "    start_id = item['startNodeId']\n",
    "    end_id = item['endNodeId']\n",
    "    if start_id in core_paper_ids and end_id not in core_paper_ids:\n",
    "        hop_1_paper_ids.add(end_id)\n",
    "        i += 1\n",
    "    elif start_id not in core_paper_ids and end_id in core_paper_ids:\n",
    "        hop_1_paper_ids.add(start_id)\n",
    "        i += 1\n",
    "print(i, len(hop_1_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99c405c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435 554\n"
     ]
    }
   ],
   "source": [
    "# add papers belong to core paper ids's citation chain\n",
    "i = 0\n",
    "for item in ps.edges_json:\n",
    "    relationship = item.get('relationshipType')\n",
    "    if relationship == 'CITES':\n",
    "        start_id = item['startNodeId']\n",
    "        end_id = item['endNodeId']\n",
    "        if start_id in core_paper_ids and end_id not in core_paper_ids:\n",
    "            hop_1_paper_ids.add(end_id)\n",
    "            i += 1\n",
    "        elif start_id not in core_paper_ids and end_id in core_paper_ids:\n",
    "            hop_1_paper_ids.add(start_id)\n",
    "            i += 1\n",
    "print(i, len(hop_1_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add papers similar to topic nodes\n",
    "paper_topic_tuple = set((x['startNodeId'], x['endNodeId']) for x in p2t_sim_pool)  # paper similar to nodes\n",
    "\n",
    "i = 0\n",
    "for edge in ps.edges_json:\n",
    "    if edge.get('relationshipType') == 'DISCUSS':  # paper discuss the topic\n",
    "        if (edge['startNodeId'], edge['endNodeId']) in paper_topic_tuple:  # paper similar to topic\n",
    "            # print(topic_name_ref.get(edge['endNodeId']), paper_title_ref.get(edge['startNodeId']))\n",
    "            hop_1_paper_ids.add(edge['startNodeId'])\n",
    "            i += 1\n",
    "print(i, len(hop_1_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a313429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6637 820\n"
     ]
    }
   ],
   "source": [
    "# 2-hop extension\n",
    "hop_2_paper_ids = set()\n",
    "\n",
    "i = 0\n",
    "for item in p2p_sim_pool:  # iterate paper -> SIMILAR_TO -> paper\n",
    "    start_id = item['startNodeId']\n",
    "    end_id = item['endNodeId']\n",
    "    if start_id in hop_1_paper_ids and end_id not in hop_1_paper_ids:\n",
    "        hop_2_paper_ids.add(end_id)\n",
    "        i += 1\n",
    "    elif start_id not in hop_1_paper_ids and end_id in hop_1_paper_ids:\n",
    "        hop_2_paper_ids.add(start_id)\n",
    "        i += 1\n",
    "print(i, len(hop_2_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67e8fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 820 1389\n"
     ]
    }
   ],
   "source": [
    "print(len(hop_1_paper_ids), len(hop_2_paper_ids), len(set(hop_1_paper_ids) | set(hop_2_paper_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7dfd3",
   "metadata": {},
   "source": [
    "Graph Pruning based on hop 1 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "570ebdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3152 2820\n"
     ]
    }
   ],
   "source": [
    "# for nodes\n",
    "hop_1_author_ids = set()\n",
    "\n",
    "i = 0\n",
    "for item in ps.edges_json:\n",
    "    relationship = item.get('relationshipType')\n",
    "    if relationship == 'WRITES':\n",
    "        start_id = item['startNodeId']\n",
    "        end_id = item['endNodeId']\n",
    "        if end_id in hop_1_paper_ids:\n",
    "            hop_1_author_ids.add(start_id)\n",
    "            i += 1\n",
    "print(i, len(hop_1_author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6073707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 5\n"
     ]
    }
   ],
   "source": [
    "# for nodes\n",
    "hop_1_topic_ids = set()\n",
    "\n",
    "i = 0\n",
    "for item in ps.edges_json:\n",
    "    relationship = item.get('relationshipType')\n",
    "    if relationship == 'DISCUSS':\n",
    "        start_id = item['startNodeId']\n",
    "        end_id = item['endNodeId']\n",
    "        if start_id in hop_1_paper_ids:\n",
    "            hop_1_topic_ids.add(end_id)\n",
    "            i += 1\n",
    "print(i, len(hop_1_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "476ff9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter node json\n",
    "node_ids = hop_1_paper_ids | hop_1_author_ids | hop_1_topic_ids\n",
    "nodes_json = [x for x in ps.nodes_json if x['id'] in node_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7094c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3730 3730\n"
     ]
    }
   ],
   "source": [
    "# filter edge json\n",
    "edges_json = []\n",
    "\n",
    "i = 0\n",
    "for edge in ps.edges_json:\n",
    "    relationship = edge.get('relationshipType')\n",
    "    if relationship in ['CITES', 'WRITES', 'DISCUSS']:\n",
    "        start_id = edge['startNodeId']\n",
    "        end_id = edge['endNodeId']\n",
    "        if start_id in node_ids and end_id in node_ids:\n",
    "            edges_json.append(edge)\n",
    "            i += 1\n",
    "print(i, len(edges_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92da2d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9004 12734\n",
      "142 12876\n"
     ]
    }
   ],
   "source": [
    "# append similarity edges\n",
    "i = 0\n",
    "for edge in p2p_sim_pool:  # iterate paper -> SIMILAR_TO -> paper\n",
    "    start_id = edge['startNodeId']\n",
    "    end_id = edge['endNodeId']\n",
    "    if start_id in node_ids and end_id in node_ids:\n",
    "        edges_json.append(edge)\n",
    "        i += 1\n",
    "print(i, len(edges_json))\n",
    "\n",
    "i = 0\n",
    "for edge in p2t_sim_pool:  # iterate paper -> DISCUSS -> topic\n",
    "    start_id = edge['startNodeId']\n",
    "    end_id = edge['endNodeId']\n",
    "    if start_id in node_ids and end_id in node_ids:\n",
    "        edges_json.append(edge)\n",
    "        i += 1\n",
    "print(i, len(edges_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92bef7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9941 13917 3514 12876\n"
     ]
    }
   ],
   "source": [
    "print(len(ps.nodes_json), len(ps.edges_json), \n",
    "      len(nodes_json), len(edges_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b6c3b",
   "metadata": {},
   "source": [
    "Generate paper graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0cb7fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate paper graph from nodes / edges json\n",
    "G_pre = PaperGraph(name='Paper Graph Pre')\n",
    "G_pre.add_graph_nodes(nodes_json)\n",
    "G_pre.add_graph_edges(edges_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0c8a84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nodes_stats(paper_graph):\n",
    "    \"\"\"calculate statistics for paper node\"\"\"\n",
    "    # ---------- 1. Initiate stats  ------------\n",
    "    # for paper stats\n",
    "    paper_stats = {\n",
    "        'id': [],\n",
    "        'citationCount': [],\n",
    "        'influentialCitationCount': [],\n",
    "        'referenceCount': [],\n",
    "        'monthlyCitationCount': [],\n",
    "        'localCitationCount': [],\n",
    "        'localReferenceCount': [],\n",
    "        'localSimilarPaperCont': [],\n",
    "        'maxSimilarityToSeedPapers': [],\n",
    "        'avgSimilarityToSeedPapers': []\n",
    "    }\n",
    "\n",
    "    # for author stats\n",
    "    author_stats = {\n",
    "        'id': [],\n",
    "        'paperCount': [],\n",
    "        'citationCount': [],\n",
    "        'hIndex': [],\n",
    "        'localPaperCount': [],\n",
    "    }\n",
    "\n",
    "    # iterate paper node to calculate measurements\n",
    "    for nid, node_data in paper_graph.nodes(data=True):\n",
    "        # ---------- 2. Calculate paper stats  ------------\n",
    "        if node_data.get('nodeType') == 'Paper':\n",
    "            paper_stats['id'].append(nid)\n",
    "            # global measurement\n",
    "            pub_dt = node_data.get('publicationDate')\n",
    "            paper_cit_cnt = node_data.get('citationCount')\n",
    "            paper_stats['citationCount'].append(paper_cit_cnt)\n",
    "            paper_stats['influentialCitationCount'].append(node_data.get('influentialCitationCount'))\n",
    "            paper_stats['referenceCount'].append(node_data.get('referenceCount'))\n",
    "\n",
    "            # generated measurement\n",
    "            if pub_dt:\n",
    "                try:\n",
    "                    tm_to_dt = relativedelta(datetime.now().date(), datetime.strptime(pub_dt, '%Y-%m-%d').date())\n",
    "                    mth_to_dt = tm_to_dt.years * 12 + tm_to_dt.months\n",
    "                    if paper_cit_cnt is not None and mth_to_dt > 0:\n",
    "                        paper_mthly_cit_cnt = paper_cit_cnt / mth_to_dt\n",
    "                        paper_stats['monthlyCitationCount'].append(paper_mthly_cit_cnt)\n",
    "                except ValueError:\n",
    "                    paper_stats['monthlyCitationCount'].append(None)\n",
    "\n",
    "            # local measurement\n",
    "            # other paper -> CITES -> this paper\n",
    "            in_edges_info = paper_graph.in_edges(nid, data=True)\n",
    "            paper_loc_cit_cnt = sum([1 for _, _, edge_data in in_edges_info if edge_data['relationshipType'] == 'CITES'])\n",
    "            paper_stats['localCitationCount'].append(paper_loc_cit_cnt)\n",
    "\n",
    "            # this paper -> CITES -> other paper\n",
    "            out_edges_info = paper_graph.out_edges(nid, data=True)\n",
    "            paper_loc_ref_cnt = sum([1 for _, _, edge_data in out_edges_info if edge_data['relationshipType'] == 'CITES'])\n",
    "            paper_stats['localReferenceCount'].append(paper_loc_ref_cnt)\n",
    "\n",
    "            # local similar papers count\n",
    "            paper_loc_sim_cnt_1 = sum([1 for _, _, edge_data in in_edges_info if edge_data['relationshipType'] == 'SIMILAR_TO'])\n",
    "            paper_loc_sim_cnt_2 = sum([1 for _, _, edge_data in out_edges_info if edge_data['relationshipType'] == 'SIMILAR_TO'])\n",
    "            paper_stats['localSimilarPaperCont'].append((paper_loc_sim_cnt_1 + paper_loc_sim_cnt_2))\n",
    "\n",
    "            # similarity score to core papers\n",
    "            paper_sim_to_core_papers_1 = [edge_data['weight'] for u, _, edge_data in in_edges_info \n",
    "                                          if edge_data['relationshipType'] == 'SIMILAR_TO'and u in core_paper_ids]\n",
    "            paper_sim_to_core_papers_2 = [edge_data['weight'] for _, v, edge_data in out_edges_info \n",
    "                                          if edge_data['relationshipType'] == 'SIMILAR_TO'and v in core_paper_ids]\n",
    "            sims_score = paper_sim_to_core_papers_1 + paper_sim_to_core_papers_2\n",
    "            if sims_score:\n",
    "                max_sim_score = max(sims_score)\n",
    "                avg_sim_score = np.average(sims_score)\n",
    "            else:\n",
    "                max_sim_score = None\n",
    "                avg_sim_score = None\n",
    "            paper_stats['maxSimilarityToSeedPapers'].append(max_sim_score)\n",
    "            paper_stats['avgSimilarityToSeedPapers'].append(avg_sim_score)\n",
    "\n",
    "        # ---------- 3. Calculate author stats  ------------\n",
    "        elif node_data.get('nodeType') == 'Author':\n",
    "            author_stats['id'].append(nid)\n",
    "            # global measurement\n",
    "            author_stats['paperCount'].append(node_data.get('paperCount'))\n",
    "            author_stats['citationCount'].append(node_data.get('citationCount'))\n",
    "            author_stats['hIndex'].append(node_data.get('hIndex'))\n",
    "\n",
    "            # local measurement\n",
    "            successors = paper_graph.successors(nid)\n",
    "            author_loc_paper_cnt = sum([1 for x in successors if paper_graph.nodes[x].get('nodeType') == 'Paper'])\n",
    "            author_stats['localPaperCount'].append(author_loc_paper_cnt)\n",
    "\n",
    "    node_stats = {'paper_stats': paper_stats, 'author_stats': author_stats}\n",
    "\n",
    "    return node_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2c914759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate paper / author node stats\n",
    "node_stats = gen_nodes_stats(G_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0e86de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = node_stats['paper_stats']['id']\n",
    "paper_local_cit_cnt = node_stats['paper_stats']['localCitationCount']\n",
    "paper_local_cit_ref = {id: cit_cnt for id, cit_cnt in zip(paper_ids, paper_local_cit_cnt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f184c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank order by local citations\n",
    "paper_local_cit_sorted = sorted(paper_local_cit_ref.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d95e68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_paper_ids = set()\n",
    "for item in paper_local_cit_sorted:\n",
    "    if item[1] > len(core_paper_ids):\n",
    "        crossref_paper_ids.add(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d8340c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross ref insufficient\n"
     ]
    }
   ],
   "source": [
    "if len(crossref_paper_ids) < 20:\n",
    "    print(\"cross ref insufficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "edea2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all citations from core papers\n",
    "core_cit_paper_ids = set()\n",
    "\n",
    "for u, v, edge_data in G_pre.edges(data=True):\n",
    "    if u in core_paper_ids:\n",
    "        core_cit_paper_ids.add(v)\n",
    "    elif v in core_paper_ids:\n",
    "        core_cit_paper_ids.add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "89f50cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = node_stats['paper_stats']['id']\n",
    "paper_local_sim_scores = node_stats['paper_stats']['maxSimilarityToSeedPapers']\n",
    "paper_local_sim_ref = {id: sim_score for id, sim_score in zip(paper_ids, paper_local_sim_scores) if sim_score is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "96a64421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank order by local citations\n",
    "paper_local_sim_sorted = sorted(paper_local_sim_ref.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "00d3cc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': 'c890b28a001c885d1f7aa05f5d24ead9bf6ae058',\n",
       " 'title': 'MARG: Multi-Agent Review Generation for Scientific Papers',\n",
       " 'year': 2024,\n",
       " 'referenceCount': 0,\n",
       " 'citationCount': 27,\n",
       " 'influentialCitationCount': 0,\n",
       " 'isOpenAccess': True,\n",
       " 'openAccessPdf': {'url': 'https://arxiv.org/pdf/2401.04259.pdf'},\n",
       " 'fieldsOfStudy': ['Computer Science'],\n",
       " 'publicationDate': '2024-01-08',\n",
       " 'arxivUrl': 'https://arxiv.org/abs/2401.04259',\n",
       " 'arxivId': '2401.04259',\n",
       " 'doi': '10.48550/arXiv.2401.04259',\n",
       " 'abstract': 'We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).',\n",
       " 'nodeType': 'Paper'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_pre.nodes['c890b28a001c885d1f7aa05f5d24ead9bf6ae058']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534db3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_paper_significant(paper_stats):\n",
    "    \"\"\"identify significant paper node\"\"\"\n",
    "    paper_stats_df = pd.DataFrame(paper_stats)\n",
    "\n",
    "    sig_paper_ids, sig_paper_info = [], []\n",
    "    for index, row in paper_stats_df.iterrows():\n",
    "        pid = row['id']\n",
    "        # rule 1: global citation greater than or equal to 20\n",
    "        if row.get('citationCount', -1) >= 20:\n",
    "            sig_paper_ids.append(pid)\n",
    "            sig_paper_info.append('citationCount')\n",
    "\n",
    "        # rule 2: influential citation greater than or equal to 3\n",
    "        elif row.get('influentialCitationCount', -1) >= 3:\n",
    "            sig_paper_ids.append(pid)\n",
    "            sig_paper_info.append('influentialCitationCount')\n",
    "\n",
    "        # rule 3: monthly citation greater than or equal to 5\n",
    "        elif row.get('monthlyCitationCount', -1) >= 5:\n",
    "            sig_paper_ids.append(pid)\n",
    "            sig_paper_info.append('monthlyCitationCount')\n",
    "\n",
    "        # rule 4: local citation greater than or equal to 5\n",
    "        elif row.get('localCitationCount', -1) >= 5:\n",
    "            sig_paper_ids.append(pid)\n",
    "            sig_paper_info.append('localCitationCount')\n",
    "\n",
    "\n",
    "def identify_author_significant(author_stats):\n",
    "    \"\"\"identify significant paper node\"\"\"\n",
    "    author_stats_df = pd.DataFrame(author_stats)\n",
    "\n",
    "    sig_author_ids, sig_author_info = [], []\n",
    "    for index, row in author_stats_df.iterrows():\n",
    "        aid = row['id']\n",
    "        # rule 1: h-index greater than or equal to 10\n",
    "        if row.get('hIndex', -1) >= 10:\n",
    "            sig_author_ids.append(aid)\n",
    "            sig_author_info.append('hIndex')\n",
    "\n",
    "        # rule 2: average paper ciation greater than or equal to 20\n",
    "        elif row.get('paperCount', -1) > 0 and row.get('citationCount', -1) > 0 and row.get('citationCount', -1) / row.get('paperCount', -1) >= 20:\n",
    "            sig_author_ids.append(aid)\n",
    "            sig_author_info.append('avgPaperCitation')\n",
    "\n",
    "        # rule 4: local citation greater than or equal to 5\n",
    "        elif row.get('localPaperCount', -1) >= 5:\n",
    "            sig_author_ids.append(aid)\n",
    "            sig_author_info.append('localPaperCount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "49692a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial significant authors: {'A1', 'A3', 'A2'}\n",
      "Initial significant papers: {'P1'}\n",
      "Propagation thresholds: k=2, m=1, n=1\n",
      "\n",
      "Iteration 1: Identified 5 new candidate significant nodes.\n",
      "Iteration 2: Identified 1 new candidate significant nodes.\n",
      "Propagation stabilized at iteration 2.\n",
      "\n",
      "--- Propagation Results ---\n",
      "Found 6 candidate significant nodes:\n",
      "Candidate significant node IDs: ['P7', 'P6', 'P4', 'P5', 'P2', 'P3']\n",
      "\n",
      "Detailed Reasons:\n",
      "  Node P2:\n",
      "    - Rule 3.2: Cited by 1 significant papers (threshold m=1)\n",
      "  Node P3:\n",
      "    - Rule 3.3: Cites 1 significant papers (threshold n=1)\n",
      "  Node P4:\n",
      "    - Rule 3.3: Cites 1 significant papers (threshold n=1)\n",
      "  Node P5:\n",
      "    - Rule 3.1: Written by 3 significant authors (threshold k=2)\n",
      "  Node P6:\n",
      "    - Rule 3.2: Cited by 1 significant papers (threshold m=1)\n",
      "    - Rule 3.3: Cites 1 significant papers (threshold n=1)\n",
      "  Node P7:\n",
      "    - Rule 3.2: Cited by 1 significant papers (threshold m=1)\n",
      "\n",
      "Algorithm execution time: 0.0006 seconds\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import time # For demo or debugging iteration process\n",
    "\n",
    "def propagate_labels(\n",
    "    paper_graph: nx.MultiDiGraph,\n",
    "    sig_paper_ids: set,\n",
    "    sig_author_ids: set,\n",
    "    k: int,\n",
    "    m: int,\n",
    "    n: int\n",
    ") -> tuple[list, dict]:\n",
    "    \"\"\"\n",
    "    Performs label propagation on a MultiDiGraph to find potentially significant nodes.\n",
    "\n",
    "    Args:\n",
    "        paper_graph (nx.MultiDiGraph): The graph containing paper and author nodes,\n",
    "                                        and edges of type 'WRITES' and 'CITES'.\n",
    "                                        Assumes edges have a 'type' attribute for distinction.\n",
    "        sig_paper_ids (set): Set of initial significant paper node IDs.\n",
    "        sig_author_ids (set): Set of initial significant author node IDs.\n",
    "        k (int): Threshold for rule 3.1 (author -> WRITES -> paper).\n",
    "                 A paper becomes potentially significant if written by k or more significant authors.\n",
    "        m (int): Threshold for rule 3.2 (other paper -> CITES -> this paper).\n",
    "                 A paper becomes potentially significant if cited by m or more significant papers.\n",
    "        n (int): Threshold for rule 3.3 (this paper -> CITES -> other paper).\n",
    "                 A paper becomes potentially significant if it cites n or more significant papers.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list, dict]:\n",
    "            - candidate_sig_node_ids (list): List of identified potentially significant node IDs (excluding initial ones).\n",
    "            - candidate_sig_node_info (dict): Dictionary where keys are candidate node IDs\n",
    "                                              and values are lists of reasons why the node was identified.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure inputs are sets for efficient lookup\n",
    "    initial_significant_nodes = set(sig_paper_ids).union(set(sig_author_ids))\n",
    "    current_significant_nodes = initial_significant_nodes.copy()\n",
    "\n",
    "    # Store the final output: candidate significant nodes and their reasons\n",
    "    candidate_sig_node_ids = set()\n",
    "    candidate_sig_node_info = defaultdict(list)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        newly_identified_nodes = set() # Nodes newly identified in this iteration\n",
    "\n",
    "        # Iterate through all nodes in the graph, checking if they meet the criteria\n",
    "        # Note: Only check nodes that are not currently significant\n",
    "        nodes_to_check = set(paper_graph.nodes()) - current_significant_nodes\n",
    "\n",
    "        for node_id in nodes_to_check:\n",
    "            reasons_for_this_node = []\n",
    "\n",
    "            # --- Check Rule 3.1 (author -> WRITES -> paper) ---\n",
    "            # Assumes only paper nodes have incoming 'WRITES' edges\n",
    "            # and only author nodes have outgoing 'WRITES' edges\n",
    "            significant_authors_count = 0\n",
    "            authors = set() # Store distinct author IDs writing this paper\n",
    "            try:\n",
    "                for u, v, data in paper_graph.in_edges(node_id, data=True):\n",
    "                     # Ensure it's a WRITES relationship and the source 'u' is significant.\n",
    "                     # A more robust way might be checking node type attributes if they exist.\n",
    "                     # Here we infer u is author, v is paper based on 'WRITES' edge type.\n",
    "                    if data.get('type') == 'WRITES' and u in current_significant_nodes:\n",
    "                        # Use a set to ensure each author is counted only once, even with parallel edges (unlikely)\n",
    "                        authors.add(u)\n",
    "                significant_authors_count = len(authors)\n",
    "\n",
    "                if significant_authors_count >= k:\n",
    "                    reason = f\"Rule 3.1: Written by {significant_authors_count} significant authors (threshold k={k})\"\n",
    "                    reasons_for_this_node.append(reason)\n",
    "            except KeyError:\n",
    "                # If the node has no incoming edges, skip this check\n",
    "                pass\n",
    "\n",
    "            # --- Check Rule 3.2 (other paper -> CITES -> this paper) ---\n",
    "            # Assumes only paper nodes have 'CITES' type edges between them\n",
    "            significant_citing_papers_count = 0\n",
    "            citing_papers = set() # Store distinct significant paper IDs citing this one\n",
    "            try:\n",
    "                for u, v, data in paper_graph.in_edges(node_id, data=True):\n",
    "                    # Ensure it's a CITES relationship\n",
    "                    if data.get('type') == 'CITES' and u in current_significant_nodes:\n",
    "                         # Infer u is a paper by checking if it's in the significant set\n",
    "                         # Use a set to count each citing paper only once\n",
    "                        citing_papers.add(u)\n",
    "                significant_citing_papers_count = len(citing_papers)\n",
    "\n",
    "                if significant_citing_papers_count >= m:\n",
    "                    reason = f\"Rule 3.2: Cited by {significant_citing_papers_count} significant papers (threshold m={m})\"\n",
    "                    reasons_for_this_node.append(reason)\n",
    "            except KeyError:\n",
    "                # If the node has no incoming edges, skip this check\n",
    "                pass\n",
    "\n",
    "            # --- Check Rule 3.3 (this paper -> CITES -> other paper) ---\n",
    "            significant_cited_papers_count = 0\n",
    "            cited_papers = set() # Store distinct significant paper IDs cited by this one\n",
    "            try:\n",
    "                for u, v, data in paper_graph.out_edges(node_id, data=True):\n",
    "                    # Ensure it's a CITES relationship\n",
    "                    if data.get('type') == 'CITES' and v in current_significant_nodes:\n",
    "                        # Infer v is a paper by checking if it's in the significant set\n",
    "                        # Use a set to count each cited paper only once\n",
    "                        cited_papers.add(v)\n",
    "                significant_cited_papers_count = len(cited_papers)\n",
    "\n",
    "                if significant_cited_papers_count >= n:\n",
    "                    reason = f\"Rule 3.3: Cites {significant_cited_papers_count} significant papers (threshold n={n})\"\n",
    "                    reasons_for_this_node.append(reason)\n",
    "            except KeyError:\n",
    "                 # If the node has no outgoing edges, skip this check\n",
    "                pass\n",
    "\n",
    "            # If this node was identified by any rule\n",
    "            if reasons_for_this_node:\n",
    "                newly_identified_nodes.add(node_id)\n",
    "                # Record the reason(s). Add new reasons even if recorded before\n",
    "                # (could be a new iteration or a different rule met).\n",
    "                # Only add to the final output list if it's a *newly found* candidate node.\n",
    "                if node_id not in candidate_sig_node_ids:\n",
    "                     candidate_sig_node_info[node_id].extend(reasons_for_this_node)\n",
    "                else:\n",
    "                    # If node was identified before, append new reasons.\n",
    "                    # The defaultdict handles appending smoothly.\n",
    "                     candidate_sig_node_info[node_id].extend(reasons_for_this_node)\n",
    "\n",
    "\n",
    "        # Check if any new nodes were identified; if not, stability reached, exit loop\n",
    "        if not newly_identified_nodes:\n",
    "            print(f\"Propagation stabilized at iteration {iteration-1}.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Iteration {iteration}: Identified {len(newly_identified_nodes)} new candidate significant nodes.\")\n",
    "            # Update the set of current significant nodes for the next iteration\n",
    "            current_significant_nodes.update(newly_identified_nodes)\n",
    "            # Add the newly identified nodes (that weren't initial) to the final candidate list\n",
    "            candidate_sig_node_ids.update(newly_identified_nodes - initial_significant_nodes)\n",
    "\n",
    "        # Safety measure: prevent infinite loops (e.g., due to graph structure or oscillating rules)\n",
    "        # You could add a max iteration limit\n",
    "        # max_iterations = 100 # Example limit\n",
    "        # if iteration > max_iterations:\n",
    "        #     print(f\"Reached maximum iteration limit ({max_iterations}).\")\n",
    "        #     break\n",
    "\n",
    "    # Clean up candidate_sig_node_info to only contain info for nodes in the final candidate_sig_node_ids set\n",
    "    # Also, remove duplicate reason strings for clarity\n",
    "    final_candidate_info = {node_id: list(set(reasons)) # Deduplicate reasons\n",
    "                           for node_id, reasons in candidate_sig_node_info.items()\n",
    "                           if node_id in candidate_sig_node_ids}\n",
    "\n",
    "\n",
    "    return list(candidate_sig_node_ids), final_candidate_info\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # 1. Create an example MultiDiGraph\n",
    "    paper_graph = nx.MultiDiGraph()\n",
    "\n",
    "    # Add nodes (explicitly adding node types might be better, but here we infer from edges)\n",
    "    # Authors\n",
    "    authors = {'A1', 'A2', 'A3', 'A4', 'A5'}\n",
    "    # Papers\n",
    "    papers = {'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7'}\n",
    "\n",
    "    paper_graph.add_nodes_from(authors)\n",
    "    paper_graph.add_nodes_from(papers)\n",
    "\n",
    "    # Add edges (author -> WRITES -> paper)\n",
    "    paper_graph.add_edge('A1', 'P1', type='WRITES')\n",
    "    paper_graph.add_edge('A2', 'P1', type='WRITES')\n",
    "    paper_graph.add_edge('A3', 'P2', type='WRITES')\n",
    "    paper_graph.add_edge('A1', 'P3', type='WRITES') # A1 wrote P3\n",
    "    paper_graph.add_edge('A4', 'P3', type='WRITES') # A4 wrote P3\n",
    "    paper_graph.add_edge('A5', 'P4', type='WRITES') # A5 wrote P4 (A5 is not significant)\n",
    "    paper_graph.add_edge('A1', 'P5', type='WRITES') # A1 wrote P5\n",
    "    paper_graph.add_edge('A2', 'P5', type='WRITES') # A2 wrote P5\n",
    "    paper_graph.add_edge('A3', 'P5', type='WRITES') # A3 wrote P5\n",
    "    paper_graph.add_edge('A4', 'P6', type='WRITES') # A4 wrote P6\n",
    "\n",
    "    # Add edges (paper -> CITES -> paper)\n",
    "    paper_graph.add_edge('P1', 'P2', type='CITES') # P1 cites P2\n",
    "    paper_graph.add_edge('P3', 'P1', type='CITES') # P3 cites P1\n",
    "    paper_graph.add_edge('P4', 'P1', type='CITES') # P4 cites P1\n",
    "    paper_graph.add_edge('P4', 'P3', type='CITES') # P4 cites P3\n",
    "    paper_graph.add_edge('P5', 'P6', type='CITES') # P5 cites P6\n",
    "    paper_graph.add_edge('P6', 'P7', type='CITES') # P6 cites P7 (P7 has no other connections)\n",
    "    paper_graph.add_edge('P1', 'P7', type='CITES') # P1 cites P7\n",
    "\n",
    "\n",
    "    # 2. Define initial significant nodes\n",
    "    sig_author_ids = {'A1', 'A2', 'A3'}\n",
    "    sig_paper_ids = {'P1'} # P1 is initially significant\n",
    "\n",
    "    # 3. Set propagation thresholds\n",
    "    k = 2 # At least 2 significant authors write a paper\n",
    "    m = 1 # Cited by at least 1 significant paper\n",
    "    n = 1 # Cites at least 1 significant paper\n",
    "\n",
    "    print(\"Initial significant authors:\", sig_author_ids)\n",
    "    print(\"Initial significant papers:\", sig_paper_ids)\n",
    "    print(f\"Propagation thresholds: k={k}, m={m}, n={n}\\n\")\n",
    "\n",
    "    # 4. Execute label propagation\n",
    "    start_time = time.time()\n",
    "    candidate_ids, candidate_info = propagate_labels(\n",
    "        paper_graph,\n",
    "        sig_paper_ids,\n",
    "        sig_author_ids,\n",
    "        k, m, n\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 5. Print results\n",
    "    print(\"\\n--- Propagation Results ---\")\n",
    "    print(f\"Found {len(candidate_ids)} candidate significant nodes:\")\n",
    "    print(\"Candidate significant node IDs:\", candidate_ids)\n",
    "\n",
    "    print(\"\\nDetailed Reasons:\")\n",
    "    if not candidate_info:\n",
    "        print(\"No new candidate significant nodes found.\")\n",
    "    else:\n",
    "        # Sort by node ID for consistent output, converting to list first if needed\n",
    "        sorted_candidate_ids = sorted(list(candidate_ids))\n",
    "        for node_id in sorted_candidate_ids:\n",
    "            print(f\"  Node {node_id}:\")\n",
    "            # Retrieve reasons, provide default if somehow missing\n",
    "            reasons = candidate_info.get(node_id, [\"No detailed reasons recorded\"])\n",
    "            # Sort reasons for consistent output\n",
    "            for reason in sorted(reasons):\n",
    "                print(f\"    - {reason}\")\n",
    "\n",
    "    print(f\"\\nAlgorithm execution time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    # Expected outcome verification:\n",
    "    # P1: Initial significant.\n",
    "    # P2: Cited by P1 (significant, m=1 met). Written by A3 (significant, k=1 < k=2). Expected candidate via citation.\n",
    "    # P3: Written by A1 (sig), A4 (non-sig) (k=1 < k=2). Cited by P4 (non-sig initially). Cites P1 (significant, n=1 met). Expected candidate via citation.\n",
    "    # P4: Written by A5 (non-sig). Cites P1 (sig), P3 (becomes sig). Expected candidate via citation (possibly in later round if P3 becomes sig).\n",
    "    # P5: Written by A1, A2, A3 (all sig) (k=3 >= k=2 met). Expected candidate via authors.\n",
    "    # P6: Cited by P5 (becomes sig, m=1 met). Written by A4 (non-sig). Cites P7. Expected candidate via being cited (after P5 becomes sig).\n",
    "    # P7: Cited by P6 (becomes sig) and P1 (sig) (m=2 >= m=1 met). Expected candidate via being cited (after P1 or P6 is sig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70501d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK\n",
    "请根据以下要求，构造一个图上的标签传播算法。请输出python代码。\n",
    "\n",
    "## INSTRUCTION\n",
    "1. 已有一个networkx的MultiDiGraph名为paper_graph. 其中的主要node类型为paper和author，主要的edge类型为 paper -> CITES -> paper, author -> WRITES -> paper；\n",
    "2. 已知该paper_graph中存在关键节点sig_paper_ids和sig_author_ids，对应为paper和author节点的id；\n",
    "3. 现希望基于以上关键节点做标签传播，找出图中更多的潜在关键节点来，传播规则如下：\n",
    "    - 3.1. 在 author -> WRITES -> paper关系中，如果paper节点对应有k个或以上的author节点为关键节点，则此paper节点为潜在的关键节点；\n",
    "    - 3.2. 在 other paper -> CITES -> this paper关系中，如果引用的other paper节点中有m个或以上为关键节点，则this paper节点为潜在的关键节点；\n",
    "    - 3.3. 在 this paper -> CITES -> other paper关系中，如果被引用的other paper节点中有n个或以上为关键节点，则this paper节点为潜在的关键节点；\n",
    "4. 关键节点/潜在关键节点可以进一步传播，直到稳定；\n",
    "5. 最终输出潜在关键节点列表 candidate_sig_node_ids 和 识别为潜在关键节点的原因 candidate_sig_node_info，二者一一对应。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a95bf9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c890b28a001c885d1f7aa05f5d24ead9bf6ae058', 0.8388),\n",
       " ('924956d6c788c9ea67ecdc80b63742d74350549e', 0.8349),\n",
       " ('d0b5194032451157f264db4a6da569f03347d1cb', 0.8272),\n",
       " ('987d0cbe751780b9b993ebf8e670fb0d18fdaabe', 0.8224),\n",
       " ('1191a81272747b2add72d16c6a1ff00d1d8b8b2f', 0.8153),\n",
       " ('9348b7b95982d0a675a767e92c23647aa6915a94', 0.8145),\n",
       " ('c7471d25c1cee7f1b4457a52190d0b1a69a024ea', 0.812),\n",
       " ('a0e76e07fec917f9e8dd11b096a6fd524c1a76f5', 0.8119),\n",
       " ('fa39f0cc3e1dcb001f53735ae3d174d308f34301', 0.8107),\n",
       " ('bfed3d4c959b64148811376965db84f77ea8292e', 0.8094),\n",
       " ('235992701c6e70872f5292fd5818d5c5719063de', 0.8049),\n",
       " ('94fb5a19f86d81a746bb5502a5debf2659814e8e', 0.8037),\n",
       " ('8a52ac9268ec522fe310c7b75df00d9aaa80efd0', 0.7977),\n",
       " ('0490dc8f2ee87418aa0a9d9f198a12895af15b98', 0.7961),\n",
       " ('62729cff7dda7614f648a84e8967076d8878a5ff', 0.7953),\n",
       " ('51b7b3ad7645a69e3c1c80cae69473b8bd472f67', 0.7949),\n",
       " ('e4eb81ad222ba047770d5a90bdd7406c138c6126', 0.7924),\n",
       " ('e7e46bc5ef80187084d2d2c626ca95e68ee6e74b', 0.7895),\n",
       " ('a509397430eca1cd3970082ed57f9c490ef6e3ae', 0.7894),\n",
       " ('c82f67d2a88d082ee28841f133494e60e8d39433', 0.7888),\n",
       " ('31d8be8bde2bf592b79481b21b955bddeb4b2a8f', 0.7883),\n",
       " ('ac01b7b37357f270f4dde45e9bffaec24869a647', 0.7877),\n",
       " ('bc54b3f4d00481a1b5231a458a7ba27f2f6a22ce', 0.7863),\n",
       " ('ec64fd9b864a5b525e70f4bf03088522facc0c47', 0.785),\n",
       " ('fc7e58340e84edf85023cac2894c51921ca8c501', 0.7845),\n",
       " ('3c00bc9650eb52b528d6630993b950cae6131cba', 0.7832),\n",
       " ('60c8a127e6ae8c8e21dd7edfc187ff7f0d9ae2bd', 0.7822),\n",
       " ('9678af027022328f143ee2b627ee93ba313a4e7c', 0.7821),\n",
       " ('82e4fb3d73d44c4ebfdd65fda4dc065eff8bf3a2', 0.7808),\n",
       " ('c17ca50bf93b19af56fe3dc3c14cc81dfbe9c29d', 0.7798),\n",
       " ('2424b7935cee3551deeea4a98b1a07abddf93649', 0.7768),\n",
       " ('92c82a51ad13c361d052987694cf93d6a72d5789', 0.7734),\n",
       " ('9acda8593fb9c70f724f6a7161be348d5a415e36', 0.7734),\n",
       " ('374d1e5fd7385353a4a0add1fadce23667662265', 0.773),\n",
       " ('8a816b4d4ee63804615dba39269948fda63b1c21', 0.7729),\n",
       " ('f2209eb5ac6747319a29b87dedabb97770be3243', 0.7726),\n",
       " ('caa21756d719cf970e5cc112659b2fd0041d8503', 0.7715),\n",
       " ('acfa5bdaab738890ac854c8d3169d407c4d64791', 0.7704),\n",
       " ('1e13c7a04a77a65ef1bae0748360c582a83bed91', 0.7704),\n",
       " ('f2d23bd0a60f46a5d99475977c2ad507b103eca7', 0.7704),\n",
       " ('5e70de28008488855ba10e04275d66ed5152689d', 0.7677),\n",
       " ('4ce81182c4069867e59cb167985345d01d6ba13a', 0.7675),\n",
       " ('115f5f5dd7e5f33da07037335698a6e89fa4a4a7', 0.7672),\n",
       " ('e09cae7f61b9480fd767f9b582ddd8f988e00e9b', 0.7659),\n",
       " ('95154330d74be6c79ac45e97b38ef0038e2b8e2a', 0.7657),\n",
       " ('1104bec9e7a0a3d9dba341ba8005f1b7350bc876', 0.7652),\n",
       " ('e2e5260de3c5768ac5df97e363890293240b41a2', 0.7637),\n",
       " ('d75d8f98bb910a9f4ef39258e2b2c2298db901d3', 0.7636),\n",
       " ('f7d0b807b9b933d03ec0145c829c18943860deac', 0.7635),\n",
       " ('8a35b005ada6ad6ce2f42608b0e88012b9be2ccd', 0.7632),\n",
       " ('ed5e56bde9c17db5329fd37b7611657077676e20', 0.7631),\n",
       " ('39983a80f111f7f6e793f02c5725a14bca76b32d', 0.7624),\n",
       " ('8071a14c38e707f4e1b773e4fdab7abc926c4e5c', 0.7619),\n",
       " ('f96e38338045a39e4fda0648aa95b68e0dc646c3', 0.7603),\n",
       " ('484e4198925ca8b415facf3bdb9d7735ab633621', 0.7603),\n",
       " ('2cbdad900dda764efa362793765ec12cb5ae66f5', 0.7596),\n",
       " ('7dfbe4882188dbac938306be93bc58b34450b87f', 0.7592),\n",
       " ('67f211fa66335dc32be2ec630b85a6f9026fa9de', 0.7592),\n",
       " ('684aaa287ab465599e74ca02e1cbbcbbb492ba94', 0.759),\n",
       " ('4c5d604add6178acef3e7ad71995608f96f01dad', 0.7589),\n",
       " ('28c6ac721f54544162865f41c5692e70d61bccab', 0.7574),\n",
       " ('713b2c3797b2bd3e80be6b85d9a1808797899eeb', 0.757),\n",
       " ('5d72dc9f1cdf618368c51e8a53474d6a9d581a7b', 0.7562),\n",
       " ('a3a2ce2de25a640c591b7ff836139510a96537df', 0.7558),\n",
       " ('6b17468813469de96852ba809388ffb54866889c', 0.7556),\n",
       " ('3bf29a0420b1042f5e0a319c27cb32d46d9cde3e', 0.7556),\n",
       " ('fd30d3189b3bc3295ddad05ac1f683ce41f5e9cb', 0.7549),\n",
       " ('33161a5a9b5dcb635b5a97475e6a6209a69ada7d', 0.7518),\n",
       " ('fdee3c8d37f99514d4044ad1bbabc28bd2af9cb6', 0.7518),\n",
       " ('7dfd0f91066a75c88ee86bbea9ccf16ab1ff8334', 0.7499),\n",
       " ('e7a7cace68d2232d4ecbc9994d60ac645ecd0702', 0.7499),\n",
       " ('deec3b70aea74f3abcde01ce8ca00b39eec5f157', 0.7496),\n",
       " ('2cfbb7c05182dec94b84daba57d033e583295e09', 0.7491),\n",
       " ('e1fef49cd96c4fba3cffc0d689425135f5741f47', 0.7489),\n",
       " ('b2acc0dafd13fa7f8075643c5b28fa7146d1d3d4', 0.7476),\n",
       " ('7aad5f1fa3071ded3bb8ce84e588a9717345eefc', 0.7476),\n",
       " ('79e57387a664a5d7b128b2790a8f38f827e09124', 0.7473),\n",
       " ('569d35124d8bf451ccbcf343ffb41292c3b811e1', 0.7467),\n",
       " ('4636c68fe1fa65c311c38f72b9f7d2f72b4891dc', 0.7463),\n",
       " ('fd8c64d0b912795e1cefc0aba4c6d90499132755', 0.7461),\n",
       " ('186a11b9010e79c1e8ce9768d9343b5cae449d0b', 0.7456),\n",
       " ('5091c414f301d5fb026d791b41ae2047dbcf7992', 0.7456),\n",
       " ('a4e996a6a3df1ac76a28b1e0513bbcb5449e4227', 0.7453),\n",
       " ('0251d301230802e2189e8721745b3a9f90d48386', 0.7431),\n",
       " ('e7923d133bb3d426ae50ed32d4eb12627892c8a6', 0.7419),\n",
       " ('4e383872d1f4b877a5b272a2739557973db23078', 0.7417),\n",
       " ('d1797bef42d5192be92db3d7ebd7aa4df114f7f1', 0.7412),\n",
       " ('023e113b11ff7bac182713a069fedcbcccad9562', 0.7408),\n",
       " ('8368c5f0ef020f8d081e60ca2572086482ce365b', 0.7407),\n",
       " ('414f89c5564cea16c256e7653862acc9f7ce2617', 0.7407),\n",
       " ('db9bcb83d461c9c8d83d229e52b72b5b73bb6a8f', 0.7383),\n",
       " ('a4ca87ab8ab083b60b6f187f2fc06a9cde144812', 0.7376),\n",
       " ('6bebe56c452f074422ad7b6d1dbbb7f29a1c4f8d', 0.7375),\n",
       " ('dddd1de22beca06119ca83122afd82059abbf48a', 0.7373),\n",
       " ('a7195d067e30d1d6a685af8ca2d25f67b5a9d4da', 0.737),\n",
       " ('61a3d27beacc14303d3500ac647122fd32329b35', 0.7368),\n",
       " ('888728745dbb769e29ed475d4f7661eebe1a71cf', 0.7364),\n",
       " ('d0cd8b45949b959c316a3ed75a4683d0a70b1aa9', 0.7362),\n",
       " ('fa5f9aa1cb6f97654ca8e6d279ceee1427a87e68', 0.7358),\n",
       " ('b5cec95a5ec34bb0a9fd888f7fad1075d5dfca3a', 0.7354),\n",
       " ('2721c748cea1d5802a6834bf7e7121d13ae36183', 0.7354),\n",
       " ('03055978e278960de9fbb5c648b1779ef9f26cd1', 0.7343),\n",
       " ('97d76e7d5d1199bf37c25bca339c9df278a41bcf', 0.734),\n",
       " ('9ab45aa875b56335303398e84a59a3756cd9d530', 0.734),\n",
       " ('3dd5ad34012164c4ec9c571a12cc6a7561683dea', 0.7337),\n",
       " ('25d2b1db905695d00e8570d51ef152bd6efca4a7', 0.7334),\n",
       " ('a7b77af6582d3ac66a6cb3d0c45e767be8f825d1', 0.732),\n",
       " ('4a028a3fcab59291a9e0b62d06e190cd16777e3c', 0.7315),\n",
       " ('4f4a16f6839a690dc648b060cf447472dd5fbe75', 0.7312),\n",
       " ('d78e3ceaabaad1821709e4663886df354bb44665', 0.7306),\n",
       " ('f9a7175198a2c9f3ab0134a12a7e9e5369428e42', 0.7294),\n",
       " ('0f6439b95aed189908c628f1103dbcef8541cc2e', 0.7291),\n",
       " ('d8fb7721a61f588bc345d10a6740843b92f74eb0', 0.7285),\n",
       " ('28a3582ecab72e2a91ec9004075d744b8bac4640', 0.7283),\n",
       " ('5b23697d36eb1d8ed1ea744cb4e7166bef25223f', 0.7281),\n",
       " ('4f88d2417fcd299dd75e1bda0c50135e04e592d7', 0.7278),\n",
       " ('500a91c618e7b68799d241529d84954b18625641', 0.7276),\n",
       " ('1a8ac21ec340d46d4fdbc2453d9f32208fef131e', 0.7274),\n",
       " ('6a5180c0c4c3ff509da40e2dfa1029da472092d2', 0.7264),\n",
       " ('319446162e5669e6426f7c0b75882571b9ba825e', 0.7256),\n",
       " ('08d2e3f0d76648a3983e1db4e3b8d6d40dad49cd', 0.7247),\n",
       " ('110f5dc6d5bfe67138d64c261d6851c727021d1f', 0.7229),\n",
       " ('89d008f2262f3fd6810f13483f900bf45c368a01', 0.7229),\n",
       " ('6a4501fefaf73261dc180ff86b52208679f3fb9c', 0.7228),\n",
       " ('aaf5a3530b56eb078a1378a1cfd9a24899e95d56', 0.7227),\n",
       " ('452880bde921d984425ed0abc7a7766e66175f7b', 0.7215),\n",
       " ('90187f5196479c76da68463bcc030318a828ac1b', 0.7213),\n",
       " ('cc66970b020e40867ad259d9b450433366e77048', 0.7208),\n",
       " ('b1ddf88c141e1b42d97848383a9150707814dbd9', 0.7206),\n",
       " ('6025e6c03c0fa2f5b392c3064735fa6db6d90455', 0.7205),\n",
       " ('4b839a63333c850c17272a94fee1c96c67c6b7dc', 0.7202),\n",
       " ('2731d0d1076761b2f638a7e7fd20e90f711fa91b', 0.7197),\n",
       " ('c8b18682965ff9dccc0130dab3d679f78cefa617', 0.719),\n",
       " ('e51dff31f56847c16de3a2f2682d16109537b96e', 0.7185),\n",
       " ('9102567bb0167f469740024e82f05b3c9e8e2627', 0.7184),\n",
       " ('63aa9598da11da04d044ecf7211b2055b7a1775c', 0.718),\n",
       " ('dbf829c977c121c3704d070d7800d29fe5914756', 0.7175),\n",
       " ('eae436d899684f2885af7bc68926d490190b6dde', 0.7174),\n",
       " ('758d937f90bd6b0794f8a6dc2940621bb3b612ae', 0.7171),\n",
       " ('4d75b347f410d0a5ba32adbf8803ba62acc7519d', 0.7168),\n",
       " ('2046b2da23eb2f79744eb391d902da9cedf87947', 0.7168),\n",
       " ('2f08cfe76ae4b72ecd7c4ba09d4f4af602f94252', 0.7164),\n",
       " ('905c78ac3e691c51cf106f2dc127961980a3df0b', 0.7162),\n",
       " ('38a62d9486f4c073ce586de2dc6d38083cd569f7', 0.716),\n",
       " ('08dd6f6adf5657516d53db1946780a5d31a10676', 0.7154),\n",
       " ('4c913d59d150fe7581386b87dfd9f90448a9adee', 0.715),\n",
       " ('d671aa30498544df784cf14ca64e31b0e0f97948', 0.7149),\n",
       " ('6a16decd8298638d40e3266b57e2efc3b969e9d1', 0.7149),\n",
       " ('a605ae336c02b3b0eaf5fe6db2e18ed27b588d77', 0.7148),\n",
       " ('7c689e168a1b07f8c2cc320f88e0fc89ef1ac73d', 0.7145),\n",
       " ('86802db6a19fe5018dc5930e234c1a8b9cbb0454', 0.7142),\n",
       " ('a8fd161e6b0561bca332ff270d9ede3bb66b7bed', 0.7142),\n",
       " ('3a7d07c638a10781c6c0e3170115ce82dfb111c5', 0.714),\n",
       " ('71f6f80f877e7f63f62096c7dd1f6e588c388bb9', 0.7139),\n",
       " ('8a2ddca9936ca5958506073037418cacded2b417', 0.7138),\n",
       " ('9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632', 0.7138),\n",
       " ('46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5', 0.7133),\n",
       " ('14715d934488c6f2002107f0d2bfda27acf276a1', 0.7131),\n",
       " ('d0c069748a65012a6aae44f0c3f17dbab2e646f4', 0.7129),\n",
       " ('566b9c782e4f8bc9f03e2381c5df700069b749a7', 0.7129),\n",
       " ('238fa1d3db6244ef7c086300a3ee455189ca2bbe', 0.7129),\n",
       " ('2dbec8020b9111f1c70e90b112e6318f7d02426c', 0.7128),\n",
       " ('e5641c189e1c4bae73308fadd88839ffa73f382b', 0.7127),\n",
       " ('ee50bc0271a66d9561003e9b53f95b77671e3564', 0.7127),\n",
       " ('9696189e213514686deab94a11b3e21b89404bbf', 0.7122),\n",
       " ('6cb53afdc2b21236c3957efd4bbeb2aa65338c50', 0.712),\n",
       " ('2393506721d2f71569356b3e5deef7655456007c', 0.7119),\n",
       " ('caae2e287e1d4e49a8179d721c6b0e7200cf537c', 0.7118),\n",
       " ('fee443e70a03d79270e3527df4e83a5eb842d2b0', 0.7118),\n",
       " ('273c145ea080f277839b89628c255017fc0e1e7c', 0.7107),\n",
       " ('a9d7a85fbd1028be86e93410f095a51148656a56', 0.7106),\n",
       " ('c5d73f59b5c55af85020c5a38ebaec9f50f66dce', 0.7106),\n",
       " ('de8c767dbfd293d0f284a3eebd8e36fbf1e563c5', 0.7105),\n",
       " ('1be5d5de1f311f7c3d930358d46b2b65f9e3ebc3', 0.7105),\n",
       " ('e9c80c5969904d322b65f30eb7f76f000f66c165', 0.7098),\n",
       " ('79f2dcea1f249c62230f5483d0b08ebb775d6ca5', 0.7094),\n",
       " ('e7bf315327da6619462130c19771e5db896a2d86', 0.7093),\n",
       " ('07dc375b95aaeb748d7b0560bfa7d81f1bddc8b2', 0.7093),\n",
       " ('03fab98a9be74a253688840dba9144737a8ca92d', 0.7092),\n",
       " ('610684735aa3a6bad1cc28c777944dbfe959fea5', 0.7092),\n",
       " ('82ba2e441933a78740445b636f52045d60780e75', 0.7086),\n",
       " ('f36e8f9d4243f344ea41acde7b62618c2b5cac85', 0.7084),\n",
       " ('0c72450890a54b68d63baa99376131fda8f06cf9', 0.7084),\n",
       " ('4478a98c554a5576c2af57ade669929761dc517d', 0.708),\n",
       " ('70813142dcee9fd455b0f587153e21deaf7b8005', 0.7078),\n",
       " ('33998aff64ce51df8dee45989cdca4b6b1329ec4', 0.7077),\n",
       " ('e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365', 0.7076),\n",
       " ('d6f12d83ccf1d8201922fd63e37679188e9043c2', 0.7072),\n",
       " ('5245a453f33a6a7eb801b4d36692c13fe8f49302', 0.7068),\n",
       " ('96ea9806a0db5d3aef2fbaa00a29f14b71831306', 0.7068),\n",
       " ('1702e4a4431dda73f06141b87491ad7da3c65bf4', 0.7068),\n",
       " ('7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe', 0.7061),\n",
       " ('8f60401b03b46c4b3e61834dc691a8587250d46b', 0.706),\n",
       " ('64c3f98b3f0163582e327ba275004208da17220e', 0.7058),\n",
       " ('c9c119f511aef4f15d85f83c355a57345778f442', 0.7058),\n",
       " ('f1bd573a645c3dad946ce517ed06f629ab627f8f', 0.7055),\n",
       " ('0907f2fe9d8305976a539877ef2b48b553e4fb63', 0.7047),\n",
       " ('9f3ae8055e227edb413c54417c9c216f1f554f52', 0.7046),\n",
       " ('a6aed0c4e0f39a55edb407f492e41f178a62907f', 0.7046),\n",
       " ('b40df4b273f255b3cb5639e220c8ab7b1bdb313e', 0.7044),\n",
       " ('d40631a850c21607a5b1cb63efc7bf4ba1ab1fe0', 0.7043),\n",
       " ('5bee1c50741e027e106d770a2b95e46c54d6ab4c', 0.7042),\n",
       " ('88dc871460a3a03699dc0a8ca248542a5d39f41e', 0.704),\n",
       " ('65fc294fb8626b77eccf52d5af21034aba7b3a08', 0.7034),\n",
       " ('35cad543604f20f1506573513f33e6a3c2ed2747', 0.7033),\n",
       " ('88884b8806262a4095036041e3567d450dba39f7', 0.7031),\n",
       " ('a1f76db91c0debcf93ae9889736bce8470902113', 0.7031),\n",
       " ('4c8e74bb70a15b3a2cf1064e2c57ce6730a3d344', 0.7026),\n",
       " ('00b5a297a25820138a1fde50f1e746a657fb70eb', 0.7026),\n",
       " ('a34b5883129fdb52b3f3473de292368bfed849d8', 0.7018),\n",
       " ('a4b05bcbc9676fcf8287bab120a44488c67ec588', 0.7017),\n",
       " ('db6ca4fe83e22a1a3a95761e62fe072460ee6d1c', 0.7014),\n",
       " ('e5379c6b2328c1d4832a2efaf276e49324fbb05a', 0.7004),\n",
       " ('c7cfcd12d90cdb8f407d55a6ec0296f321dd9617', 0.7002),\n",
       " ('c1799bf28d1ae93e1631be5b59196ee1e568f538', 0.7001)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_local_sim_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d176a1e",
   "metadata": {},
   "source": [
    "Try build crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6354f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f3b2a9",
   "metadata": {},
   "source": [
    "Significance Identification\n",
    "- absolute significance\n",
    "- relative significance\n",
    "- graph significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fe0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec78d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate paper graph from nodes / edges json\n",
    "G_pre = PaperGraph(name='Paper Graph 1')\n",
    "G_pre.add_graph_nodes(ps.nodes_json)\n",
    "G_pre.add_graph_edges(ps.edges_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(list({'a':1, 'b':2}.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_pre.nodes['2335569348']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ace20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.nodes_json[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630dbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16633a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def calculate_months_since(date_str):\n",
    "    \"\"\"\n",
    "    计算给定 'yyyy-mm-dd' 格式的日期距今的月份数。\n",
    "\n",
    "    Args:\n",
    "        date_str (str): 'yyyy-mm-dd' 格式的日期字符串。\n",
    "\n",
    "    Returns:\n",
    "        int: 给定日期距今的月份数。如果输入格式错误，返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        given_date = datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "        today = datetime.now().date()\n",
    "        difference = relativedelta(today, given_date)\n",
    "        return difference.years * 12 + difference.months\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# 示例用法\n",
    "date_to_calculate = '2024-01-15'\n",
    "months = calculate_months_since(date_to_calculate)\n",
    "\n",
    "if months is not None:\n",
    "    print(f\"日期 {date_to_calculate} 距今有 {months} 个月。\")\n",
    "else:\n",
    "    print(\"输入的日期格式不正确，请使用 'yyyy-mm-dd' 格式。\")\n",
    "\n",
    "date_to_calculate_invalid = '2024/01/15'\n",
    "months_invalid = calculate_months_since(date_to_calculate_invalid)\n",
    "\n",
    "if months_invalid is not None:\n",
    "    print(f\"日期 {date_to_calculate_invalid} 距今有 {months_invalid} 个月。\")\n",
    "else:\n",
    "    print(f\"输入的日期格式 '{date_to_calculate_invalid}' 不正确，请使用 'yyyy-mm-dd' 格式。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396eb711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.edges_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Graph Stat ---\n",
    "from graph.graph_stats import get_graph_stats, get_author_stats, get_paper_stats\n",
    "g_stat = get_graph_stats(G_pre)   # graph stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92016abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_stats = get_paper_stats(G_pre, core_paper_ids)  # paper stats on graph\n",
    "author_stats = get_author_stats(G_pre, core_author_ids)  # author stats on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check crossref\n",
    "crossref_stats = []\n",
    "for x in paper_stats:\n",
    "    if (x['if_seed'] == False  # exclude seed papers \n",
    "        and x['local_citation_cnt'] > min(len(core_paper_ids),  5)):  # select most refered papers in graph\n",
    "        crossref_stats.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity\n",
    "from collect.paper_similarity_calculation import PaperSim\n",
    "\n",
    "sim = PaperSim(\n",
    "    embed_api_key = embed_api_key,\n",
    "    embed_model_name = embed_model_name\n",
    ")\n",
    "\n",
    "# --- SIMILARITY CALCULATION ---\n",
    "# check if similarity with edge type\n",
    "edge_types = [x[0] for x in g_stat['edge_type']]\n",
    "\n",
    "# valid paper with abstracts\n",
    "complete_paper_json = [node for node in ps.nodes_json \n",
    "                        if node['labels'] == ['Paper'] \n",
    "                        and node['properties'].get('title') is not None and node['properties'].get('abstract') is not None]\n",
    "complete_paper_dois = [node['id'] for node in complete_paper_json]\n",
    "\n",
    "if 'SIMILAR_TO' not in edge_types:\n",
    "    # calculate paper nodes similarity\n",
    "    semantic_similar_pool = await sim.cal_embed_and_similarity(\n",
    "        paper_nodes_json = complete_paper_json,\n",
    "        paper_dois_1 = complete_paper_dois, \n",
    "        paper_dois_2 = complete_paper_dois,\n",
    "        similarity_threshold = 0.7,\n",
    "        )\n",
    "\n",
    "    # add similarity edges to graph\n",
    "    G_pre.add_graph_edges(semantic_similar_pool)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PRUNNING ---\n",
    "# pruning by connectivity\n",
    "sub_graphs = G_pre.find_wcc_subgraphs(target_nodes=core_paper_ids)\n",
    "if sub_graphs is not None and len(sub_graphs) > 0:\n",
    "    G_post  = sub_graphs[0]\n",
    "    # get stats after prunning\n",
    "    g_stat = get_graph_stats(G_post)\n",
    "else:\n",
    "    G_post = G_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_stats = get_paper_stats(G_post, core_paper_ids)  # paper stats on graph\n",
    "author_stats = get_author_stats(G_post, core_author_ids)  # author stats on graph\n",
    "\n",
    "# check crossref\n",
    "crossref_stats = []\n",
    "for x in paper_stats:\n",
    "    if (x['if_seed'] == False  # exclude seed papers \n",
    "        and x['local_citation_cnt'] > min(len(core_paper_ids),  5)):  # select most refered papers in graph\n",
    "        crossref_stats.append(x)\n",
    "\n",
    "# check key authors\n",
    "key_authors_stats = []\n",
    "for x in author_stats:\n",
    "    if (x['if_seed'] == False  # exclude seed authors \n",
    "        and x['local_paper_cnt'] > min(len(core_paper_ids), 5)):  # select most refered papers in graph\n",
    "        key_authors_stats.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f649431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check paper similarity\n",
    "sorted_paper_similarity = sorted(paper_stats, key=lambda x:x['max_sim_to_seed'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ids = []\n",
    "# if cross ref insufficient, further expand similar papers on citation chain\n",
    "if len(crossref_stats) < 20:\n",
    "    # filter top similar papers (to help build crossref)\n",
    "    for item in sorted_paper_similarity:\n",
    "        if item['if_seed'] == False and item['doi'] not in ps.explored_nodes['reference']:\n",
    "            if item['max_sim_to_seed'] > 0.7 and item['global_citaion_cnt'] > 10:\n",
    "                ref_ids.append(item['doi'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ids = ref_ids[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if key authors not have complete information\n",
    "author_ids = []\n",
    "if len(key_authors_stats) > 20:\n",
    "    sorted_key_authors = sorted(key_authors_stats, key=lambda x:x['local_paper_cnt'], reverse=True)\n",
    "    # filter key authors (to amplify information)\n",
    "    for item in sorted_key_authors:\n",
    "        if item['if_seed'] == False and item['author_id'] not in ps.explored_nodes['author']:\n",
    "            author_ids.append(item['author_id'])\n",
    "\n",
    "author_ids = author_ids[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e45d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ps.nodes_json:\n",
    "    if item['labels'] == ['Paper']:\n",
    "        print(item['properties']['title'])\n",
    "        # print(item.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3163461",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_sim_paper_ids = []\n",
    "for u, v, edge_data in G_post.edges(data=True):\n",
    "    if edge_data.get('relationshipType') == 'SIMILAR_TO' and edge_data.get('weight') > 0.7:\n",
    "        if u in core_paper_ids and v not in core_paper_ids:\n",
    "            hop_1_sim_paper_ids.append(v)\n",
    "        elif u not in core_paper_ids and v in core_paper_ids:\n",
    "            hop_1_sim_paper_ids.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hop_1_sim_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92714e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_citation_paper_ids = []\n",
    "for u, v, edge_data in G_post.edges(data=True):\n",
    "    if edge_data.get('relationshipType') == 'CITES':\n",
    "        if u in core_paper_ids and v not in core_paper_ids:\n",
    "            hop_1_citation_paper_ids.append(v)\n",
    "        elif u not in core_paper_ids and v in core_paper_ids:\n",
    "            hop_1_citation_paper_ids.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_topic_paper_ids = []\n",
    "for u, v, edge_data in G_post.edges(data=True):\n",
    "    if edge_data.get('relationshipType') == 'DISCUSS':\n",
    "        topic = G_post.nodes[v].get('name')\n",
    "        if u not in core_paper_ids:\n",
    "            title = G_post.nodes[u].get('title')\n",
    "            gloabl_citation = G_post.nodes[u].get('citationCount')\n",
    "            if gloabl_citation > 10 and u in hop_1_sim_paper_ids:\n",
    "                print(topic, title, gloabl_citation)\n",
    "                # hop_1_topic_paper_ids.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation, author papers\n",
    "hop_1_expand_papers = set(list(core_paper_ids) + hop_1_sim_paper_ids + hop_1_citation_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c24836",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_2_sim_paper_ids = []\n",
    "for u, v, edge_data in G_post.edges(data=True):\n",
    "    if edge_data.get('relationshipType') == 'SIMILAR_TO' and edge_data.get('weight') > 0.7:\n",
    "        if u in hop_1_expand_papers and v not in hop_1_expand_papers:\n",
    "            hop_2_sim_paper_ids.append(v)\n",
    "        elif u not in hop_1_expand_papers and v in hop_1_expand_papers:\n",
    "            hop_2_sim_paper_ids.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hop_1_expand_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290326f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(hop_2_sim_paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in {'a':1, 'b':2}.items():\n",
    "    if key == 'a': continue\n",
    "    print(key, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "None > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are 6 node types in this graph, they are:\n",
    "[('Author', 5560), ('Paper', 1933), ('Journal', 513), ('Venue', 380), ('Institution', 5), ('Topic', 4)]\n",
    "There are 7 edge types in this graph, they are:\n",
    "[('SIMILAR_TO', 20872), ('WRITES', 9366), ('RELEASES_IN', 1244), ('PRINTS_ON', 702), ('DISCUSS', 419), ('CITES', 288), ('WORKS_IN', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'a':2, 'b':1, 'c':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sorted = sorted(x.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_keys = {}\n",
    "for index, (key, value) in enumerate(x_sorted):\n",
    "    ranked_keys[key] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6da680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from apis.s2_api import SemanticScholarKit\n",
    "\n",
    "s2 = SemanticScholarKit()\n",
    "\n",
    "\n",
    "topics = ['llm security']\n",
    "\n",
    "# --- 2. Create tasks ---\n",
    "tasks = []\n",
    "\n",
    "for topic in topics:\n",
    "    tasks.append(s2.search_paper(\n",
    "        query=topic,\n",
    "        limit=100,\n",
    "    ))\n",
    "\n",
    "# --- 3. Execute and collect ---\n",
    "if tasks:\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb669509",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3159721",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results):\n",
    "    current_topic = topics_to_search[idx]\n",
    "    if isinstance(result, Exception):\n",
    "        logging.error(f\"topic_search: Task for topic '{current_topic}' failed: {result}\", exc_info=False)\n",
    "        self.not_found_nodes['topic'].add(current_topic)\n",
    "\n",
    "    elif isinstance(result, PaginatedResults) and hasattr(result, '_items') and result._items:\n",
    "        papers_list = result._items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_papers_count = 0\n",
    "new_topic_links = 0\n",
    "existing_paper_ids = {p.get('paperId') for p in self.data_pool['paper'] if p.get('paperId')}\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    current_topic = topics_to_search[idx]\n",
    "    if isinstance(result, Exception):\n",
    "        logging.error(f\"topic_search: Task for topic '{current_topic}' failed: {result}\", exc_info=False)\n",
    "        self.not_found_nodes['topic'].add(current_topic)\n",
    "\n",
    "    elif isinstance(result, PaginatedResults) and hasattr(result, '_items') and result._items:\n",
    "        papers_list = result._items\n",
    "        # Add paper metadata to data pool (deduplicated)\n",
    "        papers_dict = [item.raw_data for item in papers_list if isinstance(item, Paper) and hasattr(item, 'raw_data')]\n",
    "        new_papers = [p for p in papers_dict if p.get('paperId') and p['paperId'] not in existing_paper_ids]\n",
    "        self.data_pool['paper'].extend(new_papers)\n",
    "        new_papers_count += len(new_papers)\n",
    "        existing_paper_ids.update(p['paperId'] for p in new_papers) # Update seen IDs\n",
    "\n",
    "        # Add topic metadata links\n",
    "        paper_ids_in_result = [paper.get('paperId') for paper in papers_dict if paper.get('paperId')]\n",
    "        for pid in paper_ids_in_result:\n",
    "            # Use 'paperId' key for consistency\n",
    "            self.data_pool['topic'].append({'topic': current_topic, 'paperId': pid})\n",
    "            new_topic_links += 1\n",
    "    else:\n",
    "        logging.warning(f\"topic_search: Task for topic '{current_topic}' returned no results or failed silently.\")\n",
    "        self.not_found_nodes['topic'].add(current_topic) # Mark as not found if no results\n",
    "\n",
    "logging.info(f\"topic_search: Added {new_papers_count} new papers and {new_topic_links} topic links to data pool.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0700ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "\n",
    "def graph_basic_stats(paper_graph):\n",
    "    \"\"\"\n",
    "    统计 paper_graph 中 paper 和 author 的基本指标。\n",
    "\n",
    "    Args:\n",
    "        paper_graph: 一个 NetworkX 图对象，包含 'Paper' 和 'Author' 类型的节点。\n",
    "\n",
    "    Returns:\n",
    "        一个字典，包含 paper_stats 和 author_stats 两个子字典，\n",
    "        分别存储 paper 和 author 的基本指标。\n",
    "    \"\"\"\n",
    "    paper_stats = {\n",
    "        'citationCount': [],\n",
    "        'influentialCitationCount': [],\n",
    "        'referenceCount': [],\n",
    "        'monthlyCitationCount': [],\n",
    "        'localCitationCount': [],\n",
    "        'localReferenceCount': []\n",
    "    }\n",
    "    author_stats = {\n",
    "        'paperCount': [],\n",
    "        'citationCount': [],\n",
    "        'hIndex': [],\n",
    "        'localPaperCount': []\n",
    "    }\n",
    "\n",
    "    for nid, node_data in paper_graph.nodes(data=True):\n",
    "        if node_data.get('nodeType') == 'Paper':\n",
    "            pub_dt = node_data.get('publicationDate')\n",
    "            paper_cit_cnt = node_data.get('citationCount')\n",
    "            paper_sig_cit_cnt = node_data.get('influentialCitationCount')\n",
    "            paper_ref_cnt = node_data.get('referenceCount')\n",
    "\n",
    "            if pub_dt:\n",
    "                try:\n",
    "                    tm_to_dt = relativedelta(datetime.now().date(), datetime.strptime(pub_dt, '%Y-%m-%d').date())\n",
    "                    mth_to_dt = tm_to_dt.years * 12 + tm_to_dt.months\n",
    "                    if paper_cit_cnt is not None and mth_to_dt > 0:\n",
    "                        paper_mthly_cit_cnt = paper_cit_cnt / mth_to_dt\n",
    "                        paper_stats['monthlyCitationCount'].append(paper_mthly_cit_cnt)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid date format for paper {nid}: {pub_dt}\")\n",
    "\n",
    "            if paper_cit_cnt is not None:\n",
    "                paper_stats['citationCount'].append(paper_cit_cnt)\n",
    "            if paper_sig_cit_cnt is not None:\n",
    "                paper_stats['influentialCitationCount'].append(paper_sig_cit_cnt)\n",
    "            if paper_ref_cnt is not None:\n",
    "                paper_stats['referenceCount'].append(paper_ref_cnt)\n",
    "\n",
    "            predecessors = paper_graph.predecessors(nid)\n",
    "            paper_loc_cit_cnt = sum([1 for x in predecessors if paper_graph.nodes[x].get('nodeType') == 'Paper'])\n",
    "            paper_stats['localCitationCount'].append(paper_loc_cit_cnt)\n",
    "\n",
    "            successors = paper_graph.successors(nid)\n",
    "            paper_loc_ref_cnt = sum([1 for x in successors if paper_graph.nodes[x].get('nodeType') == 'Paper'])\n",
    "            paper_stats['localReferenceCount'].append(paper_loc_ref_cnt)\n",
    "\n",
    "        elif node_data.get('nodeType') == 'Author':\n",
    "            author_paper_cnt = node_data.get('paperCount')\n",
    "            author_citation_cnt = node_data.get('citationCount')\n",
    "            author_h_index = node_data.get('hIndex')\n",
    "\n",
    "            if author_paper_cnt is not None:\n",
    "                author_stats['paperCount'].append(author_paper_cnt)\n",
    "            if author_citation_cnt is not None:\n",
    "                author_stats['citationCount'].append(author_citation_cnt)\n",
    "            if author_h_index is not None:\n",
    "                author_stats['hIndex'].append(author_h_index)\n",
    "\n",
    "            successors = paper_graph.successors(nid)\n",
    "            author_loc_paper_cnt = sum([1 for x in successors if paper_graph.nodes[x].get('nodeType') == 'Paper'])\n",
    "            author_stats['localPaperCount'].append(author_loc_paper_cnt)\n",
    "\n",
    "    return {'paper_stats': paper_stats, 'author_stats': author_stats}\n",
    "\n",
    "def analyze_graph_stats(basic_stats):\n",
    "    \"\"\"\n",
    "    对 paper 和 author 的基本统计指标进行分析，计算 min, max, avg, 分位数等。\n",
    "\n",
    "    Args:\n",
    "        basic_stats: 一个字典，包含 paper_stats 和 author_stats 子字典，\n",
    "                     由 graph_basic_stats 函数返回。\n",
    "\n",
    "    Returns:\n",
    "        一个字典，包含 paper_analysis 和 author_analysis 两个子字典，\n",
    "        分别存储 paper 和 author 各指标的统计分析结果。\n",
    "    \"\"\"\n",
    "    paper_analysis = {}\n",
    "    author_analysis = {}\n",
    "\n",
    "    paper_data = basic_stats.get('paper_stats', {})\n",
    "    author_data = basic_stats.get('author_stats', {})\n",
    "\n",
    "    for key, values in paper_data.items():\n",
    "        valid_values = [v for v in values if isinstance(v, (int, float))] # 只处理数值类型\n",
    "        if valid_values:\n",
    "            paper_analysis[key] = {\n",
    "                'min': np.min(valid_values),\n",
    "                'max': np.max(valid_values),\n",
    "                'average': np.mean(valid_values),\n",
    "                'median': np.median(valid_values),\n",
    "                'quantile_25': np.percentile(valid_values, 25),\n",
    "                'quantile_75': np.percentile(valid_values, 75)\n",
    "            }\n",
    "        else:\n",
    "            paper_analysis[key] = {}\n",
    "\n",
    "    for key, values in author_data.items():\n",
    "        valid_values = [v for v in values if isinstance(v, (int, float))] # 只处理数值类型\n",
    "        if valid_values:\n",
    "            author_analysis[key] = {\n",
    "                'min': np.min(valid_values),\n",
    "                'max': np.max(valid_values),\n",
    "                'average': np.mean(valid_values),\n",
    "                'median': np.median(valid_values),\n",
    "                'quantile_25': np.percentile(valid_values, 25),\n",
    "                'quantile_75': np.percentile(valid_values, 75)\n",
    "            }\n",
    "        else:\n",
    "            author_analysis[key] = {}\n",
    "\n",
    "    return {'paper_analysis': paper_analysis, 'author_analysis': author_analysis}\n",
    "\n",
    "# 示例用法 (假设你已经有了一个 paper_graph 对象)\n",
    "if __name__ == '__main__':\n",
    "    import networkx as nx\n",
    "    import random\n",
    "\n",
    "    # 创建一个简单的示例 paper_graph\n",
    "    paper_graph = nx.DiGraph()\n",
    "    paper_graph.add_node(1, nodeType='Paper', publicationDate='2023-01-15', citationCount=10, influentialCitationCount=5, referenceCount=20)\n",
    "    paper_graph.add_node(2, nodeType='Paper', publicationDate='2023-03-20', citationCount=15, influentialCitationCount=8, referenceCount=25)\n",
    "    paper_graph.add_node(3, nodeType='Author', paperCount=2, citationCount=25, hIndex=3)\n",
    "    paper_graph.add_node(4, nodeType='Author', paperCount=1, citationCount=5, hIndex=1)\n",
    "    paper_graph.add_edge(1, 2)\n",
    "    paper_graph.add_edge(3, 1)\n",
    "    paper_graph.add_edge(3, 2)\n",
    "    paper_graph.add_edge(1, 3)\n",
    "\n",
    "    basic_stats = graph_basic_stats(paper_graph)\n",
    "    print(\"Basic Stats:\")\n",
    "    print(basic_stats)\n",
    "\n",
    "    analysis_results = analyze_graph_stats(basic_stats)\n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaaadc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
