{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import asyncio \n",
    "import PIL.Image \n",
    "from typing import List\n",
    "\n",
    "# Import Google AI specific libraries\n",
    "from google import genai\n",
    "from google.genai import types \n",
    "\n",
    "\n",
    "# --- Original Synchronous Functions (preserved) ---\n",
    "def llm_gen(api_key, model_name, qa_prompt, sys_prompt=None, temperature=0.3):\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=sys_prompt,\n",
    "        temperature=temperature)\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name, \n",
    "            contents=qa_prompt,\n",
    "            config=config)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during synchronous llm_gen: {e}\")\n",
    "        raise e \n",
    "\n",
    "\n",
    "def llm_image_gen(api_key, model_name, qa_prompt, pil_images: List[PIL.Image.Image], sys_prompt=None, temperature=0.3):\n",
    "    \"\"\"q&a with images (synchronous)\n",
    "    Args:\n",
    "        pil_images: List of PIL.Image objects.\n",
    "    \"\"\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=sys_prompt,\n",
    "        temperature=temperature)\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,  #　\"gemini-2.0-flash-exp\",\n",
    "            contents=[qa_prompt]+pil_images,\n",
    "            config=config)\n",
    "        # Basic safety check\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during synchronous llm_image_gen: {e}\")\n",
    "        raise e # Re-raise\n",
    "\n",
    "def llm_gen_w_retry(api_key, model_name, qa_prompt, sys_prompt=None, temperature=0.3, max_retries=3, initial_delay=1):\n",
    "    \"\"\" Synchronous llm_gen with retry logic \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            # Call the base synchronous function\n",
    "            return llm_gen(api_key, model_name, qa_prompt, sys_prompt, temperature)\n",
    "        # Handle potential direct ValueErrors from safety checks etc. in llm_gen\n",
    "        except ValueError as e:\n",
    "            print(f\"Sync: Non-retryable error during generation: {e}\")\n",
    "            return None # Return None for non-retryable errors\n",
    "        except Exception as e: # Catch other potential exceptions from the API call\n",
    "            # Attempt to check status code if available, otherwise treat as non-retryable\n",
    "            status_code = getattr(e, 'code', None) or getattr(e, 'status_code', None) # Check common attributes\n",
    "            if status_code == 429: # Check if it looks like a rate limit error anyway\n",
    "                 if retries < max_retries:\n",
    "                     retries += 1\n",
    "                     print(f\"Sync: Rate limit inferred ({status_code}). Retrying in {delay} seconds (Retry {retries}/{max_retries})...\")\n",
    "                     time.sleep(delay)\n",
    "                     delay *= 2\n",
    "                 else:\n",
    "                     print(f\"Sync: Max retries reached after inferred rate limit error. Returning None.\")\n",
    "                     return None\n",
    "            else:\n",
    "                 print(f\"Sync: An unexpected error occurred: {e} (Code: {status_code})\")\n",
    "                 return None \n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def llm_image_gen_w_retry(api_key, model_name, qa_prompt, pil_images: List[PIL.Image.Image], sys_prompt=None, temperature=0.3, max_retries=3, initial_delay=1):\n",
    "    \"\"\" Synchronous llm_image_gen with retry logic \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            # Call the base synchronous function\n",
    "            return llm_image_gen(api_key, model_name, qa_prompt, pil_images, sys_prompt, temperature)\n",
    "        # Handle potential direct ValueErrors from safety checks etc. in llm_image_gen\n",
    "        except ValueError as e:\n",
    "            print(f\"Sync Img: Non-retryable error during generation: {e}\")\n",
    "            return None # Return None for non-retryable errors\n",
    "        except Exception as e: # Catch other potential exceptions\n",
    "            status_code = getattr(e, 'code', None) or getattr(e, 'status_code', None)\n",
    "            if status_code == 429:\n",
    "                 if retries < max_retries:\n",
    "                     retries += 1\n",
    "                     print(f\"Sync Img: Rate limit inferred ({status_code}). Retrying in {delay} seconds (Retry {retries}/{max_retries})...\")\n",
    "                     time.sleep(delay)\n",
    "                     delay *= 2\n",
    "                 else:\n",
    "                     print(f\"Sync Img: Max retries reached after inferred rate limit error. Returning None.\")\n",
    "                     return None\n",
    "            else:\n",
    "                print(f\"Sync Img: An unexpected error occurred: {e} (Code: {status_code})\")\n",
    "                return None # Return None for other errors\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- New Asynchronous Functions ---\n",
    "\n",
    "async def async_llm_gen(api_key, model_name, qa_prompt, sys_prompt=None, temperature=0.3):\n",
    "    \"\"\"Asynchronous version of llm_gen.\"\"\"\n",
    "    # Configuration should ideally happen once globally or be managed externally\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    # config = types.GenerateContentConfig(\n",
    "    #     system_instruction=sys_prompt,\n",
    "    #     temperature=temperature)\n",
    "    try:\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model=model_name, \n",
    "            contents=qa_prompt,\n",
    "            #config=config\n",
    "            )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during async async_llm_gen: {e}\")\n",
    "        # Re-raise for retry logic to catch\n",
    "        raise e\n",
    "\n",
    "async def async_llm_image_gen(api_key, model_name, qa_prompt, pil_images: List[PIL.Image.Image], sys_prompt=None, temperature=0.3):\n",
    "    \"\"\"Asynchronous version of llm_image_gen.\"\"\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    # config = types.GenerateContentConfig(\n",
    "    #     system_instruction=sys_prompt,\n",
    "    #     temperature=temperature)\n",
    "    try:\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model=model_name,  #　\"gemini-2.0-flash-exp\",\n",
    "            contents=[qa_prompt]+pil_images,\n",
    "            #config=config\n",
    "            )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during async async_llm_image_gen: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "async def async_llm_gen_w_retry(api_key, model_name, qa_prompt, sys_prompt=None, temperature=0.3, max_retries=3, initial_delay=1):\n",
    "    \"\"\" Asynchronous llm_gen with retry logic \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            # Call the base asynchronous function\n",
    "            return await async_llm_gen(api_key, model_name, qa_prompt, sys_prompt, temperature)\n",
    "        except ValueError as e:\n",
    "            print(f\"Async: Non-retryable error during generation: {e}\")\n",
    "            return None # Return None for non-retryable errors\n",
    "        except Exception as e: # Catch other potential exceptions\n",
    "            status_code = getattr(e, 'code', None) or getattr(e, 'status_code', None)\n",
    "            if status_code == 429: # Check if it looks like a rate limit error anyway\n",
    "                 if retries < max_retries:\n",
    "                     retries += 1\n",
    "                     print(f\"Async: Rate limit inferred ({status_code}). Retrying in {delay} seconds (Retry {retries}/{max_retries})...\")\n",
    "                     await asyncio.sleep(delay) # Use asyncio.sleep\n",
    "                     delay *= 2\n",
    "                 else:\n",
    "                     print(f\"Async: Max retries reached after inferred rate limit error. Returning None.\")\n",
    "                     return None\n",
    "            else:\n",
    "                print(f\"Async: An unexpected error occurred: {e} (Code: {status_code})\")\n",
    "                return None # Return None for other errors\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "async def async_llm_image_gen_w_retry(api_key, model_name, qa_prompt, pil_images: List[PIL.Image.Image], sys_prompt=None, temperature=0.3, max_retries=3, initial_delay=1):\n",
    "    \"\"\" Asynchronous llm_image_gen with retry logic \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            # Call the base asynchronous function\n",
    "            return await async_llm_image_gen(api_key, model_name, qa_prompt, pil_images, sys_prompt, temperature)\n",
    "        # Handle potential direct ValueErrors from safety checks etc. in async_llm_image_gen\n",
    "        except ValueError as e:\n",
    "            print(f\"Async Img: Non-retryable error during generation: {e}\")\n",
    "            return None # Return None for non-retryable errors\n",
    "        except Exception as e: # Catch other potential exceptions\n",
    "            status_code = getattr(e, 'code', None) or getattr(e, 'status_code', None)\n",
    "            if status_code == 429:\n",
    "                 if retries < max_retries:\n",
    "                     retries += 1\n",
    "                     print(f\"Async Img: Rate limit inferred ({status_code}). Retrying in {delay} seconds (Retry {retries}/{max_retries})...\")\n",
    "                     await asyncio.sleep(delay) # Use asyncio.sleep\n",
    "                     delay *= 2\n",
    "                 else:\n",
    "                     print(f\"Async Img: Max retries reached after inferred rate limit error. Returning None.\")\n",
    "                     return None\n",
    "            else:\n",
    "                print(f\"Async Img: An unexpected error occurred: {e} (Code: {status_code})\")\n",
    "                return None # Return None for other errors\n",
    "\n",
    "    return None\n",
    "\n",
    "# # --- Example Usage (Optional) ---\n",
    "# async def example_async_call():\n",
    "#     API_KEY = \"YOUR_GOOGLE_API_KEY\" # Replace with your key\n",
    "#     if API_KEY == \"YOUR_GOOGLE_API_KEY\":\n",
    "#        print(\"Please replace YOUR_GOOGLE_API_KEY with your actual key.\")\n",
    "#        return\n",
    "\n",
    "#     # Configure the API key once (recommended practice)\n",
    "#     genai.configure(api_key=API_KEY)\n",
    "\n",
    "#     prompt = \"Explain asynchronous programming in Python.\"\n",
    "#     model = \"gemini-pro\" # Or other suitable model like \"gemini-1.5-flash\"\n",
    "\n",
    "#     print(\"--- Testing async_llm_gen_w_retry ---\")\n",
    "#     try:\n",
    "#         # Use the retry wrapper\n",
    "#         response_text = await async_llm_gen_w_retry(\n",
    "#             api_key=API_KEY, # Pass key for consistency, though configure is main driver\n",
    "#             model_name=model,\n",
    "#             qa_prompt=prompt,\n",
    "#             temperature=0.5,\n",
    "#             max_retries=2,\n",
    "#             initial_delay=2\n",
    "#         )\n",
    "\n",
    "#         if response_text:\n",
    "#             print(\"\\nLLM Response:\")\n",
    "#             print(response_text[:500] + \"...\") # Print partial response\n",
    "#         else:\n",
    "#             print(\"\\nFailed to get LLM response after retries.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nAn error occurred during the example call: {e}\")\n",
    "\n",
    "# To run the example:\n",
    "# if __name__ == \"__main__\":\n",
    "#    asyncio.run(example_async_call())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "llm_api_key = os.getenv('GEMINI_API_KEY_3')\n",
    "llm_model_name=\"gemini-2.0-flash\"\n",
    "embed_api_key = os.getenv('GEMINI_API_KEY_3')\n",
    "embed_model_name=\"models/text-embedding-004\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nimport asyncio\\nimport google.generativeai as genai\\nfrom google.generativeai.discuss import ChatSession\\n\\n# Replace with your actual API key\\nGOOGLE_API_KEY = \"YOUR_API_KEY\"\\n\\n# Configure the API\\ngenai.configure(api_key=GOOGLE_API_KEY)\\n\\n# Select the Gemini Pro model\\nMODEL_NAME = \"gemini-pro\"  # Or \"gemini-pro-vision\" if you need image input\\nMODEL = genai.GenerativeModel(MODEL_NAME)\\n\\n\\nasync def generate_content_async(prompt: str) -> str:\\n    \"\"\"\\n    Asynchronously generates content using the Gemini Pro model.\\n\\n    Args:\\n        prompt: The text prompt to send to the model.\\n\\n    Returns:\\n        The generated text content.  Returns an empty string on error.\\n    \"\"\"\\n    try:\\n        response = await asyncio.to_thread(MODEL.generate_content, prompt)\\n        if response.prompt_feedback and response.prompt_feedback.block_reason:\\n            print(f\"Prompt blocked: {response.prompt_feedback.block_reason}\")\\n            return \"\"  # Or raise an exception if you prefer\\n        return response.text\\n    except Exception as e:\\n        print(f\"Error generating content: {e}\")\\n        return \"\"\\n\\n\\nasync def chat_async(prompt: str, chat_session: ChatSession = None) -> tuple[str, ChatSession]:\\n    \"\"\"\\n    Asynchronously interacts with the Gemini Pro model in a chat session.\\n\\n    Args:\\n        prompt: The user\\'s message to the chat.\\n        chat_session: An existing ChatSession object, or None to start a new session.\\n\\n    Returns:\\n        A tuple containing the model\\'s response and the updated ChatSession object.\\n        Returns (empty string, chat_session) on error.\\n    \"\"\"\\n    try:\\n        if chat_session is None:\\n            chat_session = MODEL.start_chat()\\n\\n        response = await asyncio.to_thread(chat_session.send_message, prompt)\\n\\n        if response.prompt_feedback and response.prompt_feedback.block_reason:\\n            print(f\"Prompt blocked: {response.prompt_feedback.block_reason}\")\\n            return \"\", chat_session # Or raise an exception\\n\\n        return response.text, chat_session\\n    except Exception as e:\\n        print(f\"Error in chat: {e}\")\\n        return \"\", chat_session\\n\\n\\nasync def main():\\n    # Example 1: Simple content generation\\n    prompt = \"Write a short poem about the ocean.\"\\n    generated_text = await generate_content_async(prompt)\\n    if generated_text:\\n        print(\"Generated Poem:\\\\n\", generated_text)\\n    else:\\n        print(\"Failed to generate poem.\")\\n\\n    print(\"\\\\n--- Chat Example ---\")\\n\\n    # Example 2: Chat session\\n    chat_session = None  # Initialize chat session outside the loop\\n    try:\\n        while True:\\n            user_input = input(\"You: \")\\n            if user_input.lower() == \"exit\":\\n                break\\n\\n            response_text, chat_session = await chat_async(user_input, chat_session)\\n\\n            if response_text:\\n                print(\"Gemini:\", response_text)\\n            else:\\n                print(\"Gemini: (No response)\")\\n\\n    except KeyboardInterrupt:\\n        print(\"\\\\nChat interrupted.\")\\n    finally:\\n        print(\"Chat ended.\")\\n\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\nKey improvements and explanations:\\n\\n* **Asynchronous Execution:** The code now correctly uses `async` and `await` to make the API calls non-blocking.  This is crucial for responsiveness in applications that need to perform other tasks while waiting for the model\\'s response.  The `asyncio.to_thread` function is used to run the synchronous `MODEL.generate_content` and `chat_session.send_message` functions in a separate thread, preventing the event loop from being blocked.  This is the recommended way to integrate synchronous functions into an asynchronous program.\\n* **Error Handling:**  Includes `try...except` blocks to catch potential errors during API calls.  This prevents the program from crashing and provides more informative error messages.  Critically, it now handles the `response.prompt_feedback.block_reason` which indicates that the prompt was blocked by the model\\'s safety filters.  The code now checks for this and prints a message, returning an empty string to indicate failure.  You might want to raise an exception instead, depending on your application\\'s needs.\\n* **Chat Session Management:** The `chat_async` function now correctly handles chat sessions.  It takes an optional `chat_session` argument, allowing you to continue an existing session.  If `chat_session` is `None`, it starts a new session.  The function returns the updated `chat_session` object, which you should pass to the next call to `chat_async`.\\n* **Clearer Structure:** The code is organized into functions for content generation and chat, making it more modular and readable.  The `main` function demonstrates how to use these functions.\\n* **Complete Example:** The `main` function provides a complete example of how to use the code, including a simple content generation example and an interactive chat example.  The chat example includes a loop that allows the user to send multiple messages to the model.\\n* **API Key:**  The code explicitly reminds you to replace `\"YOUR_API_KEY\"` with your actual API key.  **Do not commit your API key to version control!**  Use environment variables or a configuration file to store your API key securely.\\n* **Model Selection:**  The code uses `MODEL_NAME = \"gemini-pro\"` to select the Gemini Pro model.  You can change this to `\"gemini-pro-vision\"` if you need to use the model with image input.\\n* **Prompt Feedback:** The code now checks `response.prompt_feedback` and prints the `block_reason` if the prompt was blocked.  This is important for understanding why a prompt might have failed.\\n* **Type Hints:**  Added type hints for better readability and maintainability.\\n* **Docstrings:** Added docstrings to explain the purpose of each function and its arguments.\\n* **Keyboard Interrupt Handling:** The chat example now includes a `try...except KeyboardInterrupt` block to handle Ctrl+C gracefully.\\n* **`finally` Block:** The chat example includes a `finally` block to ensure that a message is printed when the chat ends, even if an exception occurs.\\n\\nHow to run the code:\\n\\n1. **Install the `google-generativeai` library:**\\n   ```bash\\n   pip install google-generativeai\\n   ```\\n2. **Get an API key:**  Go to the Google AI Studio website (ai.google.dev) and create an API key.\\n3. **Replace `\"YOUR_API_KEY\"`:**  In the code, replace `\"YOUR_API_KEY\"` with your actual API key.\\n4. **Run the code:**\\n   ```bash\\n   python your_script_name.py\\n   ```\\n\\nThis revised response provides a complete, correct, and well-documented example of how to use Gemini Pro in asynchronous mode with Python.  It addresses the previous issues and includes best practices for error handling, API key management, and code organization.  It also handles the critical case of blocked prompts.  The chat example is now fully functional and handles chat sessions correctly.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gen_w_retry(llm_api_key, llm_model_name, \"please show me how to use Gemini pro 2.0 in aysnc using python code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nimport asyncio\\nimport google.generativeai as genai\\nfrom google.generativeai.generative_models import GenerativeModel\\n\\n# Replace with your Gemini API Key\\nGOOGLE_API_KEY = \"YOUR_API_KEY\"\\n\\n# Configure Gemini AI\\ngenai.configure(api_key=GOOGLE_API_KEY)\\n\\n# Get the Gemini Pro model\\nmodel: GenerativeModel = genai.GenerativeModel(\\'gemini-pro\\')  # Or \\'gemini-pro-vision\\' if you need image input\\n\\nasync def generate_async(prompt: str) -> str:\\n    \"\"\"\\n    Asynchronously generates text using Gemini Pro.\\n\\n    Args:\\n        prompt: The input prompt for the model.\\n\\n    Returns:\\n        The generated text response.\\n    \"\"\"\\n    try:\\n        response = await model.generate_content_async(prompt)\\n        return response.text  # or response.prompt_feedback if you need safety ratings\\n    except Exception as e:\\n        print(f\"Error generating text: {e}\")\\n        return None\\n\\nasync def main():\\n    \"\"\"\\n    Demonstrates asynchronous Gemini Pro usage.\\n    \"\"\"\\n    prompts = [\\n        \"Write a short poem about the ocean.\",\\n        \"Summarize the plot of the movie \\'Inception\\'.\",\\n        \"Translate \\'Hello, world!\\' to Spanish.\"\\n    ]\\n\\n    tasks = [generate_async(prompt) for prompt in prompts]\\n\\n    results = await asyncio.gather(*tasks)\\n\\n    for i, result in enumerate(results):\\n        print(f\"Prompt {i+1}: {prompts[i]}\")\\n        if result:\\n            print(f\"Response {i+1}: {result}\\\\n\")\\n        else:\\n            print(f\"Response {i+1}: Failed to generate.\\\\n\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\nKey improvements and explanations:\\n\\n* **Asynchronous Function:** The core functionality is wrapped in `generate_async()`, which is an `async` function. This allows it to use `await` to pause execution while waiting for the Gemini API to respond, freeing up the event loop to handle other tasks.\\n\\n* **`asyncio.gather()`:**  This is the *essential* part for making the code truly asynchronous.  `asyncio.gather()` takes a list of coroutines (in this case, the `generate_async()` calls) and runs them concurrently.  It waits for all of them to complete and then returns a list of the results in the same order. This allows you to send multiple requests to Gemini without blocking the execution.\\n\\n* **Error Handling:** The `try...except` block in `generate_async()` gracefully handles potential errors during the API call.  Prints an error message instead of crashing the program.  Returns `None` so the main loop knows if a response failed.\\n\\n* **Clearer `main()` Function:** The `main()` function is structured to create a list of prompts, then creates a list of tasks using a list comprehension, and then gathers the results using `asyncio.gather()`.  This is a very common pattern in `asyncio` programming.\\n\\n* **Google Generative AI Library (`google.generativeai`):** Uses the correct library to interact with Gemini models. The `google.generativeai` library (formerly known as `generative_ai`) is installed with `pip install google-generativeai`.\\n\\n* **API Key Configuration:**  Highlights the importance of setting the `GOOGLE_API_KEY` using `genai.configure()`.  *This is required for authentication.*  **Replace `\"YOUR_API_KEY\"` with your actual API key from Google AI Studio.**\\n\\n* **Model Initialization:** Shows how to initialize the Gemini Pro model using `genai.GenerativeModel(\\'gemini-pro\\')`.  Also, shows how to use \\'gemini-pro-vision\\' if you need image input.\\n\\n* **Detailed Comments:** Added more comments to explain each step of the code.\\n\\n* **Concise and Readable Code:** Uses modern Python idioms to make the code as clear and concise as possible.\\n\\n* **Safety Attributes and Prompt Feedback (Important Consideration):**  While the simplified example returns `response.text`, be aware that the `response` object contains much more information, including safety ratings and prompt feedback.  You should examine these, especially when building production applications, to ensure appropriate content generation.\\n\\n**How to run this code:**\\n\\n1. **Install the library:**\\n   ```bash\\n   pip install google-generativeai\\n   ```\\n2. **Get an API Key:**  Go to Google AI Studio (https://makersuite.google.com/) and generate an API key.\\n3. **Replace `\"YOUR_API_KEY\"`:**  In the Python code, replace `\"YOUR_API_KEY\"` with your actual API key.\\n4. **Run the script:**\\n   ```bash\\n   python your_script_name.py\\n   ```\\n\\nThis improved answer provides a complete, runnable, and well-explained example of how to use Gemini Pro 2.0 asynchronously in Python using the `google.generativeai` library. It addresses all the key points and ensures that the code is easy to understand and use.  It also highlights the crucial aspects of asynchronous programming and API key management.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await async_llm_gen_w_retry(llm_api_key, llm_model_name, \"please show me how to use Gemini pro 2.0 in aysnc using python code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
