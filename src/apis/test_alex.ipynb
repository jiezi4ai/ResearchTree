{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d0b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import asyncio\n",
    "from typing import List, Dict, Optional, Any, Callable, Union\n",
    "import pyalex\n",
    "from pyalex import Works, Authors, Concepts, Institutions # Import necessary pyalex classes including Institutions\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set your email for the OpenAlex polite pool (optional but recommended)\n",
    "# pyalex.config.email = \"your_email@example.com\"\n",
    "\n",
    "MAX_CNT = 10000\n",
    "BATCH_SIZE = 50\n",
    "DEFAULT_MAX_CONCURRENCY = 10\n",
    "DEFAULT_SLEEP_INTERVAL = 0.1\n",
    "OPENALEX_MAX_PER_PAGE = 200\n",
    "\n",
    "class OpenAlexKit:\n",
    "    def __init__(\n",
    "        self,\n",
    "        email: str = None,\n",
    "        max_concurrency: int = DEFAULT_MAX_CONCURRENCY,\n",
    "        sleep_interval: float = DEFAULT_SLEEP_INTERVAL,\n",
    "    ):\n",
    "        if max_concurrency <= 0:\n",
    "            raise ValueError(\"max_concurrency must be a positive integer\")\n",
    "        if sleep_interval < 0:\n",
    "            raise ValueError(\"sleep_interval must be non-negative\")\n",
    "\n",
    "        if email:\n",
    "            pyalex.config.email = email\n",
    "            logger.info(f\"OpenAlex email set for polite pool: {email}\")\n",
    "        else:\n",
    "            logger.warning(\"OpenAlex email not set. Using anonymous pool (lower rate limits).\")\n",
    "\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.max_cnt = MAX_CNT\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrency)\n",
    "        self.sleep_interval = sleep_interval\n",
    "        logger.info(f\"OpenAlexKit initialized with max_concurrency={max_concurrency}, sleep_interval={sleep_interval}s\")\n",
    "\n",
    "    async def _execute_sync_with_controls(self, sync_func: Callable, *args: Any, **kwargs: Any) -> Any:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        async with self.semaphore:\n",
    "            func_name = sync_func.__name__\n",
    "            logger.debug(f\"Semaphore acquired for {func_name}. Executing...\")\n",
    "            try:\n",
    "                result = await asyncio.to_thread(sync_func, *args, **kwargs)\n",
    "                logger.debug(f\"Execution of {func_name} completed. Result type: {type(result)}. Sleeping for {self.sleep_interval}s.\")\n",
    "                await asyncio.sleep(self.sleep_interval)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Exception during controlled execution of {func_name}: {e}\", exc_info=True)\n",
    "                logger.debug(f\"Sleeping for {self.sleep_interval}s after error in {func_name}.\")\n",
    "                await asyncio.sleep(self.sleep_interval)\n",
    "                if \"search\" in func_name or \"get\" in func_name:\n",
    "                     return []\n",
    "                raise e\n",
    "            finally:\n",
    "                logger.debug(f\"Semaphore released for {func_name}.\")\n",
    "\n",
    "\n",
    "    # --- Synchronous Helper Methods ---\n",
    "\n",
    "    # Updated _validate_openalex_ids function\n",
    "    def _validate_openalex_ids(self, entity_type: str, ids: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Validates input IDs and returns only the native OpenAlex IDs\n",
    "        matching the expected entity type.\n",
    "        It recognizes DOI URLs (for works), ORCID URLs (for authors),\n",
    "        and ROR URLs (for institutions) as valid input formats but does not\n",
    "        return them in the output list, logging warnings instead.\n",
    "\n",
    "        Returns a list of validated native OpenAlex IDs (e.g., W..., A...).\n",
    "        \"\"\"\n",
    "        prefix_map = {\"work\": \"W\", \"author\": \"A\", \"venue\": \"V\", \"concept\": \"C\", \"institution\": \"I\"}\n",
    "        entity_type_lower = entity_type.lower()\n",
    "        entity_prefix = prefix_map.get(entity_type_lower)\n",
    "        # No error if entity_prefix is None, just means we can only validate by structure if URL used\n",
    "\n",
    "        valid_ids = []\n",
    "        recognized_external_count = 0\n",
    "        skipped_count = 0\n",
    "        processed_ids = set() # Keep track of processed IDs to avoid duplicate warnings/processing\n",
    "\n",
    "        for item_id_raw in ids:\n",
    "            if not isinstance(item_id_raw, str) or not item_id_raw.strip():\n",
    "                # Skip empty or non-string entries silently or log if needed\n",
    "                continue\n",
    "\n",
    "            item_id = item_id_raw.strip()\n",
    "\n",
    "            # Avoid processing the same ID multiple times if it appears duplicated in the input list\n",
    "            if item_id in processed_ids:\n",
    "                continue\n",
    "            processed_ids.add(item_id)\n",
    "\n",
    "            # 1. Check for Native OpenAlex ID (URL or direct)\n",
    "            is_native = False\n",
    "            native_id_to_add = None\n",
    "            if item_id.startswith(\"https://openalex.org/\"):\n",
    "                potential_id = item_id.split(\"/\")[-1]\n",
    "                # Check if the extracted part starts with the *expected* prefix (if known)\n",
    "                if entity_prefix and potential_id.startswith(entity_prefix):\n",
    "                    native_id_to_add = potential_id\n",
    "                    is_native = True\n",
    "                # Or if type is unknown, check if it starts with *any* OA prefix\n",
    "                elif not entity_prefix and any(potential_id.startswith(p) for p in prefix_map.values()):\n",
    "                     native_id_to_add = potential_id # Assume valid if type unknown\n",
    "                     is_native = True\n",
    "                # Else: Prefix mismatch or not an OA structure after URL prefix\n",
    "            elif entity_prefix and item_id.startswith(entity_prefix):\n",
    "                 native_id_to_add = item_id\n",
    "                 is_native = True\n",
    "\n",
    "            if is_native and native_id_to_add:\n",
    "                valid_ids.append(native_id_to_add)\n",
    "                continue # Found native ID, move to next item\n",
    "\n",
    "            # 2. Check for recognized External IDs (only if entity type matches)\n",
    "            is_external_recognized = False\n",
    "            external_type = None\n",
    "            if entity_type_lower == \"work\" and item_id.startswith(\"https://doi.org/\"):\n",
    "                 is_external_recognized = True\n",
    "                 external_type = \"DOI\"\n",
    "            elif entity_type_lower == \"author\" and item_id.startswith(\"https://orcid.org/\"):\n",
    "                 is_external_recognized = True\n",
    "                 external_type = \"ORCID\"\n",
    "            elif entity_type_lower == \"institution\" and item_id.startswith(\"https://ror.org/\"):\n",
    "                 is_external_recognized = True\n",
    "                 external_type = \"ROR\"\n",
    "\n",
    "            if is_external_recognized:\n",
    "                recognized_external_count += 1\n",
    "                valid_ids.append(item_id)\n",
    "                logger.warning(f\"Recognized external ID format ({external_type}: {item_id}) for type '{entity_type}', but it will be skipped by this function. Modify calling method to handle {external_type} lookups if needed.\")\n",
    "\n",
    "            # 3. If none matched (and not native)\n",
    "            # Log specific warnings for prefix mismatches vs general invalid format\n",
    "            is_other_oa_id = False\n",
    "            if any(item_id.startswith(p) or (item_id.startswith(\"https://openalex.org/\") and any(item_id.split(\"/\")[-1].startswith(p) for p in prefix_map.values())) for p in prefix_map.values()):\n",
    "                 # It looks like an OpenAlex ID but didn't match the expected type or structure checks above\n",
    "                 is_other_oa_id = True\n",
    "\n",
    "            if is_other_oa_id:\n",
    "                 logger.warning(f\"ID {item_id} looks like an OpenAlex ID but does not match expected type '{entity_type}' or has unexpected format. Skipping.\")\n",
    "            else:\n",
    "                 logger.warning(f\"Invalid or unrecognized ID format for type '{entity_type}' skipped: {item_id}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "\n",
    "        if recognized_external_count > 0:\n",
    "             logger.info(f\"Recognized {recognized_external_count} external IDs (DOI/ORCID/ROR) for entity type '{entity_type}'.\")\n",
    "        if skipped_count > 0:\n",
    "             logger.info(f\"Skipped {skipped_count} other invalid, non-matching, or empty IDs for type '{entity_type}'.\")\n",
    "\n",
    "        # Return unique list of valid native IDs\n",
    "        # Using dict.fromkeys preserves order while removing duplicates\n",
    "        unique_valid_native_ids = list(dict.fromkeys(valid_ids))\n",
    "        logger.info(f\"Validation for type '{entity_type}' completed. Returning {len(unique_valid_native_ids)} unique native OpenAlex IDs.\")\n",
    "        return unique_valid_native_ids\n",
    "\n",
    "    # --- Other Sync Methods (_sync_get_works_by_ids, _sync_get_authors_by_ids, etc.) ---\n",
    "    # (These remain unchanged from the previous version, as they rely on the output\n",
    "    #  of _validate_openalex_ids which still returns a List[str] of native IDs)\n",
    "\n",
    "    def _sync_get_works_by_ids(self, work_ids: List[str]) -> List[Dict]:\n",
    "        \"\"\"Fetches work details for a batch of OpenAlex Work IDs.\"\"\"\n",
    "        logger.info(f\"_sync_get_works_by_ids: Thread started for batch ({len(work_ids)} IDs, first 5: {work_ids[:5]}...).\")\n",
    "        if not work_ids:\n",
    "            return []\n",
    "        try:\n",
    "            filter_query = \"|\".join(work_ids)\n",
    "            results = Works().filter(openalex_id=filter_query).get()\n",
    "            logger.info(f\"_sync_get_works_by_ids: API call successful for batch (first 5: {work_ids[:5]}...), returning {len(results)} items.\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _sync_get_works_by_ids for batch (first 5 IDs: {work_ids[:5]}...): {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def _sync_get_authors_by_ids(self, author_ids: List[str]) -> List[Dict]:\n",
    "        \"\"\"Fetches author details for a batch of OpenAlex Author IDs.\"\"\"\n",
    "        logger.info(f\"_sync_get_authors_by_ids: Thread started for batch ({len(author_ids)} IDs, first 5: {author_ids[:5]}...).\")\n",
    "        if not author_ids:\n",
    "            return []\n",
    "        try:\n",
    "            filter_query = \"|\".join(author_ids)\n",
    "            results = Authors().filter(openalex_id=filter_query).get()\n",
    "            logger.info(f\"_sync_get_authors_by_ids: API call successful for batch (first 5: {author_ids[:5]}...), returning {len(results)} items.\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _sync_get_authors_by_ids for batch (first 5 IDs: {author_ids[:5]}...): {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    # ... (rest of the _sync methods: _sync_search_works_by_keywords, _sync_get_work_references, _sync_get_work_citations remain the same) ...\n",
    "    def _sync_search_works_by_keywords(self, **kwargs) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        query = kwargs.get('query', None)\n",
    "        limit = kwargs.get('limit', self.max_cnt)\n",
    "        logger.info(f\"_sync_search_works_by_keywords: Thread started for query '{str(query)[:50]}...' with limit {limit}.\")\n",
    "        try:\n",
    "            works_query = Works()\n",
    "            if query: works_query = works_query.search(query)\n",
    "            # Apply filters... (year, type, oa, venue, concepts, pub_date, citations) - Logic unchanged\n",
    "            if kwargs.get('year'): works_query = works_query.filter(publication_year=int(kwargs['year']))\n",
    "            # ... other filters ...\n",
    "            if kwargs.get('min_citation_count') is not None: works_query = works_query.filter(cited_by_count=f\">{int(kwargs['min_citation_count'])}\")\n",
    "            # Apply sorting... - Logic unchanged\n",
    "            sort_param = kwargs.get('sort')\n",
    "            if sort_param:\n",
    "                sort_field_map = {'relevance': 'relevance_score', 'citationCount': 'cited_by_count', 'publicationDate': 'publication_date'}\n",
    "                # ... sorting logic ...\n",
    "                if field_s2 in sort_field_map: works_query = works_query.sort(**{sort_field_map[field_s2]: direction})\n",
    "\n",
    "            # Fetching Results with Pagination - Logic unchanged\n",
    "            paper_metadata = []\n",
    "            processed_count = 0\n",
    "            page_size = min(limit, OPENALEX_MAX_PER_PAGE)\n",
    "            if limit <= 0: page_size = OPENALEX_MAX_PER_PAGE\n",
    "            logger.info(f\"Executing OpenAlex search query with limit={limit}, page_size={page_size}...\")\n",
    "            for page in works_query.paginate(per_page=page_size, n_max=limit):\n",
    "                 if not page: break\n",
    "                 num_to_add = min(len(page), limit - processed_count)\n",
    "                 paper_metadata.extend(page[:num_to_add])\n",
    "                 processed_count += num_to_add\n",
    "                 if processed_count >= limit: break\n",
    "            logger.info(f\"_sync_search_works_by_keywords: API call successful for query '{str(query)[:50]}...', returning {len(paper_metadata)} items.\")\n",
    "            return paper_metadata\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _sync_search_works_by_keywords for query '{str(query)[:50]}...': {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def _sync_get_work_references(self, work_id: str, limit: int) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        logger.info(f\"_sync_get_work_references: Thread started for work {work_id} with limit {limit}.\")\n",
    "        if not work_id: return []\n",
    "        try:\n",
    "            work_data = Works()[work_id].get()\n",
    "            if not work_data or 'referenced_works' not in work_data: return []\n",
    "            referenced_urls = work_data.get('referenced_works', [])\n",
    "            if not referenced_urls: return []\n",
    "            referenced_ids = [url.split('/')[-1] for url in referenced_urls if url and url.startswith(\"https://openalex.org/W\")] # Ensure they are Work IDs\n",
    "            referenced_ids_limited = referenced_ids[:limit] # Apply limit *before* validation\n",
    "            # Validate these IDs are actual work IDs before fetching\n",
    "            valid_referenced_ids = self._validate_openalex_ids(\"work\", referenced_ids_limited) # Use internal validation\n",
    "            if not valid_referenced_ids: return []\n",
    "            logger.info(f\"Fetching details for {len(valid_referenced_ids)} referenced works for {work_id}.\")\n",
    "            referenced_works_data = self._sync_get_works_by_ids(valid_referenced_ids) # Fetch validated IDs\n",
    "            logger.info(f\"_sync_get_work_references: API call successful for work {work_id}, returning {len(referenced_works_data)} referenced items.\")\n",
    "            return referenced_works_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _sync_get_work_references for {work_id}: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _sync_get_work_citations(self, work_id: str, limit: int) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        logger.info(f\"_sync_get_work_citations: Thread started for work {work_id} with limit {limit}.\")\n",
    "        if not work_id: return []\n",
    "        try:\n",
    "            citing_works_query = Works().filter(cites=work_id)\n",
    "            citations_metadata = []\n",
    "            processed_count = 0\n",
    "            page_size = min(limit, OPENALEX_MAX_PER_PAGE)\n",
    "            if limit <= 0: page_size = OPENALEX_MAX_PER_PAGE\n",
    "            logger.info(f\"Executing OpenAlex citations query for {work_id} with limit={limit}, page_size={page_size}...\")\n",
    "            for page in citing_works_query.paginate(per_page=page_size, n_max=limit):\n",
    "                if not page: break\n",
    "                num_to_add = min(len(page), limit - processed_count)\n",
    "                citations_metadata.extend(page[:num_to_add])\n",
    "                processed_count += num_to_add\n",
    "                if processed_count >= limit: break\n",
    "            logger.info(f\"_sync_get_work_citations: API call successful for work {work_id}, returning {len(citations_metadata)} citing items.\")\n",
    "            return citations_metadata\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _sync_get_work_citations for {work_id}: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "\n",
    "    # --- Asynchronous Public Methods ---\n",
    "    # (These remain unchanged from the previous version, as they call _validate_openalex_ids\n",
    "    #  and receive back a List[str] of native IDs, which they already expect)\n",
    "\n",
    "    async def async_search_paper_by_ids(\n",
    "        self,\n",
    "        id_list: List[str] # Can contain OpenAlex IDs, DOI URLs\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search paper by OpenAlex Work IDs or DOI URLs asynchronously.\n",
    "           NOTE: Currently only processes native OpenAlex IDs due to validator output.\n",
    "        \"\"\"\n",
    "        # Validate IDs - This now recognizes DOIs but only returns native W... IDs\n",
    "        valid_native_id_list = self._validate_openalex_ids(\"work\", id_list)\n",
    "        id_cnt = len(valid_native_id_list)\n",
    "        paper_metadata = []\n",
    "\n",
    "        if id_cnt > 0:\n",
    "            batch_size = self.batch_size\n",
    "            batch_cnt = math.ceil(id_cnt / batch_size)\n",
    "            batches = [valid_native_id_list[i * batch_size:(i + 1) * batch_size] for i in range(batch_cnt)]\n",
    "\n",
    "            tasks = []\n",
    "            logger.info(f\"async_search_paper_by_ids: Creating {len(batches)} tasks for {id_cnt} valid native OpenAlex Work IDs.\")\n",
    "            for batch in batches:\n",
    "                tasks.append(self._execute_sync_with_controls(self._sync_get_works_by_ids, batch))\n",
    "\n",
    "            logger.info(f\"async_search_paper_by_ids: Gathering {len(tasks)} tasks...\")\n",
    "            batch_results_list = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            logger.info(f\"async_search_paper_by_ids: Gather complete. Processing results.\")\n",
    "\n",
    "            for result in batch_results_list:\n",
    "                if isinstance(result, Exception): logger.error(f\"A batch task for async_search_paper_by_ids failed: {result}\")\n",
    "                elif isinstance(result, list): paper_metadata.extend(result)\n",
    "                else: logger.warning(f\"Unexpected result type {type(result)} from paper batch task: {result}\")\n",
    "        else:\n",
    "            logger.warning(\"async_search_paper_by_ids: No valid native OpenAlex Work IDs found in the input list.\")\n",
    "\n",
    "        return paper_metadata\n",
    "\n",
    "    async def async_search_author_by_ids(\n",
    "        self,\n",
    "        author_ids: List[str], # Can contain OpenAlex IDs, ORCID URLs\n",
    "        with_abstract: Optional[bool] = False\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search author by OpenAlex Author IDs or ORCID URLs asynchronously.\n",
    "           NOTE: Currently only processes native OpenAlex IDs due to validator output.\n",
    "        \"\"\"\n",
    "        # Validate IDs - This now recognizes ORCIDs but only returns native A... IDs\n",
    "        valid_native_id_list = self._validate_openalex_ids(\"author\", author_ids)\n",
    "        id_cnt = len(valid_native_id_list)\n",
    "        author_metadata = []\n",
    "\n",
    "        if id_cnt > 0:\n",
    "            batch_size = self.batch_size\n",
    "            batch_cnt = math.ceil(id_cnt / batch_size)\n",
    "            batches = [valid_native_id_list[i * batch_size:(i + 1) * batch_size] for i in range(batch_cnt)]\n",
    "            logger.info(f\"async_search_author_by_ids: Fetching {id_cnt} authors by native ID in {batch_cnt} batches.\")\n",
    "\n",
    "            tasks = []\n",
    "            for batch in batches:\n",
    "                 tasks.append(self._execute_sync_with_controls(self._sync_get_authors_by_ids, batch))\n",
    "\n",
    "            logger.info(f\"async_search_author_by_ids: Gathering {len(tasks)} tasks...\")\n",
    "            batch_results_list = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            logger.info(f\"async_search_author_by_ids: Gather complete. Processing results.\")\n",
    "\n",
    "            for result in batch_results_list:\n",
    "                if isinstance(result, Exception): logger.error(f\"A batch task for async_search_author_by_ids failed: {result}\")\n",
    "                elif isinstance(result, list): author_metadata.extend(result)\n",
    "                else: logger.warning(f\"Unexpected result type {type(result)} from author batch task: {result}\")\n",
    "        else:\n",
    "             logger.warning(\"async_search_author_by_ids: No valid native OpenAlex Author IDs found in the input list.\")\n",
    "\n",
    "        if with_abstract:\n",
    "            logger.warning(\"`with_abstract=True` is not directly supported for `async_search_author_by_ids` with OpenAlex.\")\n",
    "\n",
    "        return author_metadata\n",
    "\n",
    "    # ... (rest of the async methods: async_search_paper_by_keywords, async_get_s2_cited_papers, async_get_s2_citing_papers, async_get_s2_recommended_papers remain the same) ...\n",
    "    async def async_search_paper_by_keywords(self, query: str, year: str = None, publication_types: list = None, open_access_pdf: bool = None, venue: list = None, fields_of_study: list = None, publication_date_or_year: str = None, min_citation_count: int = None, limit: int = 100, bulk: bool = False, sort: str = None, match_title: bool = False) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        search_kwargs = { k: v for k, v in locals().items() if k != 'self' and v is not None and k != 'match_title'} # Simplified kwargs creation\n",
    "        search_kwargs['limit'] = min(search_kwargs.get('limit', 100), self.max_cnt) # Ensure limit is applied correctly\n",
    "        logger.info(f\"async_search_paper_by_keywords: Searching papers by keyword: '{query[:50]}...' with effective limit {search_kwargs.get('limit')}.\")\n",
    "        try:\n",
    "            paper_metadata = await self._execute_sync_with_controls(self._sync_search_works_by_keywords, **search_kwargs)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"async_search_paper_by_keywords: Failed for query '{query[:50]}...': {e}\")\n",
    "             paper_metadata = []\n",
    "        return paper_metadata\n",
    "\n",
    "    async def async_get_s2_cited_papers(self, paper_id: str, limit: int = 100, with_abstract: Optional[bool] = False) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        # Note: paper_id here MUST be a native OpenAlex Work ID for _sync_get_work_references to work\n",
    "        valid_paper_id = self._validate_openalex_ids(\"work\", [paper_id])\n",
    "        if not valid_paper_id:\n",
    "             logger.error(f\"async_get_s2_cited_papers: Invalid native OpenAlex Work ID provided: {paper_id}\")\n",
    "             return []\n",
    "        work_id = valid_paper_id[0]\n",
    "\n",
    "        max_limit = min(limit, self.max_cnt)\n",
    "        logger.info(f\"async_get_s2_cited_papers: Fetching references for paper {work_id} with effective limit {max_limit}.\")\n",
    "        refs_metadata = []\n",
    "        try:\n",
    "            refs_metadata = await self._execute_sync_with_controls(self._sync_get_work_references, work_id, max_limit)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"async_get_s2_cited_papers: Failed for paper {work_id}: {e}\")\n",
    "             refs_metadata = []\n",
    "\n",
    "        # Handle with_abstract... (Logic unchanged)\n",
    "        if with_abstract and refs_metadata:\n",
    "            papers_missing_abstracts_ids = set()\n",
    "            # ... abstract fetching logic ...\n",
    "            if papers_missing_abstracts_ids:\n",
    "                 # ... call async_search_paper_by_ids ...\n",
    "                 # ... map abstracts back ...\n",
    "                 pass # Placeholder for brevity\n",
    "\n",
    "        return refs_metadata\n",
    "\n",
    "    async def async_get_s2_citing_papers(self, paper_id: str, limit: int = 100, with_abstract: Optional[bool] = False) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version)\n",
    "        # Note: paper_id here MUST be a native OpenAlex Work ID for _sync_get_work_citations to work\n",
    "        valid_paper_id = self._validate_openalex_ids(\"work\", [paper_id])\n",
    "        if not valid_paper_id:\n",
    "             logger.error(f\"async_get_s2_citing_papers: Invalid native OpenAlex Work ID provided: {paper_id}\")\n",
    "             return []\n",
    "        work_id = valid_paper_id[0]\n",
    "\n",
    "        max_limit = min(limit, self.max_cnt)\n",
    "        logger.info(f\"async_get_s2_citing_papers: Fetching citations for paper {work_id} with effective limit {max_limit}.\")\n",
    "        citedby_metadata = []\n",
    "        try:\n",
    "            citedby_metadata = await self._execute_sync_with_controls(self._sync_get_work_citations, work_id, max_limit)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"async_get_s2_citing_papers: Failed for paper {work_id}: {e}\")\n",
    "            citedby_metadata = []\n",
    "\n",
    "        # Handle with_abstract... (Logic unchanged)\n",
    "        if with_abstract and citedby_metadata:\n",
    "             papers_missing_abstracts_ids = set()\n",
    "             # ... abstract fetching logic ...\n",
    "             if papers_missing_abstracts_ids:\n",
    "                  # ... call async_search_paper_by_ids ...\n",
    "                  # ... map abstracts back ...\n",
    "                  pass # Placeholder for brevity\n",
    "\n",
    "        return citedby_metadata\n",
    "\n",
    "    async def async_get_s2_recommended_papers(self, positive_paper_ids: List[str], negative_paper_ids: List[str] = None, limit: int = 100, with_abstract: Optional[bool] = False) -> List[Dict]:\n",
    "        # (Implementation remains the same as previous version - logs warning, returns [])\n",
    "        logger.warning(\"OpenAlex does not support recommendations based on positive/negative paper ID lists like Semantic Scholar.\")\n",
    "        logger.warning(\"async_get_s2_recommended_papers will return an empty list.\")\n",
    "        valid_pos_ids = self._validate_openalex_ids(\"work\", positive_paper_ids) # Validate for logging\n",
    "        valid_neg_ids = []\n",
    "        if negative_paper_ids: valid_neg_ids = self._validate_openalex_ids(\"work\", negative_paper_ids)\n",
    "        logger.info(f\"async_get_s2_recommended_papers called with {len(valid_pos_ids)} valid native positive IDs. Limit: {limit}. Returning [].\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f355d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dois = ['https://doi.org/10.48550/arXiv.2406.10252',  # AutoSurvey: Large Language Models Can Automatically Write Surveys\n",
    "            'https://doi.org/10.48550/arXiv.2412.10415',  # Generative Adversarial Reviews: When LLMs Become the Critic\n",
    "            'https://doi.org/10.48550/arXiv.2402.12928',  # A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d4b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:13:15,246 - INFO - OpenAlex email set for polite pool: ai4fun@gmail.com\n",
      "2025-04-21 15:13:15,247 - INFO - OpenAlexKit initialized with max_concurrency=10, sleep_interval=0.1s\n",
      "2025-04-21 15:13:15,247 - WARNING - Recognized external ID format (DOI: https://doi.org/10.7717/peerj.4375) for type 'work', but it will be skipped by this function. Modify calling method to handle DOI lookups if needed.\n",
      "2025-04-21 15:13:15,249 - WARNING - Invalid or unrecognized ID format for type 'work' skipped: https://doi.org/10.7717/peerj.4375\n",
      "2025-04-21 15:13:15,249 - INFO - Recognized 1 external IDs (DOI/ORCID/ROR) for entity type 'work'.\n",
      "2025-04-21 15:13:15,250 - INFO - Skipped 1 other invalid, non-matching, or empty IDs for type 'work'.\n",
      "2025-04-21 15:13:15,250 - INFO - Validation for type 'work' completed. Returning 1 unique native OpenAlex IDs.\n",
      "2025-04-21 15:13:15,251 - INFO - async_search_paper_by_ids: Creating 1 tasks for 1 valid native OpenAlex Work IDs.\n",
      "2025-04-21 15:13:15,251 - INFO - async_search_paper_by_ids: Gathering 1 tasks...\n",
      "2025-04-21 15:13:15,252 - INFO - _sync_get_works_by_ids: Thread started for batch (1 IDs, first 5: ['https://doi.org/10.7717/peerj.4375']...).\n",
      "2025-04-21 15:13:16,261 - ERROR - Error in _sync_get_works_by_ids for batch (first 5 IDs: ['https://doi.org/10.7717/peerj.4375']...): 'https://doi.org/10.7717/peerj.4375' is not a valid OpenAlex ID.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/dr/n5l88r750zl4pk13dhslspzh0000gn/T/ipykernel_11050/1416760490.py\", line 177, in _sync_get_works_by_ids\n",
      "    results = Works().filter(openalex_id=filter_query).get()\n",
      "  File \"/opt/miniconda3/envs/jiezi4ai/lib/python3.10/site-packages/pyalex/api.py\", line 551, in get\n",
      "    resp_list = self._get_from_url(self.url)\n",
      "  File \"/opt/miniconda3/envs/jiezi4ai/lib/python3.10/site-packages/pyalex/api.py\", line 522, in _get_from_url\n",
      "    raise QueryError(res.json()[\"message\"])\n",
      "pyalex.api.QueryError: 'https://doi.org/10.7717/peerj.4375' is not a valid OpenAlex ID.\n",
      "2025-04-21 15:13:16,367 - INFO - async_search_paper_by_ids: Gather complete. Processing results.\n"
     ]
    }
   ],
   "source": [
    "oa = OpenAlexKit(email=\"ai4fun@gmail.com\")\n",
    "papers_info = await oa.async_search_paper_by_ids(id_list=[\"https://doi.org/10.7717/peerj.4375\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0a2d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf49477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders\n",
    "\n",
    "import pyalex\n",
    "\n",
    "pyalex.config.email = \"ai4fun2004@gmail.com\"\n",
    "\n",
    "# same as\n",
    "Works()[\"10.48550/arXiv.2501.04682\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167d7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
